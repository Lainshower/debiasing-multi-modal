{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinsu/anaconda3/envs/cuda_test/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# sys.path.append(\"/home/jinsu/workstation/project/debiasing-multi-modal\") # when running in not-root folder \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import tqdm\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from demo.util import AverageMeter\n",
    "from demo.util import adjust_learning_rate, warmup_learning_rate, accuracy, adjust_learning_rate_reg, warmup_learning_rate_reg\n",
    "from demo.util import set_optimizer, set_optimizer_reg, get_lr\n",
    "from demo.util import set_seed\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from data.waterbirds_embeddings import WaterbirdsEmbeddings, load_waterbirds_embeddings\n",
    "from data.celeba_embeddings import CelebaEmbeddings, load_celeba_embeddings\n",
    "\n",
    "model_dict = {'resnet50': [None, 1024]} # (nn.module, 1024)\n",
    "new_order_for_print = [\n",
    "    'weighted_mean_acc',\n",
    "    'worst_acc',\n",
    "    'acc_0_0',\n",
    "    'acc_0_1',\n",
    "    'acc_1_0',\n",
    "    'acc_1_1',\n",
    "    'mean_acc'\n",
    "]\n",
    "from functools import partial\n",
    "\n",
    "class LinearClassifier(nn.Module): # Linear probing\n",
    "    def __init__(self, input_dim, num_classes=2):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.fc(features)\n",
    "\n",
    "\n",
    "\n",
    "class CustomCLIP(nn.Module): # Adapter / Contrastive Adapter\n",
    "    def __init__(self, adapter, text_embedding_dir, text_spurious_embedding_dir, text_group_embedding_dir, temperature=0.01):\n",
    "        super().__init__()\n",
    "        self.text_embedding_dir = text_embedding_dir \n",
    "        self.text_spurious_embedding_dir = text_spurious_embedding_dir\n",
    "        self.text_group_embedding_dir = text_group_embedding_dir #NOTE Joonwon Added\n",
    "        self.adapter = adapter\n",
    "        self.temperature = temperature # CA default : 0.01, B2T default : 0.02 (?) NOTE\n",
    "        \n",
    "        self.text_features = get_text_embedding(self.text_embedding_dir).cuda()\n",
    "        self.n_cls = self.text_features.shape[0]\n",
    "        self.text_spurious_features = get_text_embedding(self.text_spurious_embedding_dir).cuda()\n",
    "        \n",
    "    def forward(self, features, use_group=False): \n",
    "        image_features =  self.adapter(features) # Un-normalized (B, 1024)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "\n",
    "        #NOTE Joonwon Added\n",
    "        if use_group:\n",
    "            text_features = get_text_embedding(self.text_group_embedding_dir).cuda() # (Pre) Normalized (B, 2, 1024)\n",
    "        else:\n",
    "            text_features = self.text_features # (Pre) Normalized (B, 2, 1024)\n",
    "        \n",
    "        # Check if we have to normalize the text features\n",
    "        text_features = text_features / text_features.norm(dim=0, keepdim=True)\n",
    "        logits = image_features @ text_features / self.temperature # (B, 1024) X (B, C, 1024) = # (B, C)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def forward_spurious(self, features): \n",
    "        image_features =  self.adapter(features) # Un-normalized (B, 1024)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "\n",
    "        text_spurious_features = self.text_spurious_features # \n",
    "        text_spurious_features = text_spurious_features / text_spurious_features.norm(dim=0, keepdim=True) # \n",
    "        \n",
    "        \n",
    "        logits = image_features @ text_spurious_features / self.temperature # (B, 1024) X (1024, 2) = # (B, 2)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# 제대로 복사 됐는지.\n",
    "# 파라미터 Freezing(detach) 잘 되는지. \n",
    "class MultipleAdapter(nn.Module): # Adapter / Contrastive Adapter\n",
    "    def __init__(self, old_cls, new_adapter, init_near_identity=True, ebd_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.old_cls = old_cls\n",
    "        self.text_embedding_dir = self.old_cls.text_embedding_dir # 따로 짜둔 코드에서 오류 안 생기게 전부 할당.\n",
    "        self.text_spurious_embedding_dir = self.old_cls.text_spurious_embedding_dir\n",
    "        self.text_group_embedding_dir = self.old_cls.text_group_embedding_dir #NOTE Joonwon Added\n",
    "        \n",
    "        self.text_features = get_text_embedding(self.text_embedding_dir).cuda()\n",
    "        self.n_cls = self.text_features.shape[0]\n",
    "        self.text_spurious_features = get_text_embedding(self.text_spurious_embedding_dir).cuda()\n",
    "        \n",
    "        \n",
    "        self.new_adapter = new_adapter\n",
    "        self.ebd_weight = ebd_weight\n",
    "        if init_near_identity:\n",
    "            print(\"Initialize paramters of [New adapter] from [Old adapter]\")\n",
    "            \n",
    "            self.new_adapter.load_state_dict(self.old_cls.adapter.state_dict())\n",
    "            \n",
    "        self.temperature = self.old_cls.temperature\n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self, features, use_group=False): \n",
    "        old_image_features =  self.old_cls.adapter(features) # Un-normalized (B, 1024)\n",
    "        old_image_features = old_image_features / old_image_features.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "        new_image_features = self.new_adapter(features) # Un-normalized (B, 1024)\n",
    "        new_image_features = new_image_features / new_image_features.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "        \n",
    "        image_features = self.ebd_weight * old_image_features.detach() + (1 - self.ebd_weight) * new_image_features # 혹시 몰라 Detach까지.\n",
    "        \n",
    "        #NOTE Joonwon Added\n",
    "        if use_group:\n",
    "            text_features = get_text_embedding(self.text_group_embedding_dir).cuda() # (Pre) Normalized (B, 2, 1024)\n",
    "        else:\n",
    "            text_features = self.text_features # (Pre) Normalized (B, 2, 1024)\n",
    "        \n",
    "        # Check if we have to normalize the text features\n",
    "        text_features = text_features / text_features.norm(dim=0, keepdim=True)\n",
    "        \n",
    "        logits = image_features @ text_features / self.temperature # (B, 1024) X (B, C, 1024) = # (B, C)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def forward_spurious(self, features): \n",
    "        old_image_features =  self.old_cls.adapter(features) # Un-normalized (B, 1024)\n",
    "        old_image_features = old_image_features / old_image_features.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "        \n",
    "        new_image_features = self.new_adapter(features) # Un-normalized (B, 1024)\n",
    "        new_image_features = new_image_features / new_image_features.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "        \n",
    "        image_features = self.ebd_weight * old_image_features.detach() + (1 - self.ebd_weight) * new_image_features # 혹시 몰라 Detach까지.\n",
    "        \n",
    "        \n",
    "        text_spurious_features = self.text_spurious_features # \n",
    "        text_spurious_features = text_spurious_features / text_spurious_features.norm(dim=0, keepdim=True) # \n",
    "        \n",
    "\n",
    "        logits = image_features @ text_spurious_features / self.temperature # (B, 1024) X (1024, 2) = # (B, 2)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    \"\"\"\n",
    "    - Residual connetion : 제외 (original Adapter - 0.2*images + 0.8*adapter)\n",
    "    - Hidden dimension : args.adapter_feat_dim (original Adatper - input_dim // 4)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    def forward(self, features):\n",
    "        return self.layers(features)\n",
    "\n",
    "def parse_option():\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--print_freq', type=int, default=10,\n",
    "                        help='print frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=50,\n",
    "                        help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='batch_size')\n",
    "    parser.add_argument('--batch_size_reg', type=int, default=128,\n",
    "                        help='batch_size for adpater_reg')\n",
    "    parser.add_argument('--num_workers', type=int, default=16,\n",
    "                        help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=10,\n",
    "                        help='number of training epochs')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-1, \n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--learning_rate_reg', type=float, default=1e-3, \n",
    "                        help='learning rate for \"adapter_reg_seq\" model, in stage 2 ') \n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',\n",
    "                        help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=1,\n",
    "                        help='decay rate for learning rate') \n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-5,\n",
    "                        help='weight decay') # Tuning needed. \n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='momentum')\n",
    "\n",
    "    # model dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet50')\n",
    "    parser.add_argument('--dataset', type=str, default='waterbirds',\n",
    "                        choices=['celeba', 'waterbirds'], help='dataset')\n",
    "\n",
    "    # other setting\n",
    "    parser.add_argument('--cosine', action='store_true',\n",
    "                        help='using cosine annealing') # Tuning needed. \n",
    "    parser.add_argument('--warm', action='store_true',\n",
    "                        help='warm-up for large batch training') # Tuning needed. \n",
    "    parser.add_argument('--warm_reg', action='store_true',\n",
    "                        help='warm-up for stable \"adapter_reg_seq\" training') # Tuning needed. \n",
    "\n",
    "    parser.add_argument('--image_embedding_dir', type=str,\n",
    "                        help='extracted image embedding')\n",
    "    parser.add_argument('--text_embedding_dir', type=str,\n",
    "                        help='extracted text embedding')\n",
    "    parser.add_argument('--text_group_embedding_dir', type=str,\n",
    "                        help='extracted group embedding')\n",
    "    parser.add_argument('--text_spurious_embedding_dir', type=str,\n",
    "                        help='extracted text embedding (about spurious attributes)')\n",
    "    parser.add_argument('--train_target', type=str, default=\"class\", choices=[\"class\", \"spurious\", \"group\"]) # label for prediction.\n",
    "    parser.add_argument('--data_dir', type=str,\n",
    "                    help='folder, in which metadata.csv] exist')\n",
    "    parser.add_argument('--tl_method', type=str, default= \"linear_probing\", choices=[\"linear_probing\", \"adapter\", \"adapter_reg\", \"adapter_reg_seq\",\"adapter_reg_seq_alter\", \"contrastive_adapter\"]\n",
    "                        ,help='transfer learning method')\n",
    "    parser.add_argument('--balance_val', action='store_true', help=\"Balancing Val-reg loader.\") #\n",
    "    parser.add_argument('--resample_ce', action='store_true', help=\"Re-sampling Train loader.\") #\n",
    "    parser.add_argument('--use_cls_prompt_in_reg', action='store_true', help=\"True: use class-text-prompt in regularization.\") # [10, 50] in Waterbird\n",
    "    parser.add_argument('--add_adapter', action='store_true', default=False, help=\"Additional Adapter in regularization.\") # [10, 50] in Waterbird\n",
    "    parser.add_argument('--init_near_identity', action='store_true', help=\"Initialize additional adapter, making classifier' output near-identity before/after initialization \") # [10, 50] in Waterbird\n",
    "    \n",
    "    parser.add_argument('--epochs_feature_learning', type=int, help=\"epochs for feature learning in 'adapter_reg_seq'\") # [10, 50] in Waterbird\n",
    "    parser.add_argument('--continue_from_best', action='store_true', help=\"In stage 2, start from the best-worst-acc model.\")\n",
    "    parser.add_argument('--adapter_feat_dim', type=int, default= 128, help='reduced dimension in adapter')\n",
    "    parser.add_argument('--zs_temperature', type=float, default= 0.01, help='Temperature in zero-shot prediction')\n",
    "    parser.add_argument('--watch_batch_results', action='store_true', help='Print results in each bach by [opt.print_freq]. Recommdned: True when single-run of CelebA(Large # of batch), False others')\n",
    "    parser.add_argument('--save_results', action='store_true', help='Save the results of transfer learning (and final feature quality) in the folder where ')\n",
    "    \n",
    "    parser.add_argument('--num_iter', type=int, default=3, help=\"Averaging [num_iter] run, at different seed\")\n",
    "    parser.add_argument('--random_seeds', type=str, default='42,32,22', help=\"random seed for iterative training.\" )\n",
    "    parser.add_argument('--lr_multiple', type=float, default=1.0, help=\"lr multiple.\" )\n",
    "\n",
    "\n",
    "    parser.add_argument('--lr_list', type=str, default='1e-1', help='Learning rate list')\n",
    "    parser.add_argument('--bs_list', type=str, default='128', help='Batch size list')\n",
    "    parser.add_argument('--bsr_list', type=str, default='128', help='Batch size (reg) list')\n",
    "\n",
    "\n",
    "    # parser.add_argument('--lr_linear_probing', type=float, default=1e-3, chocies=[1e-3, 1e-2, 1e-1, 1, 3, 10], help='learning rate for linear probing') # Tuning needed. \n",
    "      # -> Zero-shot으로 대체하는 게 맞을듯.\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "    \n",
    "    return opt\n",
    "\n",
    "def refine_option(opt):\n",
    "\n",
    "    # set the random seeds.\n",
    "    random_seeds = opt.random_seeds.split(',')\n",
    "    opt.random_seeds = [int(seed) for seed in random_seeds]\n",
    "    print(\"random seeds : \", opt.random_seeds)\n",
    "    \n",
    "    # set the path according to the environment\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "    if opt.warm:\n",
    "        opt.warmup_from = 0.01\n",
    "        opt.warm_epochs = 10\n",
    "        if opt.cosine:\n",
    "            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "        else:\n",
    "            opt.warmup_to = opt.learning_rate\n",
    "    \n",
    "    if opt.warm_reg:\n",
    "        opt.warmup_from_reg = opt.learning_rate_reg / 1e2\n",
    "        \n",
    "        if opt.dataset =='celeba':\n",
    "            opt.warm_epochs_reg = 2\n",
    "        else:\n",
    "            opt.warm_epochs_reg = 10\n",
    "            \n",
    "        if opt.cosine:\n",
    "            eta_min = opt.learning_rate_reg * (opt.lr_decay_rate ** 3)\n",
    "            opt.warmup_to_reg = eta_min + (opt.learning_rate_reg - eta_min) * (\n",
    "                    1 + math.cos(math.pi * opt.warm_epochs_reg / (opt.epochs - opt.epochs_feature_learning))) / 2\n",
    "        else:\n",
    "            opt.warmup_to_reg = opt.learning_rate_reg\n",
    "    \n",
    "    if opt.dataset == 'celeba':\n",
    "        opt.n_cls = 2\n",
    "    elif opt.dataset == 'waterbirds':\n",
    "        opt.n_cls = 2\n",
    "    else:\n",
    "        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n",
    "\n",
    "    \n",
    "    if opt.tl_method == \"adapter\":\n",
    "        assert not opt.add_adapter\n",
    "        assert not opt.balance_val\n",
    "    return opt\n",
    "\n",
    "\n",
    "def set_model(opt):\n",
    "    # model = SupConResNet(name=opt.model)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    _ , input_dim = model_dict[opt.model] # (Encoder(not use), feature dim)\n",
    "    \n",
    "    if opt.tl_method =='linear_probing':\n",
    "        print(\"Off-the-shelf classifier : [Linear Classifier]\")\n",
    "        classifier = LinearClassifier(input_dim = input_dim, num_classes = opt.n_cls)\n",
    "    elif opt.tl_method =='adapter':\n",
    "        print(\"Off-the-shelf classifier : [Adapter + (temperatured) image-text jointly normalized prediction]\")\n",
    "        adapter = Adapter(input_dim = input_dim, hidden_dim = opt.adapter_feat_dim) # Fixed by heuristics\n",
    "        classifier = CustomCLIP(adapter, opt.text_embedding_dir, opt.text_spurious_embedding_dir, opt.text_group_embedding_dir, temperature=opt.zs_temperature)\n",
    "    elif opt.tl_method in ['adapter_reg', 'adapter_reg_seq', 'adapter_reg_seq_alter']:\n",
    "        print(\"Off-the-shelf classifier : [Adapter + (temperatured) image-text jointly normalized prediction] with group regularized training\")\n",
    "        adapter = Adapter(input_dim = input_dim, hidden_dim = opt.adapter_feat_dim) # Fixed by heuristics\n",
    "        classifier = CustomCLIP(adapter, opt.text_embedding_dir, opt.text_spurious_embedding_dir, opt.text_group_embedding_dir, temperature=opt.zs_temperature)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        classifier = classifier.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    return classifier, criterion # model, \n",
    "\n",
    "def set_model_multiple_adapter(opt, erm_classifier):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    _ , input_dim = model_dict[opt.model] # (Encoder(not use), feature dim)\n",
    "    \n",
    "    assert opt.tl_method in ['adapter_reg_seq', 'adapter_reg_seq_alter']\n",
    "    \n",
    "    print(\"================== Stage 2) New adapter for Balanced-Text-Prompt ==================\")\n",
    "    \n",
    "    new_adapter = Adapter(input_dim = input_dim, hidden_dim = opt.adapter_feat_dim) # Fixed by heuristics\n",
    "    \n",
    "    new_classifier =  MultipleAdapter(erm_classifier, new_adapter, init_near_identity=opt.init_near_identity, ebd_weight=0.5) \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        classifier = new_classifier.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    return classifier, criterion # model, \n",
    "\n",
    "\n",
    "def balance_val(val_loader, opt, print_procedure = False):\n",
    "    sub_dataset = val_loader.dataset\n",
    "    n_groups = sub_dataset.dataset.n_groups\n",
    "    g_idx = [np.where(sub_dataset.dataset.group_array[sub_dataset.indices] == g)[0] for g in range(n_groups)]\n",
    "    min_g = np.min([len(g) for g in g_idx]) \n",
    "    \n",
    "    if print_procedure:  print(\"> Before balancing (Shuffling for every epoch)\")\n",
    "    for i, g in enumerate(g_idx):\n",
    "        np.random.shuffle(g) # 같은 Group 내에 있는 sample들 Shuffle (for up-sampling)\n",
    "        if print_procedure:  print(f\"(Group {i}): \", len(g), g[:10])\n",
    "        \n",
    "        # Balancing\n",
    "        g_idx[i] = g[:min_g] \n",
    "        # print(f\"After balancing (Group {i}): \", len(g_idx[i]), g_idx[i][:10])\n",
    "\n",
    "    if print_procedure: print(\"> After balancing\")\n",
    "    [print(f\"Group {i} : {len(g)} per epoch ({g[:4]})\") for i, g in enumerate(g_idx) if print_procedure]\n",
    "    balanced_indices = list(zip(*g_idx))\n",
    "\n",
    "    balanced_indices = np.array(balanced_indices)\n",
    "    balanced_indices = balanced_indices.reshape(-1)\n",
    "    if print_procedure:  print(f\"Balanced sample indices : {len(balanced_indices)} per epoch ({balanced_indices[:16]})\")\n",
    "    \n",
    "    \n",
    "    if opt.batch_size_reg <= len(balanced_indices):\n",
    "        adjusted_batch_size_reg = opt.batch_size_reg\n",
    "    else:\n",
    "        if print_procedure: print(f\"Adjust batch size for regularizaiton : [{opt.batch_size_reg}] -> [{len(balanced_indices)}]\")\n",
    "        adjusted_batch_size_reg = len(balanced_indices)\n",
    "        \n",
    "    balanced_subset = Subset(sub_dataset, balanced_indices)\n",
    "    balanced_loader = DataLoader(balanced_subset, shuffle=False, batch_size = adjusted_batch_size_reg)\n",
    "    \n",
    "    return balanced_loader\n",
    "            \n",
    "\n",
    "# Group-wise Accuracy Update.\n",
    "def update_dict(acc_groups, y, g, logits):\n",
    "    preds = torch.argmax(logits, axis=1)\n",
    "    correct_batch = (preds == y)\n",
    "    g = g.cpu()\n",
    "    for g_val in np.unique(g):\n",
    "        mask = g == g_val\n",
    "        n = mask.sum().item()\n",
    "        corr = correct_batch[mask].sum().item()\n",
    "        acc_groups[g_val].update(corr / n, n) \n",
    "\n",
    "\n",
    "# Mean/Worst acc (not weighted average)\n",
    "def get_results(acc_groups, get_yp_func): # Input 중 acc_groups : AverageMeter()를 담고있는 dict. get_yp_func : 미리 partial을 이용해 n_groups를 저장해놓음. \n",
    "    groups = acc_groups.keys() # 0, 1, 2, 3\n",
    "    results = {\n",
    "            f\"acc_{get_yp_func(g)[0]}_{get_yp_func(g)[1]}\": acc_groups[g].avg\n",
    "            for g in groups\n",
    "    }\n",
    "    all_correct = sum([acc_groups[g].sum for g in groups])\n",
    "    all_total = sum([acc_groups[g].count for g in groups])\n",
    "    results.update({\"mean_acc\" : all_correct / all_total})\n",
    "    results.update({\"worst_acc\" : min(results.values())})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Group -> class / spurious attributes\n",
    "def get_y_p(g, n_places):\n",
    "    y = g // n_places\n",
    "    p = g % n_places\n",
    "    return y, p\n",
    "\n",
    "def get_text_embedding(text_embedding_dir):\n",
    "    with open(text_embedding_dir, 'r') as f:\n",
    "        text_embeddings = json.load(f)\n",
    "\n",
    "    text_features = []\n",
    "    for class_template, class_embedding in text_embeddings.items():\n",
    "        text_features.append(torch.tensor(class_embedding))\n",
    "    text_features = torch.stack(text_features, dim=1) # (B, 2, 1024)\n",
    "    \n",
    "    \n",
    "    return text_features\n",
    "\n",
    "def train_one_epoch(opt, train_loader, \n",
    "                    classifier, criterion, optimizer, epoch, get_yp_func, target, print_label='Train', predict_group = True): # model,\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    # model.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    acc_groups = {g_idx : AverageMeter() for g_idx in range(train_loader.dataset.n_groups)}\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, data in enumerate(train_loader):  \n",
    "        \n",
    "        embeddings, all_labels, img_filenames = data # all_labels.keys() : ['class', 'group', 'spurious', 'ebd_pred'(CLIP-zeroshot)] \n",
    "        labels = all_labels[target] # target : one of [y, spurious, group]\n",
    "        groups = all_labels['group'] # For evaluating group accuracy (and further developing group-information-aware approaches)\n",
    "    \n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        embeddings = embeddings.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # warm-up learning rate\n",
    "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
    "\n",
    "        # compute loss\n",
    "        output = classifier(embeddings.detach())  \n",
    "        loss = criterion(output, labels) \n",
    "\n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "        acc1 = accuracy(output, labels, bsz)\n",
    "        acc.update(acc1, bsz)\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Update acc dict\n",
    "        update_dict(acc_groups, labels, groups, output)\n",
    "        \n",
    "        group_acc = get_results(acc_groups, get_yp_func) # NOTE declared in [def main]\n",
    "        group_acc = {key: group_acc[key] for key in new_order_for_print[1:]}\n",
    "        group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "        \n",
    "        if opt.watch_batch_results:\n",
    "            if (idx + 1) % opt.print_freq == 0:\n",
    "                print(f'{print_label}: [{0}][{1}/{2}]\\t'\n",
    "                    'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                    'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
    "                    'Acc@1 {acc.val:.3f} ({acc.avg:.3f})\\t'\n",
    "                    'Group Acc {group_acc}'.format(\n",
    "                    epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                    data_time=data_time, loss=losses, acc=acc, group_acc = group_acc))\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "    group_acc = get_results(acc_groups, get_yp_func) # NOTE declared in [def main]\n",
    "    group_acc = {key: group_acc[key] for key in new_order_for_print[1:]}\n",
    "    group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "    print(f\"{print_label}:\", str(group_acc))\n",
    "    \n",
    "    return losses.avg, acc.avg, group_acc\n",
    "\n",
    "def train_reg_one_epoch(opt, train_loader1, train_loader2, classifier, criterion, optimizer, epoch, get_yp_func, target, group_prompt = True, print_label='Train'): # model,\n",
    "    \"\"\"one epoch training with regulalizar\"\"\"\n",
    "    # model.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    acc_groups = {g_idx : AverageMeter() for g_idx in range(train_loader1.dataset.n_groups)}\n",
    "\n",
    "    end = time.time()\n",
    "    for dataloader, use_group in zip([train_loader1, train_loader2], [False, group_prompt]):\n",
    "\n",
    "        for idx, data in enumerate(dataloader):  \n",
    "            \n",
    "            embeddings, all_labels, img_filenames = data # all_labels.keys() : ['class', 'group', 'spurious', 'ebd_pred'(CLIP-zeroshot)] \n",
    "            labels = all_labels[target] # target : one of [y, spurious, group]\n",
    "            groups = all_labels['group'] # For evaluating group accuracy (and further developing group-information-aware approaches)\n",
    "        \n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            embeddings = embeddings.cuda(non_blocking=True)\n",
    "            # NOTE joonwon added\n",
    "            if use_group is True:\n",
    "                labels = groups\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            bsz = labels.shape[0]\n",
    "\n",
    "            # warm-up learning rate\n",
    "            warmup_learning_rate(opt, epoch, idx, len(dataloader), optimizer)\n",
    "\n",
    "            # compute loss\n",
    "            output = classifier(embeddings.detach(), use_group)  \n",
    "            loss = criterion(output, labels) \n",
    "\n",
    "            # update metric\n",
    "            if use_group is False:\n",
    "                losses.update(loss.item(), bsz)\n",
    "                acc1 = accuracy(output, labels, bsz)\n",
    "                acc.update(acc1, bsz)\n",
    "\n",
    "            # SGD\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            # Update acc dict\n",
    "            if use_group is False:\n",
    "                update_dict(acc_groups, labels, groups, output)\n",
    "            \n",
    "            if opt.watch_batch_results:\n",
    "                if (idx + 1) % opt.print_freq == 0:\n",
    "                    print(f'{print_label}: [{0}][{1}/{2}]\\t'\n",
    "                        'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                        'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                        'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
    "                        'Acc@1 {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                        epoch, idx + 1, len(dataloader), batch_time=batch_time,\n",
    "                        data_time=data_time, loss=losses, acc=acc))\n",
    "                    sys.stdout.flush()\n",
    "                \n",
    "    group_acc = get_results(acc_groups, get_yp_func) # NOTE declared in [def main]\n",
    "    group_acc = {key: group_acc[key] for key in new_order_for_print[1:]}\n",
    "    group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "    print(f\"{print_label}:\", str(group_acc))\n",
    "    \n",
    "    return losses.avg, acc.avg, group_acc\n",
    "\n",
    "def train_reg_seq_one_epoch(opt, train_loader, classifier, criterion, optimizer, epoch, get_yp_func, target, print_label='Train', predict_group = True, use_group=False): # model,\n",
    "    \"\"\"one epoch training with regulalizar\"\"\"\n",
    "    # model.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    try: \n",
    "        acc_groups = {g_idx : AverageMeter() for g_idx in range(train_loader.dataset.n_groups)} # Train / Val / Test Loader\n",
    "    except:\n",
    "        try:\n",
    "            acc_groups = {g_idx : AverageMeter() for g_idx in range(train_loader.dataset.dataset.n_groups)} # Val-reg / Val-eval Loadaer\n",
    "        except:\n",
    "            # ㅋㅋㅋ \n",
    "            acc_groups = {g_idx : AverageMeter() for g_idx in range(train_loader.dataset.dataset.dataset.n_groups)} # (Balanced) Val-reg Loader \n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for idx, data in enumerate(train_loader):  \n",
    "        \n",
    "        embeddings, all_labels, img_filenames = data # all_labels.keys() : ['class', 'group', 'spurious', 'ebd_pred'(CLIP-zeroshot)] \n",
    "        labels = all_labels[target] # target : one of [y, spurious, group]\n",
    "        groups = all_labels['group'] # For evaluating group accuracy (and further developing group-information-aware approaches)\n",
    "    \n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        embeddings = embeddings.cuda(non_blocking=True)\n",
    "        # NOTE joonwon added\n",
    "        if use_group is True:\n",
    "            labels = groups\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # warm-up learning rate\n",
    "        warmup_learning_rate_reg(opt, epoch - opt.epochs_feature_learning, idx, len(train_loader), optimizer)\n",
    "\n",
    "        # compute loss\n",
    "        output = classifier(embeddings.detach(), use_group)  \n",
    "        loss = criterion(output, labels) \n",
    "        \n",
    "        \n",
    "\n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "        acc1 = accuracy(output, labels, bsz)\n",
    "        acc.update(acc1, bsz)\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Update acc dict\n",
    "        update_dict(acc_groups, labels, groups, output)\n",
    "        \n",
    "        group_acc = get_results(acc_groups, get_yp_func) # NOTE declared in [def main]\n",
    "        group_acc = {key: group_acc[key] for key in new_order_for_print[1:]}\n",
    "        group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "        \n",
    "        if opt.watch_batch_results:\n",
    "            if (idx + 1) % opt.print_freq == 0:\n",
    "                print(f'{print_label}: [{0}][{1}/{2}]\\t'\n",
    "                    'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                    'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
    "                    'Acc@1 {acc.val:.3f} ({acc.avg:.3f})\\t'\n",
    "                    'Group Acc {group_acc}'.format(\n",
    "                    epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                    data_time=data_time, loss=losses, acc=acc, group_acc = group_acc))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "    group_acc = get_results(acc_groups, get_yp_func) # NOTE declared in [def main]\n",
    "    group_acc = {key: group_acc[key] for key in new_order_for_print[1:]}\n",
    "    group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "    print(f\"{print_label}:\", str(group_acc))\n",
    "    \n",
    "    return losses.avg, acc.avg, group_acc\n",
    "\n",
    "def validate(opt, val_loader, classifier, criterion, get_yp_func, train_group_ratio, target, print_label='Test'):\n",
    "    \"\"\"validation\"\"\"\n",
    "    \n",
    "    classifier.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    try:\n",
    "        acc_groups = {g_idx : AverageMeter() for g_idx in range(val_loader.dataset.dataset.n_groups)}\n",
    "    except:\n",
    "        acc_groups = {g_idx : AverageMeter() for g_idx in range(val_loader.dataset.n_groups)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, data in enumerate(val_loader):\n",
    "            embeddings, all_labels, img_filenames = data # all_labels.keys() : ['class', 'group', 'spurious', 'ebd_pred'(CLIP-zeroshot)] \n",
    "            labels = all_labels[target] # target : one of [class, spurious, group]\n",
    "            groups = all_labels['group'] # For evaluating group accuracy (and further developing group-information-aware approaches)\n",
    "            \n",
    "            embeddings = embeddings.float().cuda()\n",
    "            labels = labels.cuda()\n",
    "            bsz = labels.shape[0]\n",
    "\n",
    "            # forward\n",
    "            output = classifier(embeddings)\n",
    "            loss = criterion(output, labels) #NOTE 준원 : validation은 그대로 두는 게 맞을듯 (class에 대해서 Training도 들어가고, 거기에 Validation 해야되는 거니까.)\n",
    "\n",
    "            # update metric\n",
    "            losses.update(loss.item(), bsz)\n",
    "            acc1 = accuracy(output, labels, bsz)\n",
    "            acc.update(acc1, bsz)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            # Update acc dict\n",
    "            update_dict(acc_groups, labels, groups, output)\n",
    "        \n",
    "            # if opt.watch_batch_results:\n",
    "            #     if (idx+1) % opt.print_freq == 0:\n",
    "            #         print(f'{print_label}: [{0}/{1}]\\t'\n",
    "            #             'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            #             'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "            #             'Acc@1 {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "            #             idx, len(val_loader), batch_time=batch_time,\n",
    "            #             loss=losses, acc=acc))\n",
    "                    \n",
    "    group_acc = get_results(acc_groups, get_yp_func)\n",
    "\n",
    "    # NOTE Add Weighted mean acc.\n",
    "    try:\n",
    "        groups = range(val_loader.dataset.dataset.n_groups) # 0, 1, 2, 3\n",
    "    except:\n",
    "        groups = range(val_loader.dataset.n_groups) # 0, 1, 2, 3\n",
    "    group_acc_indiv =  [group_acc[f\"acc_{get_yp_func(g)[0]}_{get_yp_func(g)[1]}\"] for g in groups]\n",
    "    weighted_mean_acc = (np.array(group_acc_indiv) * np.array(train_group_ratio)).sum() # Weighted Sum \\\n",
    "    \n",
    "    group_acc[\"weighted_mean_acc\"] = weighted_mean_acc\n",
    "    group_acc = {key: group_acc[key] for key in new_order_for_print}\n",
    "    group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "    print(f\"{print_label}:\", str(group_acc))\n",
    "\n",
    "    return losses.avg, acc.avg, group_acc    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def validate_zs(opt, val_loader, classifier, criterion, get_yp_func, train_group_ratio, target, print_label='Zero-shot Prediction (Test) (Class)'):\n",
    "    \"\"\"(Feature quality) validation using zeroshot-prediction\"\"\"\n",
    "\n",
    "    classifier.eval()\n",
    "\n",
    "    if opt.tl_method in [\"linear_probing\"]:\n",
    "        temperature = opt.zs_temperature\n",
    "        \n",
    "        if target==\"class\":\n",
    "            text_embeddings = get_text_embedding(opt.text_embedding_dir)\n",
    "        elif target=='spurious':\n",
    "            text_embeddings = get_text_embedding(opt.text_spurious_embedding_dir)\n",
    "        text_embeddings = text_embeddings.cuda()\n",
    "        text_embeddings = text_embeddings / text_embeddings.norm(dim=0, keepdim=True)\n",
    "        \n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    acc_groups = {g_idx : AverageMeter() for g_idx in range(val_loader.dataset.n_groups)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, data in enumerate(val_loader):\n",
    "            image_embeddings, all_labels, img_filenames = data # all_labels.keys() : ['class', 'group', 'spurious', 'ebd_pred'(CLIP-zeroshot)] \n",
    "            labels = all_labels[target] # target : one of [class, spurious, group]\n",
    "            groups = all_labels['group'] # For evaluating group accuracy (and further developing group-information-aware approaches)\n",
    "            \n",
    "            \n",
    "            image_embeddings = image_embeddings.float().cuda()\n",
    "            labels = labels.cuda()\n",
    "            bsz = labels.shape[0]\n",
    "            \n",
    "            if opt.tl_method in ['linear_probing']: # same to CLIP Embedding\n",
    "                image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "                output = image_embeddings @ text_embeddings / temperature # (B, 1024) X (B, 2, 1024) = # (B, 2)\n",
    "                \n",
    "            elif opt.tl_method in ['adapter', 'adapter_reg', 'adapter_reg_seq', 'adapter_reg_seq_alter', 'contrastive_adapter']: # Adpater, Contrastive Adapter : Embedding -> (1) (Adapted) Embedding -> (2) ZeroShot prediction as logit    (CustomCLIP.forward : (1)+(2))\n",
    "                # forward\n",
    "                if target=='class':\n",
    "                    output = classifier(image_embeddings)\n",
    "                elif target=='spurious':\n",
    "                    output = classifier.forward_spurious(image_embeddings)\n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # update metric\n",
    "            losses.update(loss.item(), bsz)\n",
    "            acc1 = accuracy(output, labels, bsz)\n",
    "            acc.update(acc1, bsz)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            # Update acc dict\n",
    "            update_dict(acc_groups, labels, groups, output)\n",
    "        \n",
    "            # if opt.watch_batch_results:\n",
    "            #     if (idx+1) % opt.print_freq == 0:\n",
    "            #         print(f'{print_label}: [{0}/{1}]\\t'\n",
    "            #             'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            #             'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "            #             'Acc@1 {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "            #             idx, len(val_loader), batch_time=batch_time,\n",
    "            #             loss=losses, acc=acc))\n",
    "                    \n",
    "    group_acc = get_results(acc_groups, get_yp_func)\n",
    "\n",
    "    # NOTE Add Weighted mean acc.\n",
    "    groups = range(val_loader.dataset.n_groups) # 0, 1, 2, 3\n",
    "    group_acc_indiv =  [group_acc[f\"acc_{get_yp_func(g)[0]}_{get_yp_func(g)[1]}\"] for g in groups]\n",
    "    weighted_mean_acc = (np.array(group_acc_indiv) * np.array(train_group_ratio)).sum() # Weighted Sum \\\n",
    "    \n",
    "    group_acc[\"weighted_mean_acc\"] = weighted_mean_acc\n",
    "    group_acc = {key: group_acc[key] for key in new_order_for_print}\n",
    "    group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "    print(f\"{print_label}:\", str(group_acc))\n",
    "    \n",
    "    return losses.avg, acc.avg, group_acc    \n",
    "\n",
    "def train_all_epochs(opt):\n",
    "    best_acc = 0\n",
    "    best_epoch = 0\n",
    "    best_model = None\n",
    "    # opt = parse_option()    \n",
    "    \n",
    "    if opt.tl_method in [\"adapter_reg\", \"adapter_reg_seq\", 'adapter_reg_seq_alter'] :\n",
    "        print(f\"====== TL:[{opt.tl_method}] LR:[{opt.learning_rate}] BS:[{opt.batch_size}] BSr:[{opt.batch_size_reg}] ======\")\n",
    "        reg_loader = deepcopy(opt.reg_loader)\n",
    "    else:\n",
    "        print(f\"====== TL:[{opt.tl_method}] LR:[{opt.learning_rate}] BS:[{opt.batch_size}]======\")\n",
    "    \n",
    "    print(\"> Simply copy the data loader ... \")\n",
    "    # train_all_epochs 함수에서 다시 가져온다.\n",
    "    # trainset = opt.trainset\n",
    "    # train_loader = opt.train_loader\n",
    "    # val_loader = opt.val_loader\n",
    "    # test_loader = opt.test_loader\n",
    "    \n",
    "    # for Balancing Validation Loader.\n",
    "    if opt.balance_val and (opt.tl_method in [\"adapter_reg\", \"adapter_reg_seq\", 'adapter_reg_seq_alter']):\n",
    "        print(\"Using [Balanced] Validation loader for regularized training\")\n",
    "        origin_reg_loader = opt.reg_loader # -> From this loader, sampling balanced validation dataset for every epoch.\n",
    "        \n",
    "    if opt.resample_ce:\n",
    "        print(\"Using [Resampled] Train loader for erm/feature laerning\")\n",
    "        opt.correct_class_bias = True\n",
    "        opt.reweighting_by_class = False\n",
    "        from visualizer_supcon import compute_slice_indices, prepare_contrastive_points, GetNegativesByClass, GetResampledWeightsCE\n",
    "        from torch.utils.data.sampler import WeightedRandomSampler\n",
    "        sliced_data_indices, sliced_data_correct = compute_slice_indices(opt, opt.trainset)\n",
    "        contrastive_points = prepare_contrastive_points(opt.trainset,sliced_data_indices,sliced_data_correct)\n",
    "        _, _, positives_by_class, _ = contrastive_points\n",
    "    \n",
    "        negatives_by_class = GetNegativesByClass(opt, opt.trainset, positives_by_class)\n",
    "        weights_resampled_ce = GetResampledWeightsCE(opt.trainset, positives_by_class, negatives_by_class, opt)\n",
    "        ce_sampler = WeightedRandomSampler(weights = weights_resampled_ce, num_samples = len(opt.trainset), replacement=True) # num_samples = len(trainset) -> oversampling 한 만큼 major group에서 unseen-sample 나옴\n",
    "        resampled_train_loader = DataLoader(opt.trainset, sampler=ce_sampler, batch_size=opt.batch_size, num_workers=16)\n",
    "    \n",
    "    # group information\n",
    "    get_yp_func = partial(get_y_p, n_places=opt.trainset.n_places)\n",
    "    train_group_ratio = opt.trainset.group_ratio\n",
    "    \n",
    "    # build model and criterion\n",
    "\n",
    "    classifier, criterion = set_model(opt) # model,  # CE\n",
    "    # cl_loss = # Contrastive adpater\n",
    "\n",
    "    # build optimizer\n",
    "    print(\"Set Optimizer: SGD (default)\")\n",
    "    print('========================================================================')\n",
    "    optimizer = set_optimizer(opt, classifier)\n",
    "    \n",
    "    # training routine\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    train_group_accs = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_group_accs = []\n",
    "    \n",
    "    test_losses = [] # NOTE: Don't peek ! \n",
    "    test_accs = [] # NOTE: Don't peek ! \n",
    "    test_group_accs = [] # NOTE: Don't peek ! \n",
    "    \n",
    "    # entire training\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "        adjust_learning_rate(opt, optimizer, epoch)\n",
    "        print(f'--- Epoch {epoch} ---')\n",
    "        \n",
    "        # Sample Balanced Validation dataset.\n",
    "        if opt.balance_val and (opt.tl_method in [\"adapter_reg\", \"adapter_reg_seq\", 'adapter_reg_seq_alter']):\n",
    "            reg_loader = balance_val(origin_reg_loader, opt, print_procedure=False)\n",
    "        \n",
    "        # train one epoch\n",
    "        if opt.tl_method == \"adapter_reg\":\n",
    "            # Alternative Training\n",
    "            if opt.use_cls_prompt_in_reg:\n",
    "                loss, acc, group_acc = train_reg_one_epoch(opt, opt.train_loader, reg_loader, classifier, criterion, \n",
    "                                                            optimizer, epoch, get_yp_func, target=opt.train_target, group_prompt = False, print_label=f'Train (Alternative Learning)(Class prompt)')\n",
    "            else:\n",
    "                loss, acc, group_acc = train_reg_one_epoch(opt, opt.train_loader, reg_loader, classifier, criterion, \n",
    "                                                            optimizer, epoch, get_yp_func, target=opt.train_target, group_prompt = True, print_label=f'Train (Alternative Learning)(Group prompt)')\n",
    "        elif opt.tl_method in  [\"adapter_reg_seq\", \"adapter_reg_seq_alter\"]:\n",
    "            if epoch <= opt.epochs_feature_learning:\n",
    "            # Sequetional Training\n",
    "                loss, acc, group_acc = train_one_epoch(opt, opt.train_loader, classifier, criterion, \n",
    "                                                        optimizer, epoch, get_yp_func, target=opt.train_target, print_label=f'Train-1 (Feature Learning)')\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                if epoch == opt.epochs_feature_learning + 1:\n",
    "                    if opt.continue_from_best:\n",
    "                        print(\"Load Best (Worst-acc) Model.\")\n",
    "                        classifier = deepcopy(best_model)\n",
    "                    \n",
    "                    if opt.add_adapter:\n",
    "                        multiple_adapter, criterion = set_model_multiple_adapter(opt, classifier) # model,  # CE\n",
    "                        optimizer_reg = set_optimizer_reg(opt, multiple_adapter) # Set new optimzer.\n",
    "                        \n",
    "                    else:\n",
    "                        optimizer_reg = set_optimizer_reg(opt, classifier) # Set new optimzer.\n",
    "                    \n",
    "                adjust_learning_rate_reg(opt, optimizer_reg, epoch)\n",
    "                \n",
    "                if opt.tl_method == \"adapter_reg_seq_alter\":\n",
    "                    if not opt.add_adapter:\n",
    "                        if (epoch % 2) == 1:\n",
    "                            loss, acc, group_acc = train_reg_seq_one_epoch(opt, reg_loader, classifier, criterion, \n",
    "                                                                    optimizer_reg, epoch, get_yp_func, target=opt.train_target, print_label=f'Train-2 (Balanced Learning)(Class prompt)', use_group=False)\n",
    "                        else:\n",
    "                            loss, acc, group_acc = train_reg_seq_one_epoch(opt, reg_loader, classifier, criterion, \n",
    "                                                                    optimizer_reg, epoch, get_yp_func, target=opt.train_target, print_label=f'Train-2 (Balanced Learning)(Group prompt)', use_group=True)\n",
    "                    else:  \n",
    "                        if (epoch % 2) == 1:\n",
    "                            loss, acc, group_acc = train_reg_seq_one_epoch(opt, reg_loader, multiple_adapter, criterion, \n",
    "                                                                    optimizer_reg, epoch, get_yp_func, target=opt.train_target, print_label=f'Train-2 (Balanced Learning)(new adapter)(Class prompt)', use_group=False)\n",
    "                        else:\n",
    "                            loss, acc, group_acc = train_reg_seq_one_epoch(opt, reg_loader, multiple_adapter, criterion, \n",
    "                                                                    optimizer_reg, epoch, get_yp_func, target=opt.train_target, print_label=f'Train-2 (Balanced Learning)(new adapter)(Group prompt)', use_group=True)\n",
    "                    \n",
    "                else:\n",
    "                    if not opt.add_adapter:\n",
    "                        if not opt.use_cls_prompt_in_reg:\n",
    "                            loss, acc, group_acc = train_reg_seq_one_epoch(opt, reg_loader, classifier, criterion, \n",
    "                                                                    optimizer_reg, epoch, get_yp_func, target=opt.train_target, print_label=f'Train-2 (Balanced Learning)(Group prompt)', use_group=True)\n",
    "                        else:\n",
    "                            loss, acc, group_acc = train_reg_seq_one_epoch(opt, reg_loader, classifier, criterion, \n",
    "                                                                    optimizer_reg, epoch, get_yp_func, target=opt.train_target, print_label=f'Train-2 (Balanced Learning)(Class prompt)', use_group=False)\n",
    "                    else:\n",
    "                        if not opt.use_cls_prompt_in_reg:\n",
    "                            loss, acc, group_acc = train_reg_seq_one_epoch(opt, reg_loader, multiple_adapter, criterion, \n",
    "                                                                    optimizer_reg, epoch, get_yp_func, target=opt.train_target, print_label=f'Train-2 (Balanced Learning)(new adapter)(Group prompt)', use_group=True)\n",
    "                        else:\n",
    "                            loss, acc, group_acc = train_reg_seq_one_epoch(opt, reg_loader, multiple_adapter, criterion, \n",
    "                                                                    optimizer_reg, epoch, get_yp_func, target=opt.train_target, print_label=f'Train-2 (Balanced Learning)(new adapter)(Class prompt)', use_group=False)\n",
    "                            \n",
    "        else:\n",
    "            loss, acc, group_acc = train_one_epoch(opt, opt.train_loader, classifier, criterion,\n",
    "                          optimizer, epoch, get_yp_func, target=opt.train_target, print_label=f'Train({opt.train_target})')\n",
    "        train_losses.append(loss); train_accs.append(acc); train_group_accs.append(group_acc)\n",
    "        \n",
    "        # eval for one epoch\n",
    "        \n",
    "        if opt.add_adapter and epoch > opt.epochs_feature_learning:\n",
    "            val_loss, val_acc, val_group_acc = validate(opt, opt.val_loader, multiple_adapter, criterion, get_yp_func, train_group_ratio, target=opt.train_target, print_label=f'Val({opt.train_target})(new adapter)')\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc); val_group_accs.append(val_group_acc)\n",
    "        else:\n",
    "            val_loss, val_acc, val_group_acc = validate(opt, opt.val_loader, classifier, criterion, get_yp_func, train_group_ratio, target=opt.train_target, print_label=f'Val({opt.train_target})')\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc); val_group_accs.append(val_group_acc)\n",
    "            \n",
    "        # update best epoch by worst_group accuracy (default)\n",
    "        if val_group_acc['worst_acc'] > best_acc:\n",
    "            best_acc = val_group_acc['worst_acc']\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            if opt.add_adapter and epoch > opt.epochs_feature_learning:\n",
    "                best_model = deepcopy(multiple_adapter)\n",
    "            else:\n",
    "                best_model = deepcopy(classifier)\n",
    "                \n",
    "        \n",
    "        # test for one epoch\n",
    "        if opt.add_adapter and epoch > opt.epochs_feature_learning:\n",
    "            test_loss, test_acc, test_group_acc = validate(opt, opt.test_loader, multiple_adapter, criterion, get_yp_func, train_group_ratio, target='class', print_label=f'Test({opt.train_target})(new adapter)')\n",
    "            test_losses.append(test_loss); test_accs.append(test_acc); test_group_accs.append(test_group_acc)\n",
    "        else:\n",
    "            test_loss, test_acc, test_group_acc = validate(opt, opt.test_loader, classifier, criterion, get_yp_func, train_group_ratio, target='class', print_label=f'Test({opt.train_target})')\n",
    "            test_losses.append(test_loss); test_accs.append(test_acc); test_group_accs.append(test_group_acc)\n",
    "\n",
    "    print('========================================================================')\n",
    "    print(\"> end of training. \\n\")\n",
    "    print('best epoch : {}'.format(best_epoch))\n",
    "    \n",
    "    best_train_group_acc = train_group_accs[best_epoch-1]\n",
    "    best_val_group_acc = val_group_accs[best_epoch-1]\n",
    "    best_test_group_acc = test_group_accs[best_epoch-1]\n",
    "    \n",
    "    print(f'best training accuracy on [{opt.train_target}]: {best_train_group_acc}')\n",
    "    print(f'best validation accuracy on [{opt.train_target}]: {best_val_group_acc}')\n",
    "    print(f'best test accuracy on [{opt.train_target}]: {best_test_group_acc}')\n",
    "    \n",
    "    # Evaluate Feature Quality using (Embedding-based) Zero-shot Prediction\n",
    "    print('========================================================================')\n",
    "    print(\"> start evaluating feature quality of best model. (using zero-shot prediction)\\n\")\n",
    "    \n",
    "    \n",
    "    # Zero-shot [class] prediction\n",
    "    zs_loss, zs_acc, zs_group_acc = validate_zs(opt, opt.test_loader, best_model, criterion, get_yp_func, train_group_ratio, target=\"class\", print_label='zero-shot prediction (test) (class)')    \n",
    "    \n",
    "    if opt.tl_method in [\"linear_probing\"]:\n",
    "        print(f\" ㄴ Note that it should be same to [CLIP Zero-shot Baselines, of which worst acc is about 39%], in {opt.tl_method}\")\n",
    "    elif opt.tl_method in [\"adapter\", \"contrastive_adapt\"]: \n",
    "        print(f\" ㄴ Note that it should be same to [best test accuracy on [{opt.train_target}]], above, in {opt.tl_method}\")\n",
    "    \n",
    "    # Zero-shot [spurious] prediction \n",
    "    zs_loss_spurious, zs_acc_spurious, zs_group_acc_spurious = validate_zs(opt, opt.test_loader, best_model, criterion, get_yp_func, train_group_ratio, target=\"spurious\", print_label='zero-shot prediction (test) (spurious)')    \n",
    "    print(f\" ㄴ Note that it is related to [richness of non-target (spurious) information] (-> 'mean_acc' is important)\")\n",
    "    \n",
    "    print('========================================================================')\n",
    "    # Recommendation : False when multiple training\n",
    "    if opt.save_results and opt.num_iter == 1 :\n",
    "        print('> Save results\\n')\n",
    "        all_results = {}\n",
    "        \n",
    "        for epoch in range(1, opt.epochs + 1):\n",
    "            all_results[f\"Epoch {epoch}\"] = {\"Train\": train_group_accs[epoch-1], \"Val\": test_group_accs[epoch-1], \"Test\": test_group_accs[epoch-1]}\n",
    "        \n",
    "        final_results = {\"Final Results (best epoch)\":  {f\"Epoch {best_epoch}\": {\"Train\": best_train_group_acc, \"Val\": best_val_group_acc, \"Test\": best_test_group_acc}}, \n",
    "                         \"Feature Quality (using zs)\":  {\"class\":  zs_group_acc, \"spurious\": zs_group_acc_spurious}, \n",
    "                         \"All Results (all epoch)\": all_results}\n",
    "        \n",
    "        # make result folder \n",
    "        final_result_folder = os.path.dirname(opt.image_embedding_dir).replace('data', 'results')\n",
    "        if not os.path.exists(final_result_folder):\n",
    "            os.makedirs(final_result_folder)\n",
    "            \n",
    "        image_ebd_file_name = os.path.basename(opt.image_embedding_dir).split(\".\")[0]\n",
    "        text_ebd_file_name = os.path.basename(opt.text_embedding_dir).split(\".\")[0]\n",
    "        \n",
    "        # result name\n",
    "        final_result_file_name = f\"im_{image_ebd_file_name}_t_{text_ebd_file_name}_tl_{opt.tl_method}_t_{opt.train_target}_lr_{opt.learning_rate}_bs_{opt.batch_size}\"\n",
    "        \n",
    "        if \"reg\" in opt.tl_method:\n",
    "            final_result_file_name += f\"_lrr{opt.learning_rate_reg}_bsr_{opt.batch_size_reg}\"\n",
    "\n",
    "            if opt.balance_val:\n",
    "                final_result_file_name += \"_balval\"\n",
    "            \n",
    "            \n",
    "            if opt.tl_method != \"adapter_reg_seq_alter\": # Alter : GP <-> CP Alternative Training in Stage 2.\n",
    "                if opt.use_cls_prompt_in_reg:\n",
    "                    final_result_file_name+=\"_CP\"\n",
    "                else:\n",
    "                    final_result_file_name+=\"_GP\"\n",
    "                \n",
    "            if opt.add_adapter:\n",
    "                final_result_file_name+=\"_MA\"\n",
    "                if not opt.init_near_identity:\n",
    "                    final_result_file_name+=\"+rdm\"\n",
    "            \n",
    "            if opt.continue_from_best and ('seq' in opt.tl_method):\n",
    "                final_result_file_name+=\"_cont\"\n",
    "        \n",
    "                 \n",
    "        # NOTE This file name can be modified when we add baselines\n",
    "        \"\"\"\n",
    "        E.g., if we use [flexable_adapter] and corresponding h.p. [flexable_weight], then, \n",
    "        if opt.tl_method == \"flexable_adpater\":\n",
    "            final_result_file_name += f\"_{opt.flexable_weight}\"\n",
    "        if opt.cosine:\n",
    "            opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "        if opt.warm:\n",
    "            opt.model_name = '{}_warm'.format(opt.model_name)\n",
    "        \"\"\"\n",
    "\n",
    "        # result path\n",
    "        final_result_file_path = os.path.join(final_result_folder, final_result_file_name + \".json\")\n",
    "        final_model_path = os.path.join(final_result_folder, final_result_file_name + \".pth\")\n",
    "        \n",
    "        print('final result path: ', final_result_file_path)\n",
    "        print('final model path: ', final_model_path)\n",
    "        \n",
    "        # save results, as json.\n",
    "        with open(final_result_file_path, \"w\") as f:\n",
    "            json.dump(final_results, f, indent=4)\n",
    "        \n",
    "        # save final model, as pth \n",
    "        torch.save(best_model.state_dict(), final_model_path)    \n",
    "            \n",
    "    \n",
    "    print('========================================================================')\n",
    "    print(\"> end\")\n",
    "    # del trainset; del train_loader; del val_loader; del test_loader\n",
    "    # if opt.tl_method in [\"adapter_reg\", \"adapter_reg_seq\", 'adapter_reg_seq_alter'] :\n",
    "    #     del reg_loader\n",
    "    \n",
    "    return (best_train_group_acc, best_val_group_acc, best_test_group_acc), (zs_group_acc, zs_group_acc_spurious)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seeds :  [42, 32, 22]\n",
      "> Start Transfer Learning using [adapter_reg_seq_alter]\n",
      "========================================================================\n",
      "Load embedding of CelebA: data/embeddings_unnormalized/celeba/RN50/clip.json\n",
      "Load Data Loader (train, validation, test)\n",
      "Load embedding of CelebA: data/embeddings_unnormalized/celeba/RN50/clip.json\n",
      "Load Data Loader (train, validation, test)\n",
      "Training target : class (non-blond hair(0) / blond hair(1))\n",
      "f=============Iteration : 1/2=============\n",
      "====== TL:[adapter_reg_seq_alter] LR:[0.1] BS:[1024] BSr:[4] ======\n",
      "> Simply copy the data loader ... \n",
      "Using [Balanced] Validation loader for regularized training\n",
      "Off-the-shelf classifier : [Adapter + (temperatured) image-text jointly normalized prediction] with group regularized training\n",
      "Set Optimizer: SGD (default)\n",
      "========================================================================\n",
      "--- Epoch 1 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.3079, 'acc_0_0': 0.9458, 'acc_0_1': 0.9944, 'acc_1_0': 0.8455, 'acc_1_1': 0.3079, 'mean_acc': 0.9462}\n",
      "Val(class): {'weighted_mean_acc': 0.9516, 'worst_acc': 0.2308, 'acc_0_0': 0.9634, 'acc_0_1': 0.9973, 'acc_1_0': 0.8246, 'acc_1_1': 0.2308, 'mean_acc': 0.9508}\n",
      "Test(class): {'weighted_mean_acc': 0.9509, 'worst_acc': 0.2722, 'acc_0_0': 0.9671, 'acc_0_1': 0.9969, 'acc_1_0': 0.8065, 'acc_1_1': 0.2722, 'mean_acc': 0.9522}\n",
      "--- Epoch 2 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.336, 'acc_0_0': 0.9515, 'acc_0_1': 0.9955, 'acc_1_0': 0.856, 'acc_1_1': 0.336, 'mean_acc': 0.9509}\n",
      "Val(class): {'weighted_mean_acc': 0.9492, 'worst_acc': 0.2198, 'acc_0_0': 0.9625, 'acc_0_1': 0.9976, 'acc_1_0': 0.81, 'acc_1_1': 0.2198, 'mean_acc': 0.9483}\n",
      "Test(class): {'weighted_mean_acc': 0.9519, 'worst_acc': 0.2722, 'acc_0_0': 0.9673, 'acc_0_1': 0.9977, 'acc_1_0': 0.8109, 'acc_1_1': 0.2722, 'mean_acc': 0.9531}\n",
      "--- Epoch 3 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.3663, 'acc_0_0': 0.9539, 'acc_0_1': 0.9958, 'acc_1_0': 0.8619, 'acc_1_1': 0.3663, 'mean_acc': 0.9532}\n",
      "Val(class): {'weighted_mean_acc': 0.9434, 'worst_acc': 0.2967, 'acc_0_0': 0.9096, 'acc_0_1': 0.9966, 'acc_1_0': 0.9332, 'acc_1_1': 0.2967, 'mean_acc': 0.9436}\n",
      "Test(class): {'weighted_mean_acc': 0.9492, 'worst_acc': 0.3389, 'acc_0_0': 0.9258, 'acc_0_1': 0.9955, 'acc_1_0': 0.9242, 'acc_1_1': 0.3389, 'mean_acc': 0.9466}\n",
      "--- Epoch 4 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.372, 'acc_0_0': 0.9548, 'acc_0_1': 0.9956, 'acc_1_0': 0.862, 'acc_1_1': 0.372, 'mean_acc': 0.9535}\n",
      "Val(class): {'weighted_mean_acc': 0.9533, 'worst_acc': 0.4615, 'acc_0_0': 0.9431, 'acc_0_1': 0.9942, 'acc_1_0': 0.8956, 'acc_1_1': 0.4615, 'mean_acc': 0.9531}\n",
      "Test(class): {'weighted_mean_acc': 0.9545, 'worst_acc': 0.4389, 'acc_0_0': 0.9478, 'acc_0_1': 0.994, 'acc_1_0': 0.8911, 'acc_1_1': 0.4389, 'mean_acc': 0.9536}\n",
      "--- Epoch 5 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.3958, 'acc_0_0': 0.9553, 'acc_0_1': 0.9955, 'acc_1_0': 0.8641, 'acc_1_1': 0.3958, 'mean_acc': 0.9542}\n",
      "Val(class): {'weighted_mean_acc': 0.9535, 'worst_acc': 0.2857, 'acc_0_0': 0.9499, 'acc_0_1': 0.9966, 'acc_1_0': 0.8796, 'acc_1_1': 0.2857, 'mean_acc': 0.9531}\n",
      "Test(class): {'weighted_mean_acc': 0.9545, 'worst_acc': 0.3111, 'acc_0_0': 0.9542, 'acc_0_1': 0.9971, 'acc_1_0': 0.8698, 'acc_1_1': 0.3111, 'mean_acc': 0.9541}\n",
      "--- Epoch 6 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4189, 'acc_0_0': 0.956, 'acc_0_1': 0.9956, 'acc_1_0': 0.8632, 'acc_1_1': 0.4189, 'mean_acc': 0.9546}\n",
      "Val(class): {'weighted_mean_acc': 0.9541, 'worst_acc': 0.1538, 'acc_0_0': 0.9597, 'acc_0_1': 0.9986, 'acc_1_0': 0.8553, 'acc_1_1': 0.1538, 'mean_acc': 0.9534}\n",
      "Test(class): {'weighted_mean_acc': 0.9551, 'worst_acc': 0.2667, 'acc_0_0': 0.9621, 'acc_0_1': 0.9981, 'acc_1_0': 0.8488, 'acc_1_1': 0.2667, 'mean_acc': 0.9554}\n",
      "--- Epoch 7 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4247, 'acc_0_0': 0.9571, 'acc_0_1': 0.9958, 'acc_1_0': 0.866, 'acc_1_1': 0.4247, 'mean_acc': 0.9556}\n",
      "Val(class): {'weighted_mean_acc': 0.9521, 'worst_acc': 0.3187, 'acc_0_0': 0.9541, 'acc_0_1': 0.9964, 'acc_1_0': 0.8553, 'acc_1_1': 0.3187, 'mean_acc': 0.9516}\n",
      "Test(class): {'weighted_mean_acc': 0.9544, 'worst_acc': 0.3611, 'acc_0_0': 0.9595, 'acc_0_1': 0.9962, 'acc_1_0': 0.8524, 'acc_1_1': 0.3611, 'mean_acc': 0.9546}\n",
      "--- Epoch 8 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4463, 'acc_0_0': 0.9579, 'acc_0_1': 0.9956, 'acc_1_0': 0.868, 'acc_1_1': 0.4463, 'mean_acc': 0.9564}\n",
      "Val(class): {'weighted_mean_acc': 0.9541, 'worst_acc': 0.2967, 'acc_0_0': 0.9569, 'acc_0_1': 0.9976, 'acc_1_0': 0.858, 'acc_1_1': 0.2967, 'mean_acc': 0.9535}\n",
      "Test(class): {'weighted_mean_acc': 0.9536, 'worst_acc': 0.3944, 'acc_0_0': 0.9578, 'acc_0_1': 0.9955, 'acc_1_0': 0.852, 'acc_1_1': 0.3944, 'mean_acc': 0.9538}\n",
      "--- Epoch 9 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4506, 'acc_0_0': 0.9578, 'acc_0_1': 0.9955, 'acc_1_0': 0.8725, 'acc_1_1': 0.4506, 'mean_acc': 0.957}\n",
      "Val(class): {'weighted_mean_acc': 0.95, 'worst_acc': 0.2527, 'acc_0_0': 0.9698, 'acc_0_1': 0.9966, 'acc_1_0': 0.794, 'acc_1_1': 0.2527, 'mean_acc': 0.949}\n",
      "Test(class): {'weighted_mean_acc': 0.9501, 'worst_acc': 0.2889, 'acc_0_0': 0.9728, 'acc_0_1': 0.9976, 'acc_1_0': 0.7802, 'acc_1_1': 0.2889, 'mean_acc': 0.9521}\n",
      "--- Epoch 10 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.447, 'acc_0_0': 0.9587, 'acc_0_1': 0.9956, 'acc_1_0': 0.8716, 'acc_1_1': 0.447, 'mean_acc': 0.9573}\n",
      "Val(class): {'weighted_mean_acc': 0.9553, 'worst_acc': 0.2747, 'acc_0_0': 0.9578, 'acc_0_1': 0.9983, 'acc_1_0': 0.8629, 'acc_1_1': 0.2747, 'mean_acc': 0.9547}\n",
      "Test(class): {'weighted_mean_acc': 0.9541, 'worst_acc': 0.3056, 'acc_0_0': 0.9601, 'acc_0_1': 0.9964, 'acc_1_0': 0.8508, 'acc_1_1': 0.3056, 'mean_acc': 0.9543}\n",
      "--- Epoch 11 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4845, 'acc_0_0': 0.9594, 'acc_0_1': 0.9959, 'acc_1_0': 0.8753, 'acc_1_1': 0.4845, 'mean_acc': 0.9586}\n",
      "Val(class): {'weighted_mean_acc': 0.9519, 'worst_acc': 0.2308, 'acc_0_0': 0.9616, 'acc_0_1': 0.9983, 'acc_1_0': 0.8295, 'acc_1_1': 0.2308, 'mean_acc': 0.9511}\n",
      "Test(class): {'weighted_mean_acc': 0.9536, 'worst_acc': 0.3444, 'acc_0_0': 0.9669, 'acc_0_1': 0.9969, 'acc_1_0': 0.8222, 'acc_1_1': 0.3444, 'mean_acc': 0.9547}\n",
      "--- Epoch 12 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4845, 'acc_0_0': 0.9597, 'acc_0_1': 0.9956, 'acc_1_0': 0.874, 'acc_1_1': 0.4845, 'mean_acc': 0.9584}\n",
      "Val(class): {'weighted_mean_acc': 0.952, 'worst_acc': 0.3956, 'acc_0_0': 0.9503, 'acc_0_1': 0.9954, 'acc_1_0': 0.8643, 'acc_1_1': 0.3956, 'mean_acc': 0.9516}\n",
      "Test(class): {'weighted_mean_acc': 0.9536, 'worst_acc': 0.4222, 'acc_0_0': 0.9568, 'acc_0_1': 0.9935, 'acc_1_0': 0.8593, 'acc_1_1': 0.4222, 'mean_acc': 0.9537}\n",
      "--- Epoch 13 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5076, 'acc_0_0': 0.9604, 'acc_0_1': 0.9957, 'acc_1_0': 0.8768, 'acc_1_1': 0.5076, 'mean_acc': 0.9593}\n",
      "Val(class): {'weighted_mean_acc': 0.9524, 'worst_acc': 0.4945, 'acc_0_0': 0.9496, 'acc_0_1': 0.9935, 'acc_1_0': 0.8685, 'acc_1_1': 0.4945, 'mean_acc': 0.952}\n",
      "Test(class): {'weighted_mean_acc': 0.9543, 'worst_acc': 0.45, 'acc_0_0': 0.9566, 'acc_0_1': 0.9926, 'acc_1_0': 0.8657, 'acc_1_1': 0.45, 'mean_acc': 0.9543}\n",
      "--- Epoch 14 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5133, 'acc_0_0': 0.9598, 'acc_0_1': 0.9957, 'acc_1_0': 0.8791, 'acc_1_1': 0.5133, 'mean_acc': 0.9594}\n",
      "Val(class): {'weighted_mean_acc': 0.9517, 'worst_acc': 0.3297, 'acc_0_0': 0.9536, 'acc_0_1': 0.9969, 'acc_1_0': 0.8518, 'acc_1_1': 0.3297, 'mean_acc': 0.9512}\n",
      "Test(class): {'weighted_mean_acc': 0.9529, 'worst_acc': 0.3556, 'acc_0_0': 0.9588, 'acc_0_1': 0.9955, 'acc_1_0': 0.846, 'acc_1_1': 0.3556, 'mean_acc': 0.9532}\n",
      "--- Epoch 15 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.54, 'acc_0_0': 0.9608, 'acc_0_1': 0.9956, 'acc_1_0': 0.8799, 'acc_1_1': 0.54, 'mean_acc': 0.9602}\n",
      "Val(class): {'weighted_mean_acc': 0.9497, 'worst_acc': 0.3297, 'acc_0_0': 0.9677, 'acc_0_1': 0.9964, 'acc_1_0': 0.7947, 'acc_1_1': 0.3297, 'mean_acc': 0.9488}\n",
      "Test(class): {'weighted_mean_acc': 0.9491, 'worst_acc': 0.3611, 'acc_0_0': 0.9699, 'acc_0_1': 0.995, 'acc_1_0': 0.7859, 'acc_1_1': 0.3611, 'mean_acc': 0.951}\n",
      "--- Epoch 16 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5422, 'acc_0_0': 0.9611, 'acc_0_1': 0.9958, 'acc_1_0': 0.8798, 'acc_1_1': 0.5422, 'mean_acc': 0.9603}\n",
      "Val(class): {'weighted_mean_acc': 0.9522, 'worst_acc': 0.2418, 'acc_0_0': 0.9592, 'acc_0_1': 0.9978, 'acc_1_0': 0.8399, 'acc_1_1': 0.2418, 'mean_acc': 0.9515}\n",
      "Test(class): {'weighted_mean_acc': 0.9533, 'worst_acc': 0.3444, 'acc_0_0': 0.9624, 'acc_0_1': 0.9972, 'acc_1_0': 0.8335, 'acc_1_1': 0.3444, 'mean_acc': 0.954}\n",
      "--- Epoch 17 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5544, 'acc_0_0': 0.9614, 'acc_0_1': 0.9957, 'acc_1_0': 0.8812, 'acc_1_1': 0.5544, 'mean_acc': 0.9607}\n",
      "Val(class): {'weighted_mean_acc': 0.9438, 'worst_acc': 0.1758, 'acc_0_0': 0.9756, 'acc_0_1': 0.9983, 'acc_1_0': 0.7314, 'acc_1_1': 0.1758, 'mean_acc': 0.9424}\n",
      "Test(class): {'weighted_mean_acc': 0.9442, 'worst_acc': 0.2389, 'acc_0_0': 0.9777, 'acc_0_1': 0.9981, 'acc_1_0': 0.7246, 'acc_1_1': 0.2389, 'mean_acc': 0.9473}\n",
      "--- Epoch 18 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.558, 'acc_0_0': 0.9615, 'acc_0_1': 0.996, 'acc_1_0': 0.8826, 'acc_1_1': 0.558, 'mean_acc': 0.9612}\n",
      "Val(class): {'weighted_mean_acc': 0.9529, 'worst_acc': 0.4176, 'acc_0_0': 0.9515, 'acc_0_1': 0.9937, 'acc_1_0': 0.8706, 'acc_1_1': 0.4176, 'mean_acc': 0.9525}\n",
      "Test(class): {'weighted_mean_acc': 0.9544, 'worst_acc': 0.4056, 'acc_0_0': 0.9572, 'acc_0_1': 0.9947, 'acc_1_0': 0.8609, 'acc_1_1': 0.4056, 'mean_acc': 0.9544}\n",
      "--- Epoch 19 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5847, 'acc_0_0': 0.9623, 'acc_0_1': 0.9958, 'acc_1_0': 0.8834, 'acc_1_1': 0.5847, 'mean_acc': 0.9618}\n",
      "Val(class): {'weighted_mean_acc': 0.9483, 'worst_acc': 0.4176, 'acc_0_0': 0.937, 'acc_0_1': 0.9937, 'acc_1_0': 0.8831, 'acc_1_1': 0.4176, 'mean_acc': 0.9481}\n",
      "Test(class): {'weighted_mean_acc': 0.9518, 'worst_acc': 0.4556, 'acc_0_0': 0.9457, 'acc_0_1': 0.9926, 'acc_1_0': 0.8819, 'acc_1_1': 0.4556, 'mean_acc': 0.9511}\n",
      "--- Epoch 20 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5811, 'acc_0_0': 0.9626, 'acc_0_1': 0.9959, 'acc_1_0': 0.887, 'acc_1_1': 0.5811, 'mean_acc': 0.9624}\n",
      "Val(class): {'weighted_mean_acc': 0.9512, 'worst_acc': 0.3846, 'acc_0_0': 0.9571, 'acc_0_1': 0.9935, 'acc_1_0': 0.8434, 'acc_1_1': 0.3846, 'mean_acc': 0.9506}\n",
      "Test(class): {'weighted_mean_acc': 0.9519, 'worst_acc': 0.4222, 'acc_0_0': 0.962, 'acc_0_1': 0.9931, 'acc_1_0': 0.8323, 'acc_1_1': 0.4222, 'mean_acc': 0.9528}\n",
      "--- Epoch 21 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5926, 'acc_0_0': 0.9634, 'acc_0_1': 0.9956, 'acc_1_0': 0.8852, 'acc_1_1': 0.5926, 'mean_acc': 0.9625}\n",
      "Val(class): {'weighted_mean_acc': 0.9525, 'worst_acc': 0.3516, 'acc_0_0': 0.9576, 'acc_0_1': 0.9969, 'acc_1_0': 0.8434, 'acc_1_1': 0.3516, 'mean_acc': 0.9519}\n",
      "Test(class): {'weighted_mean_acc': 0.952, 'worst_acc': 0.3889, 'acc_0_0': 0.9604, 'acc_0_1': 0.9951, 'acc_1_0': 0.8343, 'acc_1_1': 0.3889, 'mean_acc': 0.9527}\n",
      "--- Epoch 22 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6157, 'acc_0_0': 0.9635, 'acc_0_1': 0.9959, 'acc_1_0': 0.8902, 'acc_1_1': 0.6157, 'mean_acc': 0.9635}\n",
      "Val(class): {'weighted_mean_acc': 0.9466, 'worst_acc': 0.4396, 'acc_0_0': 0.9306, 'acc_0_1': 0.994, 'acc_1_0': 0.8887, 'acc_1_1': 0.4396, 'mean_acc': 0.9464}\n",
      "Test(class): {'weighted_mean_acc': 0.9505, 'worst_acc': 0.4722, 'acc_0_0': 0.9446, 'acc_0_1': 0.9918, 'acc_1_0': 0.877, 'acc_1_1': 0.4722, 'mean_acc': 0.9498}\n",
      "--- Epoch 23 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6128, 'acc_0_0': 0.9635, 'acc_0_1': 0.9961, 'acc_1_0': 0.8885, 'acc_1_1': 0.6128, 'mean_acc': 0.9633}\n",
      "Val(class): {'weighted_mean_acc': 0.9493, 'worst_acc': 0.3407, 'acc_0_0': 0.9623, 'acc_0_1': 0.9925, 'acc_1_0': 0.8191, 'acc_1_1': 0.3407, 'mean_acc': 0.9485}\n",
      "Test(class): {'weighted_mean_acc': 0.9518, 'worst_acc': 0.3833, 'acc_0_0': 0.9664, 'acc_0_1': 0.994, 'acc_1_0': 0.8169, 'acc_1_1': 0.3833, 'mean_acc': 0.953}\n",
      "--- Epoch 24 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6172, 'acc_0_0': 0.964, 'acc_0_1': 0.9958, 'acc_1_0': 0.8887, 'acc_1_1': 0.6172, 'mean_acc': 0.9635}\n",
      "Val(class): {'weighted_mean_acc': 0.9508, 'worst_acc': 0.2527, 'acc_0_0': 0.9456, 'acc_0_1': 0.9983, 'acc_1_0': 0.8706, 'acc_1_1': 0.2527, 'mean_acc': 0.9504}\n",
      "Test(class): {'weighted_mean_acc': 0.9525, 'worst_acc': 0.35, 'acc_0_0': 0.9515, 'acc_0_1': 0.9972, 'acc_1_0': 0.8613, 'acc_1_1': 0.35, 'mean_acc': 0.9521}\n",
      "--- Epoch 25 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.62, 'acc_0_0': 0.9637, 'acc_0_1': 0.9959, 'acc_1_0': 0.8886, 'acc_1_1': 0.62, 'mean_acc': 0.9634}\n",
      "Val(class): {'weighted_mean_acc': 0.9492, 'worst_acc': 0.2857, 'acc_0_0': 0.9625, 'acc_0_1': 0.9969, 'acc_1_0': 0.8086, 'acc_1_1': 0.2857, 'mean_acc': 0.9484}\n",
      "Test(class): {'weighted_mean_acc': 0.95, 'worst_acc': 0.3222, 'acc_0_0': 0.9656, 'acc_0_1': 0.9958, 'acc_1_0': 0.8056, 'acc_1_1': 0.3222, 'mean_acc': 0.9513}\n",
      "--- Epoch 26 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6222, 'acc_0_0': 0.9647, 'acc_0_1': 0.9961, 'acc_1_0': 0.8909, 'acc_1_1': 0.6222, 'mean_acc': 0.9643}\n",
      "Val(class): {'weighted_mean_acc': 0.9498, 'worst_acc': 0.4286, 'acc_0_0': 0.9426, 'acc_0_1': 0.9937, 'acc_1_0': 0.8754, 'acc_1_1': 0.4286, 'mean_acc': 0.9495}\n",
      "Test(class): {'weighted_mean_acc': 0.9501, 'worst_acc': 0.4556, 'acc_0_0': 0.9478, 'acc_0_1': 0.9922, 'acc_1_0': 0.8641, 'acc_1_1': 0.4556, 'mean_acc': 0.9497}\n",
      "--- Epoch 27 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6561, 'acc_0_0': 0.965, 'acc_0_1': 0.9962, 'acc_1_0': 0.8942, 'acc_1_1': 0.6561, 'mean_acc': 0.9652}\n",
      "Val(class): {'weighted_mean_acc': 0.9504, 'worst_acc': 0.2747, 'acc_0_0': 0.9405, 'acc_0_1': 0.9973, 'acc_1_0': 0.8852, 'acc_1_1': 0.2747, 'mean_acc': 0.9501}\n",
      "Test(class): {'weighted_mean_acc': 0.9518, 'worst_acc': 0.3167, 'acc_0_0': 0.9483, 'acc_0_1': 0.9956, 'acc_1_0': 0.873, 'acc_1_1': 0.3167, 'mean_acc': 0.9511}\n",
      "--- Epoch 28 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6431, 'acc_0_0': 0.9654, 'acc_0_1': 0.996, 'acc_1_0': 0.8925, 'acc_1_1': 0.6431, 'mean_acc': 0.965}\n",
      "Val(class): {'weighted_mean_acc': 0.948, 'worst_acc': 0.4725, 'acc_0_0': 0.9386, 'acc_0_1': 0.993, 'acc_1_0': 0.8747, 'acc_1_1': 0.4725, 'mean_acc': 0.9478}\n",
      "Test(class): {'weighted_mean_acc': 0.9517, 'worst_acc': 0.4778, 'acc_0_0': 0.9495, 'acc_0_1': 0.9926, 'acc_1_0': 0.8681, 'acc_1_1': 0.4778, 'mean_acc': 0.9514}\n",
      "--- Epoch 29 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6402, 'acc_0_0': 0.9647, 'acc_0_1': 0.9961, 'acc_1_0': 0.892, 'acc_1_1': 0.6402, 'mean_acc': 0.9646}\n",
      "Val(class): {'weighted_mean_acc': 0.9475, 'worst_acc': 0.2527, 'acc_0_0': 0.9536, 'acc_0_1': 0.9961, 'acc_1_0': 0.8281, 'acc_1_1': 0.2527, 'mean_acc': 0.9467}\n",
      "Test(class): {'weighted_mean_acc': 0.9511, 'worst_acc': 0.3833, 'acc_0_0': 0.9581, 'acc_0_1': 0.9956, 'acc_1_0': 0.8335, 'acc_1_1': 0.3833, 'mean_acc': 0.9516}\n",
      "--- Epoch 30 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6655, 'acc_0_0': 0.9653, 'acc_0_1': 0.9962, 'acc_1_0': 0.8916, 'acc_1_1': 0.6655, 'mean_acc': 0.9651}\n",
      "Val(class): {'weighted_mean_acc': 0.9511, 'worst_acc': 0.3846, 'acc_0_0': 0.9632, 'acc_0_1': 0.9925, 'acc_1_0': 0.8267, 'acc_1_1': 0.3846, 'mean_acc': 0.9504}\n",
      "Test(class): {'weighted_mean_acc': 0.9502, 'worst_acc': 0.4444, 'acc_0_0': 0.9637, 'acc_0_1': 0.992, 'acc_1_0': 0.8165, 'acc_1_1': 0.4444, 'mean_acc': 0.9514}\n",
      "--- Epoch 31 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6597, 'acc_0_0': 0.9658, 'acc_0_1': 0.9963, 'acc_1_0': 0.8945, 'acc_1_1': 0.6597, 'mean_acc': 0.9657}\n",
      "Val(class): {'weighted_mean_acc': 0.9475, 'worst_acc': 0.3846, 'acc_0_0': 0.9372, 'acc_0_1': 0.9942, 'acc_1_0': 0.8775, 'acc_1_1': 0.3846, 'mean_acc': 0.9473}\n",
      "Test(class): {'weighted_mean_acc': 0.9516, 'worst_acc': 0.4889, 'acc_0_0': 0.9454, 'acc_0_1': 0.9919, 'acc_1_0': 0.881, 'acc_1_1': 0.4889, 'mean_acc': 0.9509}\n",
      "--- Epoch 32 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6626, 'acc_0_0': 0.9663, 'acc_0_1': 0.9962, 'acc_1_0': 0.8943, 'acc_1_1': 0.6626, 'mean_acc': 0.9659}\n",
      "Val(class): {'weighted_mean_acc': 0.9493, 'worst_acc': 0.3846, 'acc_0_0': 0.9499, 'acc_0_1': 0.9949, 'acc_1_0': 0.8483, 'acc_1_1': 0.3846, 'mean_acc': 0.9488}\n",
      "Test(class): {'weighted_mean_acc': 0.9494, 'worst_acc': 0.4389, 'acc_0_0': 0.9554, 'acc_0_1': 0.9932, 'acc_1_0': 0.8335, 'acc_1_1': 0.4389, 'mean_acc': 0.9499}\n",
      "--- Epoch 33 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6806, 'acc_0_0': 0.9669, 'acc_0_1': 0.9961, 'acc_1_0': 0.8973, 'acc_1_1': 0.6806, 'mean_acc': 0.9667}\n",
      "Val(class): {'weighted_mean_acc': 0.9475, 'worst_acc': 0.2418, 'acc_0_0': 0.9452, 'acc_0_1': 0.9973, 'acc_1_0': 0.8518, 'acc_1_1': 0.2418, 'mean_acc': 0.9469}\n",
      "Test(class): {'weighted_mean_acc': 0.948, 'worst_acc': 0.3222, 'acc_0_0': 0.9501, 'acc_0_1': 0.9946, 'acc_1_0': 0.8431, 'acc_1_1': 0.3222, 'mean_acc': 0.948}\n",
      "--- Epoch 34 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6792, 'acc_0_0': 0.9668, 'acc_0_1': 0.9962, 'acc_1_0': 0.8991, 'acc_1_1': 0.6792, 'mean_acc': 0.9669}\n",
      "Val(class): {'weighted_mean_acc': 0.9496, 'worst_acc': 0.3407, 'acc_0_0': 0.9419, 'acc_0_1': 0.9949, 'acc_1_0': 0.8782, 'acc_1_1': 0.3407, 'mean_acc': 0.9493}\n",
      "Test(class): {'weighted_mean_acc': 0.9519, 'worst_acc': 0.4056, 'acc_0_0': 0.9486, 'acc_0_1': 0.9938, 'acc_1_0': 0.873, 'acc_1_1': 0.4056, 'mean_acc': 0.9514}\n",
      "--- Epoch 35 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6799, 'acc_0_0': 0.9673, 'acc_0_1': 0.9963, 'acc_1_0': 0.9003, 'acc_1_1': 0.6799, 'mean_acc': 0.9674}\n",
      "Val(class): {'weighted_mean_acc': 0.9463, 'worst_acc': 0.2747, 'acc_0_0': 0.9332, 'acc_0_1': 0.9964, 'acc_1_0': 0.8817, 'acc_1_1': 0.2747, 'mean_acc': 0.946}\n",
      "Test(class): {'weighted_mean_acc': 0.9495, 'worst_acc': 0.3167, 'acc_0_0': 0.9457, 'acc_0_1': 0.9948, 'acc_1_0': 0.8669, 'acc_1_1': 0.3167, 'mean_acc': 0.9488}\n",
      "--- Epoch 36 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6763, 'acc_0_0': 0.9663, 'acc_0_1': 0.9965, 'acc_1_0': 0.8969, 'acc_1_1': 0.6763, 'mean_acc': 0.9665}\n",
      "Val(class): {'weighted_mean_acc': 0.9454, 'worst_acc': 0.3626, 'acc_0_0': 0.9567, 'acc_0_1': 0.9913, 'acc_1_0': 0.8114, 'acc_1_1': 0.3626, 'mean_acc': 0.9446}\n",
      "Test(class): {'weighted_mean_acc': 0.9486, 'worst_acc': 0.4, 'acc_0_0': 0.9634, 'acc_0_1': 0.992, 'acc_1_0': 0.8081, 'acc_1_1': 0.4, 'mean_acc': 0.9499}\n",
      "--- Epoch 37 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.695, 'acc_0_0': 0.9675, 'acc_0_1': 0.9964, 'acc_1_0': 0.8981, 'acc_1_1': 0.695, 'mean_acc': 0.9673}\n",
      "Val(class): {'weighted_mean_acc': 0.9437, 'worst_acc': 0.3187, 'acc_0_0': 0.9213, 'acc_0_1': 0.9949, 'acc_1_0': 0.9019, 'acc_1_1': 0.3187, 'mean_acc': 0.9436}\n",
      "Test(class): {'weighted_mean_acc': 0.9494, 'worst_acc': 0.4222, 'acc_0_0': 0.9339, 'acc_0_1': 0.9947, 'acc_1_0': 0.898, 'acc_1_1': 0.4222, 'mean_acc': 0.9478}\n",
      "--- Epoch 38 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6986, 'acc_0_0': 0.9671, 'acc_0_1': 0.9964, 'acc_1_0': 0.8998, 'acc_1_1': 0.6986, 'mean_acc': 0.9674}\n",
      "Val(class): {'weighted_mean_acc': 0.9492, 'worst_acc': 0.3626, 'acc_0_0': 0.9618, 'acc_0_1': 0.9957, 'acc_1_0': 0.8093, 'acc_1_1': 0.3626, 'mean_acc': 0.9484}\n",
      "Test(class): {'weighted_mean_acc': 0.948, 'worst_acc': 0.3667, 'acc_0_0': 0.964, 'acc_0_1': 0.994, 'acc_1_0': 0.7988, 'acc_1_1': 0.3667, 'mean_acc': 0.9494}\n",
      "--- Epoch 39 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.7073, 'acc_0_0': 0.9682, 'acc_0_1': 0.9965, 'acc_1_0': 0.8998, 'acc_1_1': 0.7073, 'mean_acc': 0.968}\n",
      "Val(class): {'weighted_mean_acc': 0.9486, 'worst_acc': 0.5165, 'acc_0_0': 0.9473, 'acc_0_1': 0.9894, 'acc_1_0': 0.8594, 'acc_1_1': 0.5165, 'mean_acc': 0.9482}\n",
      "Test(class): {'weighted_mean_acc': 0.9478, 'worst_acc': 0.5389, 'acc_0_0': 0.9498, 'acc_0_1': 0.9877, 'acc_1_0': 0.85, 'acc_1_1': 0.5389, 'mean_acc': 0.948}\n",
      "--- Epoch 40 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6914, 'acc_0_0': 0.9686, 'acc_0_1': 0.9965, 'acc_1_0': 0.9014, 'acc_1_1': 0.6914, 'mean_acc': 0.9683}\n",
      "Val(class): {'weighted_mean_acc': 0.9471, 'worst_acc': 0.3077, 'acc_0_0': 0.9585, 'acc_0_1': 0.9959, 'acc_1_0': 0.8072, 'acc_1_1': 0.3077, 'mean_acc': 0.9462}\n",
      "Test(class): {'weighted_mean_acc': 0.9481, 'worst_acc': 0.3944, 'acc_0_0': 0.9615, 'acc_0_1': 0.9944, 'acc_1_0': 0.804, 'acc_1_1': 0.3944, 'mean_acc': 0.9493}\n",
      "--- Epoch 41 ---\n",
      "================== Stage 2) New adapter for Balanced-Text-Prompt ==================\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.3407, 'acc_0_0': 1.0, 'acc_0_1': 1.0, 'acc_1_0': 0.5714, 'acc_1_1': 0.3407, 'mean_acc': 0.728}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9398, 'worst_acc': 0.7033, 'acc_0_0': 0.9274, 'acc_0_1': 0.9797, 'acc_1_0': 0.8761, 'acc_1_1': 0.7033, 'mean_acc': 0.9397}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9405, 'worst_acc': 0.7056, 'acc_0_0': 0.9385, 'acc_0_1': 0.9733, 'acc_1_0': 0.8653, 'acc_1_1': 0.7056, 'mean_acc': 0.9404}\n",
      "--- Epoch 42 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.7802, 'acc_0_0': 0.9341, 'acc_0_1': 0.7802, 'acc_1_0': 0.8462, 'acc_1_1': 0.9231, 'mean_acc': 0.8709}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9273, 'worst_acc': 0.8022, 'acc_0_0': 0.891, 'acc_0_1': 0.9657, 'acc_1_0': 0.936, 'acc_1_1': 0.8022, 'mean_acc': 0.9278}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9315, 'worst_acc': 0.7333, 'acc_0_0': 0.9091, 'acc_0_1': 0.962, 'acc_1_0': 0.9246, 'acc_1_1': 0.7333, 'mean_acc': 0.9294}\n",
      "--- Epoch 43 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.8571, 'acc_0_0': 0.8571, 'acc_0_1': 0.9451, 'acc_1_0': 0.9341, 'acc_1_1': 0.9121, 'mean_acc': 0.9121}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9182, 'worst_acc': 0.8571, 'acc_0_0': 0.8885, 'acc_0_1': 0.9456, 'acc_1_0': 0.9346, 'acc_1_1': 0.8571, 'mean_acc': 0.9187}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9218, 'worst_acc': 0.8278, 'acc_0_0': 0.9025, 'acc_0_1': 0.9435, 'acc_1_0': 0.9242, 'acc_1_1': 0.8278, 'mean_acc': 0.92}\n",
      "--- Epoch 44 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.6484, 'acc_0_0': 0.6484, 'acc_0_1': 0.9231, 'acc_1_0': 0.7582, 'acc_1_1': 0.8681, 'mean_acc': 0.7995}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.6853, 'worst_acc': 0.5773, 'acc_0_0': 0.6821, 'acc_0_1': 0.5773, 'acc_1_0': 0.9916, 'acc_1_1': 1.0, 'mean_acc': 0.6861}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.7085, 'worst_acc': 0.5643, 'acc_0_0': 0.7476, 'acc_0_1': 0.5643, 'acc_1_0': 0.9915, 'acc_1_1': 0.9778, 'mean_acc': 0.7108}\n",
      "--- Epoch 45 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9121, 'acc_0_0': 0.9341, 'acc_0_1': 0.9121, 'acc_1_0': 0.989, 'acc_1_1': 0.9341, 'mean_acc': 0.9423}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9134, 'worst_acc': 0.8462, 'acc_0_0': 0.8859, 'acc_0_1': 0.9294, 'acc_1_0': 0.9569, 'acc_1_1': 0.8462, 'mean_acc': 0.9139}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9184, 'worst_acc': 0.8556, 'acc_0_0': 0.902, 'acc_0_1': 0.9297, 'acc_1_0': 0.9407, 'acc_1_1': 0.8556, 'mean_acc': 0.9168}\n",
      "--- Epoch 46 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9231, 'acc_0_0': 0.9231, 'acc_0_1': 0.956, 'acc_1_0': 0.9231, 'acc_1_1': 0.956, 'mean_acc': 0.9396}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8183, 'worst_acc': 0.7489, 'acc_0_0': 0.8294, 'acc_0_1': 0.7489, 'acc_1_0': 0.9756, 'acc_1_1': 0.989, 'mean_acc': 0.8185}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.832, 'worst_acc': 0.7537, 'acc_0_0': 0.8592, 'acc_0_1': 0.7537, 'acc_1_0': 0.9669, 'acc_1_1': 0.9722, 'mean_acc': 0.8338}\n",
      "--- Epoch 47 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9451, 'acc_0_0': 0.9451, 'acc_0_1': 0.9451, 'acc_1_0': 0.9451, 'acc_1_1': 0.967, 'mean_acc': 0.9505}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9114, 'worst_acc': 0.8681, 'acc_0_0': 0.8957, 'acc_0_1': 0.9227, 'acc_1_0': 0.9304, 'acc_1_1': 0.8681, 'mean_acc': 0.9117}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.916, 'worst_acc': 0.8833, 'acc_0_0': 0.9111, 'acc_0_1': 0.9201, 'acc_1_0': 0.921, 'acc_1_1': 0.8833, 'mean_acc': 0.9155}\n",
      "--- Epoch 48 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9341, 'acc_0_0': 0.956, 'acc_0_1': 0.9341, 'acc_1_0': 0.9451, 'acc_1_1': 0.989, 'mean_acc': 0.956}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8971, 'worst_acc': 0.8871, 'acc_0_0': 0.8936, 'acc_0_1': 0.8871, 'acc_1_0': 0.936, 'acc_1_1': 0.9121, 'mean_acc': 0.8972}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8971, 'worst_acc': 0.878, 'acc_0_0': 0.904, 'acc_0_1': 0.878, 'acc_1_0': 0.9319, 'acc_1_1': 0.8889, 'mean_acc': 0.8975}\n",
      "--- Epoch 49 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9341, 'acc_0_0': 0.9341, 'acc_0_1': 0.956, 'acc_1_0': 0.978, 'acc_1_1': 0.967, 'mean_acc': 0.9588}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9217, 'worst_acc': 0.8681, 'acc_0_0': 0.8908, 'acc_0_1': 0.9468, 'acc_1_0': 0.9485, 'acc_1_1': 0.8681, 'mean_acc': 0.9223}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9259, 'worst_acc': 0.85, 'acc_0_0': 0.9058, 'acc_0_1': 0.9445, 'acc_1_0': 0.9387, 'acc_1_1': 0.85, 'mean_acc': 0.924}\n",
      "--- Epoch 50 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9231, 'acc_0_0': 0.9231, 'acc_0_1': 0.967, 'acc_1_0': 0.956, 'acc_1_1': 0.967, 'mean_acc': 0.9533}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8928, 'worst_acc': 0.8433, 'acc_0_0': 0.8433, 'acc_0_1': 0.9207, 'acc_1_0': 0.968, 'acc_1_1': 0.8681, 'mean_acc': 0.8938}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9035, 'worst_acc': 0.8722, 'acc_0_0': 0.8722, 'acc_0_1': 0.9173, 'acc_1_0': 0.9605, 'acc_1_1': 0.9111, 'mean_acc': 0.9006}\n",
      "--- Epoch 51 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.956, 'acc_0_0': 0.956, 'acc_0_1': 0.978, 'acc_1_0': 0.967, 'acc_1_1': 0.978, 'mean_acc': 0.9698}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9108, 'worst_acc': 0.8988, 'acc_0_0': 0.8988, 'acc_0_1': 0.9149, 'acc_1_0': 0.9367, 'acc_1_1': 0.9011, 'mean_acc': 0.911}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9142, 'worst_acc': 0.9, 'acc_0_0': 0.9172, 'acc_0_1': 0.9111, 'acc_1_0': 0.9149, 'acc_1_1': 0.9, 'mean_acc': 0.9144}\n",
      "--- Epoch 52 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9231, 'acc_0_0': 0.967, 'acc_0_1': 0.9231, 'acc_1_0': 1.0, 'acc_1_1': 0.978, 'mean_acc': 0.967}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9284, 'worst_acc': 0.7802, 'acc_0_0': 0.9157, 'acc_0_1': 0.9613, 'acc_1_0': 0.881, 'acc_1_1': 0.7802, 'mean_acc': 0.9284}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9323, 'worst_acc': 0.7556, 'acc_0_0': 0.9287, 'acc_0_1': 0.9591, 'acc_1_0': 0.8754, 'acc_1_1': 0.7556, 'mean_acc': 0.932}\n",
      "--- Epoch 53 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.956, 'acc_0_0': 0.956, 'acc_0_1': 0.989, 'acc_1_0': 0.956, 'acc_1_1': 0.989, 'mean_acc': 0.9725}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8945, 'worst_acc': 0.8404, 'acc_0_0': 0.8404, 'acc_0_1': 0.9292, 'acc_1_0': 0.9645, 'acc_1_1': 0.8571, 'mean_acc': 0.8955}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9047, 'worst_acc': 0.8444, 'acc_0_0': 0.8706, 'acc_0_1': 0.9262, 'acc_1_0': 0.9524, 'acc_1_1': 0.8444, 'mean_acc': 0.9015}\n",
      "--- Epoch 54 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9341, 'acc_0_0': 0.9341, 'acc_0_1': 0.956, 'acc_1_0': 0.956, 'acc_1_1': 0.967, 'mean_acc': 0.9533}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9164, 'worst_acc': 0.858, 'acc_0_0': 0.9391, 'acc_0_1': 0.9128, 'acc_1_0': 0.858, 'acc_1_1': 0.8791, 'mean_acc': 0.9158}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9208, 'worst_acc': 0.8484, 'acc_0_0': 0.9505, 'acc_0_1': 0.9151, 'acc_1_0': 0.8484, 'acc_1_1': 0.8556, 'mean_acc': 0.9236}\n",
      "--- Epoch 55 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9121, 'acc_0_0': 0.967, 'acc_0_1': 0.967, 'acc_1_0': 0.9121, 'acc_1_1': 0.989, 'mean_acc': 0.9588}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9161, 'worst_acc': 0.8728, 'acc_0_0': 0.8728, 'acc_0_1': 0.9493, 'acc_1_0': 0.9569, 'acc_1_1': 0.8791, 'mean_acc': 0.9169}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9179, 'worst_acc': 0.85, 'acc_0_0': 0.8902, 'acc_0_1': 0.9384, 'acc_1_0': 0.9488, 'acc_1_1': 0.85, 'mean_acc': 0.9153}\n",
      "--- Epoch 56 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9451, 'acc_0_0': 0.9451, 'acc_0_1': 0.978, 'acc_1_0': 0.9451, 'acc_1_1': 0.989, 'mean_acc': 0.9643}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.922, 'worst_acc': 0.8681, 'acc_0_0': 0.9213, 'acc_0_1': 0.9316, 'acc_1_0': 0.8998, 'acc_1_1': 0.8681, 'mean_acc': 0.922}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9246, 'worst_acc': 0.8444, 'acc_0_0': 0.9354, 'acc_0_1': 0.925, 'acc_1_0': 0.8944, 'acc_1_1': 0.8444, 'mean_acc': 0.9256}\n",
      "--- Epoch 57 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9451, 'acc_0_0': 0.9451, 'acc_0_1': 0.989, 'acc_1_0': 0.967, 'acc_1_1': 1.0, 'mean_acc': 0.9753}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9282, 'worst_acc': 0.8462, 'acc_0_0': 0.9037, 'acc_0_1': 0.9551, 'acc_1_0': 0.9311, 'acc_1_1': 0.8462, 'mean_acc': 0.9285}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9317, 'worst_acc': 0.8, 'acc_0_0': 0.9212, 'acc_0_1': 0.951, 'acc_1_0': 0.9165, 'acc_1_1': 0.8, 'mean_acc': 0.9308}\n",
      "--- Epoch 58 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.956, 'acc_0_0': 0.967, 'acc_0_1': 0.956, 'acc_1_0': 0.989, 'acc_1_1': 0.956, 'mean_acc': 0.967}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8738, 'worst_acc': 0.8519, 'acc_0_0': 0.8662, 'acc_0_1': 0.8519, 'acc_1_0': 0.9576, 'acc_1_1': 0.9451, 'mean_acc': 0.8742}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8793, 'worst_acc': 0.8446, 'acc_0_0': 0.8877, 'acc_0_1': 0.8446, 'acc_1_0': 0.9512, 'acc_1_1': 0.9333, 'mean_acc': 0.8797}\n",
      "--- Epoch 59 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.956, 'acc_0_0': 0.978, 'acc_0_1': 0.956, 'acc_1_0': 0.967, 'acc_1_1': 1.0, 'mean_acc': 0.9753}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.904, 'worst_acc': 0.8784, 'acc_0_0': 0.8784, 'acc_0_1': 0.9159, 'acc_1_0': 0.9506, 'acc_1_1': 0.8791, 'mean_acc': 0.9045}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9095, 'worst_acc': 0.8889, 'acc_0_0': 0.8973, 'acc_0_1': 0.9113, 'acc_1_0': 0.9435, 'acc_1_1': 0.8889, 'mean_acc': 0.9083}\n",
      "--- Epoch 60 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9121, 'acc_0_0': 0.9121, 'acc_0_1': 0.967, 'acc_1_0': 0.9231, 'acc_1_1': 0.989, 'mean_acc': 0.9478}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8765, 'worst_acc': 0.7648, 'acc_0_0': 0.7648, 'acc_0_1': 0.9594, 'acc_1_0': 0.9875, 'acc_1_1': 0.8242, 'mean_acc': 0.8786}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8904, 'worst_acc': 0.7667, 'acc_0_0': 0.8073, 'acc_0_1': 0.9498, 'acc_1_0': 0.9843, 'acc_1_1': 0.7667, 'mean_acc': 0.8827}\n",
      "--- Epoch 61 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9451, 'acc_0_0': 0.9451, 'acc_0_1': 1.0, 'acc_1_0': 0.9451, 'acc_1_1': 0.989, 'mean_acc': 0.9698}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8871, 'worst_acc': 0.809, 'acc_0_0': 0.809, 'acc_0_1': 0.9372, 'acc_1_0': 0.9861, 'acc_1_1': 0.8681, 'mean_acc': 0.8886}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8966, 'worst_acc': 0.8416, 'acc_0_0': 0.8416, 'acc_0_1': 0.9285, 'acc_1_0': 0.9774, 'acc_1_1': 0.8611, 'mean_acc': 0.8914}\n",
      "--- Epoch 62 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9121, 'acc_0_0': 0.9121, 'acc_0_1': 0.967, 'acc_1_0': 0.9341, 'acc_1_1': 0.967, 'mean_acc': 0.9451}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8702, 'worst_acc': 0.7969, 'acc_0_0': 0.7969, 'acc_0_1': 0.9087, 'acc_1_0': 0.9854, 'acc_1_1': 0.9011, 'mean_acc': 0.8717}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8807, 'worst_acc': 0.831, 'acc_0_0': 0.831, 'acc_0_1': 0.8994, 'acc_1_0': 0.9798, 'acc_1_1': 0.9167, 'mean_acc': 0.8761}\n",
      "--- Epoch 63 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9121, 'acc_0_0': 0.9121, 'acc_0_1': 0.989, 'acc_1_0': 0.978, 'acc_1_1': 1.0, 'mean_acc': 0.9698}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9162, 'worst_acc': 0.8681, 'acc_0_0': 0.8807, 'acc_0_1': 0.9442, 'acc_1_0': 0.9485, 'acc_1_1': 0.8681, 'mean_acc': 0.9169}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9161, 'worst_acc': 0.8611, 'acc_0_0': 0.893, 'acc_0_1': 0.9311, 'acc_1_0': 0.9476, 'acc_1_1': 0.8611, 'mean_acc': 0.9139}\n",
      "--- Epoch 64 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9231, 'acc_0_0': 0.9451, 'acc_0_1': 0.989, 'acc_1_0': 0.9231, 'acc_1_1': 0.956, 'mean_acc': 0.9533}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8958, 'worst_acc': 0.8594, 'acc_0_0': 0.8594, 'acc_0_1': 0.9111, 'acc_1_0': 0.9638, 'acc_1_1': 0.9121, 'mean_acc': 0.8965}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8979, 'worst_acc': 0.8751, 'acc_0_0': 0.8751, 'acc_0_1': 0.9005, 'acc_1_0': 0.9613, 'acc_1_1': 0.9111, 'mean_acc': 0.8957}\n",
      "--- Epoch 65 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9231, 'acc_0_0': 0.9231, 'acc_0_1': 0.967, 'acc_1_0': 0.967, 'acc_1_1': 1.0, 'mean_acc': 0.9643}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9036, 'worst_acc': 0.873, 'acc_0_0': 0.873, 'acc_0_1': 0.9183, 'acc_1_0': 0.9562, 'acc_1_1': 0.9121, 'mean_acc': 0.9043}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.906, 'worst_acc': 0.8874, 'acc_0_0': 0.8874, 'acc_0_1': 0.91, 'acc_1_0': 0.952, 'acc_1_1': 0.9167, 'mean_acc': 0.9042}\n",
      "========================================================================\n",
      "> end of training. \n",
      "\n",
      "best epoch : 51\n",
      "best training accuracy on [class]: {'worst_acc': 0.956, 'acc_0_0': 0.956, 'acc_0_1': 0.978, 'acc_1_0': 0.967, 'acc_1_1': 0.978, 'mean_acc': 0.9698}\n",
      "best validation accuracy on [class]: {'weighted_mean_acc': 0.9108, 'worst_acc': 0.8988, 'acc_0_0': 0.8988, 'acc_0_1': 0.9149, 'acc_1_0': 0.9367, 'acc_1_1': 0.9011, 'mean_acc': 0.911}\n",
      "best test accuracy on [class]: {'weighted_mean_acc': 0.9142, 'worst_acc': 0.9, 'acc_0_0': 0.9172, 'acc_0_1': 0.9111, 'acc_1_0': 0.9149, 'acc_1_1': 0.9, 'mean_acc': 0.9144}\n",
      "========================================================================\n",
      "> start evaluating feature quality of best model. (using zero-shot prediction)\n",
      "\n",
      "zero-shot prediction (test) (class): {'weighted_mean_acc': 0.9142, 'worst_acc': 0.9, 'acc_0_0': 0.9172, 'acc_0_1': 0.9111, 'acc_1_0': 0.9149, 'acc_1_1': 0.9, 'mean_acc': 0.9144}\n",
      "zero-shot prediction (test) (spurious): {'weighted_mean_acc': 0.9643, 'worst_acc': 0.9358, 'acc_0_0': 0.9358, 'acc_0_1': 0.9895, 'acc_1_0': 0.9786, 'acc_1_1': 0.9833, 'mean_acc': 0.9618}\n",
      " ㄴ Note that it is related to [richness of non-target (spurious) information] (-> 'mean_acc' is important)\n",
      "========================================================================\n",
      "========================================================================\n",
      "> end\n",
      "f=============Iteration : 2/2=============\n",
      "====== TL:[adapter_reg_seq_alter] LR:[0.1] BS:[1024] BSr:[4] ======\n",
      "> Simply copy the data loader ... \n",
      "Using [Balanced] Validation loader for regularized training\n",
      "Off-the-shelf classifier : [Adapter + (temperatured) image-text jointly normalized prediction] with group regularized training\n",
      "Set Optimizer: SGD (default)\n",
      "========================================================================\n",
      "--- Epoch 1 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.3028, 'acc_0_0': 0.9456, 'acc_0_1': 0.9945, 'acc_1_0': 0.8457, 'acc_1_1': 0.3028, 'mean_acc': 0.9461}\n",
      "Val(class): {'weighted_mean_acc': 0.953, 'worst_acc': 0.3516, 'acc_0_0': 0.9649, 'acc_0_1': 0.9964, 'acc_1_0': 0.8253, 'acc_1_1': 0.3516, 'mean_acc': 0.9522}\n",
      "Test(class): {'weighted_mean_acc': 0.9529, 'worst_acc': 0.3667, 'acc_0_0': 0.9684, 'acc_0_1': 0.9965, 'acc_1_0': 0.8125, 'acc_1_1': 0.3667, 'mean_acc': 0.9542}\n",
      "--- Epoch 2 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.349, 'acc_0_0': 0.9517, 'acc_0_1': 0.9954, 'acc_1_0': 0.8544, 'acc_1_1': 0.349, 'mean_acc': 0.9508}\n",
      "Val(class): {'weighted_mean_acc': 0.9535, 'worst_acc': 0.3956, 'acc_0_0': 0.948, 'acc_0_1': 0.9949, 'acc_1_0': 0.8838, 'acc_1_1': 0.3956, 'mean_acc': 0.9532}\n",
      "Test(class): {'weighted_mean_acc': 0.9549, 'worst_acc': 0.4278, 'acc_0_0': 0.9511, 'acc_0_1': 0.995, 'acc_1_0': 0.8815, 'acc_1_1': 0.4278, 'mean_acc': 0.9543}\n",
      "--- Epoch 3 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.3583, 'acc_0_0': 0.9539, 'acc_0_1': 0.9952, 'acc_1_0': 0.861, 'acc_1_1': 0.3583, 'mean_acc': 0.9527}\n",
      "Val(class): {'weighted_mean_acc': 0.9534, 'worst_acc': 0.3956, 'acc_0_0': 0.948, 'acc_0_1': 0.9957, 'acc_1_0': 0.881, 'acc_1_1': 0.3956, 'mean_acc': 0.9531}\n",
      "Test(class): {'weighted_mean_acc': 0.9548, 'worst_acc': 0.4167, 'acc_0_0': 0.9553, 'acc_0_1': 0.994, 'acc_1_0': 0.8714, 'acc_1_1': 0.4167, 'mean_acc': 0.9546}\n",
      "--- Epoch 4 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.3879, 'acc_0_0': 0.9555, 'acc_0_1': 0.9957, 'acc_1_0': 0.8647, 'acc_1_1': 0.3879, 'mean_acc': 0.9544}\n",
      "Val(class): {'weighted_mean_acc': 0.9472, 'worst_acc': 0.2308, 'acc_0_0': 0.9663, 'acc_0_1': 0.9966, 'acc_1_0': 0.7864, 'acc_1_1': 0.2308, 'mean_acc': 0.9461}\n",
      "Test(class): {'weighted_mean_acc': 0.9494, 'worst_acc': 0.35, 'acc_0_0': 0.9707, 'acc_0_1': 0.996, 'acc_1_0': 0.7827, 'acc_1_1': 0.35, 'mean_acc': 0.9513}\n",
      "--- Epoch 5 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.3965, 'acc_0_0': 0.9565, 'acc_0_1': 0.9957, 'acc_1_0': 0.8656, 'acc_1_1': 0.3965, 'mean_acc': 0.9551}\n",
      "Val(class): {'weighted_mean_acc': 0.9458, 'worst_acc': 0.1978, 'acc_0_0': 0.9738, 'acc_0_1': 0.9976, 'acc_1_0': 0.7523, 'acc_1_1': 0.1978, 'mean_acc': 0.9445}\n",
      "Test(class): {'weighted_mean_acc': 0.9469, 'worst_acc': 0.2333, 'acc_0_0': 0.9787, 'acc_0_1': 0.9987, 'acc_1_0': 0.7391, 'acc_1_1': 0.2333, 'mean_acc': 0.9498}\n",
      "--- Epoch 6 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4239, 'acc_0_0': 0.9559, 'acc_0_1': 0.9957, 'acc_1_0': 0.8682, 'acc_1_1': 0.4239, 'mean_acc': 0.9554}\n",
      "Val(class): {'weighted_mean_acc': 0.9516, 'worst_acc': 0.2418, 'acc_0_0': 0.962, 'acc_0_1': 0.9966, 'acc_1_0': 0.8302, 'acc_1_1': 0.2418, 'mean_acc': 0.9508}\n",
      "Test(class): {'weighted_mean_acc': 0.9544, 'worst_acc': 0.3444, 'acc_0_0': 0.9684, 'acc_0_1': 0.9969, 'acc_1_0': 0.823, 'acc_1_1': 0.3444, 'mean_acc': 0.9555}\n",
      "--- Epoch 7 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4362, 'acc_0_0': 0.9573, 'acc_0_1': 0.9955, 'acc_1_0': 0.8696, 'acc_1_1': 0.4362, 'mean_acc': 0.9562}\n",
      "Val(class): {'weighted_mean_acc': 0.9501, 'worst_acc': 0.3846, 'acc_0_0': 0.9342, 'acc_0_1': 0.9947, 'acc_1_0': 0.904, 'acc_1_1': 0.3846, 'mean_acc': 0.95}\n",
      "Test(class): {'weighted_mean_acc': 0.9523, 'worst_acc': 0.4278, 'acc_0_0': 0.9429, 'acc_0_1': 0.9934, 'acc_1_0': 0.894, 'acc_1_1': 0.4278, 'mean_acc': 0.9512}\n",
      "--- Epoch 8 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4521, 'acc_0_0': 0.9583, 'acc_0_1': 0.9956, 'acc_1_0': 0.8706, 'acc_1_1': 0.4521, 'mean_acc': 0.957}\n",
      "Val(class): {'weighted_mean_acc': 0.9515, 'worst_acc': 0.2857, 'acc_0_0': 0.9475, 'acc_0_1': 0.9969, 'acc_1_0': 0.872, 'acc_1_1': 0.2857, 'mean_acc': 0.9511}\n",
      "Test(class): {'weighted_mean_acc': 0.9561, 'worst_acc': 0.3389, 'acc_0_0': 0.9569, 'acc_0_1': 0.9972, 'acc_1_0': 0.8706, 'acc_1_1': 0.3389, 'mean_acc': 0.9558}\n",
      "--- Epoch 9 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4722, 'acc_0_0': 0.9581, 'acc_0_1': 0.9956, 'acc_1_0': 0.8715, 'acc_1_1': 0.4722, 'mean_acc': 0.9572}\n",
      "Val(class): {'weighted_mean_acc': 0.9486, 'worst_acc': 0.2527, 'acc_0_0': 0.9623, 'acc_0_1': 0.9969, 'acc_1_0': 0.8072, 'acc_1_1': 0.2527, 'mean_acc': 0.9478}\n",
      "Test(class): {'weighted_mean_acc': 0.9508, 'worst_acc': 0.3111, 'acc_0_0': 0.9632, 'acc_0_1': 0.9964, 'acc_1_0': 0.8173, 'acc_1_1': 0.3111, 'mean_acc': 0.9518}\n",
      "--- Epoch 10 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4758, 'acc_0_0': 0.9594, 'acc_0_1': 0.9955, 'acc_1_0': 0.8713, 'acc_1_1': 0.4758, 'mean_acc': 0.9577}\n",
      "Val(class): {'weighted_mean_acc': 0.9499, 'worst_acc': 0.2418, 'acc_0_0': 0.9538, 'acc_0_1': 0.9969, 'acc_1_0': 0.8434, 'acc_1_1': 0.2418, 'mean_acc': 0.9493}\n",
      "Test(class): {'weighted_mean_acc': 0.9545, 'worst_acc': 0.3222, 'acc_0_0': 0.961, 'acc_0_1': 0.9969, 'acc_1_0': 0.8484, 'acc_1_1': 0.3222, 'mean_acc': 0.9548}\n",
      "--- Epoch 11 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.4845, 'acc_0_0': 0.9595, 'acc_0_1': 0.9957, 'acc_1_0': 0.8747, 'acc_1_1': 0.4845, 'mean_acc': 0.9584}\n",
      "Val(class): {'weighted_mean_acc': 0.9534, 'worst_acc': 0.3956, 'acc_0_0': 0.9548, 'acc_0_1': 0.9949, 'acc_1_0': 0.8615, 'acc_1_1': 0.3956, 'mean_acc': 0.9529}\n",
      "Test(class): {'weighted_mean_acc': 0.9529, 'worst_acc': 0.3889, 'acc_0_0': 0.9595, 'acc_0_1': 0.9934, 'acc_1_0': 0.848, 'acc_1_1': 0.3889, 'mean_acc': 0.9533}\n",
      "--- Epoch 12 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5004, 'acc_0_0': 0.9593, 'acc_0_1': 0.9955, 'acc_1_0': 0.875, 'acc_1_1': 0.5004, 'mean_acc': 0.9585}\n",
      "Val(class): {'weighted_mean_acc': 0.9518, 'worst_acc': 0.4615, 'acc_0_0': 0.947, 'acc_0_1': 0.9954, 'acc_1_0': 0.8692, 'acc_1_1': 0.4615, 'mean_acc': 0.9515}\n",
      "Test(class): {'weighted_mean_acc': 0.9538, 'worst_acc': 0.4333, 'acc_0_0': 0.9543, 'acc_0_1': 0.9915, 'acc_1_0': 0.8738, 'acc_1_1': 0.4333, 'mean_acc': 0.9537}\n",
      "--- Epoch 13 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5126, 'acc_0_0': 0.9605, 'acc_0_1': 0.9956, 'acc_1_0': 0.8763, 'acc_1_1': 0.5126, 'mean_acc': 0.9593}\n",
      "Val(class): {'weighted_mean_acc': 0.9504, 'worst_acc': 0.4286, 'acc_0_0': 0.9567, 'acc_0_1': 0.9954, 'acc_1_0': 0.8309, 'acc_1_1': 0.4286, 'mean_acc': 0.9498}\n",
      "Test(class): {'weighted_mean_acc': 0.9532, 'worst_acc': 0.4389, 'acc_0_0': 0.9655, 'acc_0_1': 0.9934, 'acc_1_0': 0.8282, 'acc_1_1': 0.4389, 'mean_acc': 0.9542}\n",
      "--- Epoch 14 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5184, 'acc_0_0': 0.9609, 'acc_0_1': 0.9954, 'acc_1_0': 0.8763, 'acc_1_1': 0.5184, 'mean_acc': 0.9594}\n",
      "Val(class): {'weighted_mean_acc': 0.9502, 'worst_acc': 0.2967, 'acc_0_0': 0.9517, 'acc_0_1': 0.9957, 'acc_1_0': 0.8525, 'acc_1_1': 0.2967, 'mean_acc': 0.9497}\n",
      "Test(class): {'weighted_mean_acc': 0.9548, 'worst_acc': 0.35, 'acc_0_0': 0.9607, 'acc_0_1': 0.996, 'acc_1_0': 0.8524, 'acc_1_1': 0.35, 'mean_acc': 0.9551}\n",
      "--- Epoch 15 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5458, 'acc_0_0': 0.9614, 'acc_0_1': 0.9958, 'acc_1_0': 0.8799, 'acc_1_1': 0.5458, 'mean_acc': 0.9606}\n",
      "Val(class): {'weighted_mean_acc': 0.9515, 'worst_acc': 0.4286, 'acc_0_0': 0.9506, 'acc_0_1': 0.9935, 'acc_1_0': 0.8636, 'acc_1_1': 0.4286, 'mean_acc': 0.9511}\n",
      "Test(class): {'weighted_mean_acc': 0.9521, 'worst_acc': 0.4889, 'acc_0_0': 0.9547, 'acc_0_1': 0.9906, 'acc_1_0': 0.8593, 'acc_1_1': 0.4889, 'mean_acc': 0.9522}\n",
      "--- Epoch 16 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5422, 'acc_0_0': 0.9608, 'acc_0_1': 0.996, 'acc_1_0': 0.8796, 'acc_1_1': 0.5422, 'mean_acc': 0.9603}\n",
      "Val(class): {'weighted_mean_acc': 0.9499, 'worst_acc': 0.3077, 'acc_0_0': 0.9585, 'acc_0_1': 0.9947, 'acc_1_0': 0.8309, 'acc_1_1': 0.3077, 'mean_acc': 0.9492}\n",
      "Test(class): {'weighted_mean_acc': 0.953, 'worst_acc': 0.3778, 'acc_0_0': 0.9642, 'acc_0_1': 0.9951, 'acc_1_0': 0.8302, 'acc_1_1': 0.3778, 'mean_acc': 0.9539}\n",
      "--- Epoch 17 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5523, 'acc_0_0': 0.9622, 'acc_0_1': 0.9958, 'acc_1_0': 0.8812, 'acc_1_1': 0.5523, 'mean_acc': 0.9611}\n",
      "Val(class): {'weighted_mean_acc': 0.9498, 'worst_acc': 0.2088, 'acc_0_0': 0.9414, 'acc_0_1': 0.9976, 'acc_1_0': 0.881, 'acc_1_1': 0.2088, 'mean_acc': 0.9494}\n",
      "Test(class): {'weighted_mean_acc': 0.9526, 'worst_acc': 0.2611, 'acc_0_0': 0.9514, 'acc_0_1': 0.9968, 'acc_1_0': 0.869, 'acc_1_1': 0.2611, 'mean_acc': 0.9521}\n",
      "--- Epoch 18 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5501, 'acc_0_0': 0.9627, 'acc_0_1': 0.9958, 'acc_1_0': 0.8835, 'acc_1_1': 0.5501, 'mean_acc': 0.9616}\n",
      "Val(class): {'weighted_mean_acc': 0.9469, 'worst_acc': 0.5714, 'acc_0_0': 0.9239, 'acc_0_1': 0.9896, 'acc_1_0': 0.9172, 'acc_1_1': 0.5714, 'mean_acc': 0.9471}\n",
      "Test(class): {'weighted_mean_acc': 0.9479, 'worst_acc': 0.5278, 'acc_0_0': 0.9336, 'acc_0_1': 0.9857, 'acc_1_0': 0.9081, 'acc_1_1': 0.5278, 'mean_acc': 0.9464}\n",
      "--- Epoch 19 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5789, 'acc_0_0': 0.9632, 'acc_0_1': 0.9961, 'acc_1_0': 0.8858, 'acc_1_1': 0.5789, 'mean_acc': 0.9626}\n",
      "Val(class): {'weighted_mean_acc': 0.9473, 'worst_acc': 0.2198, 'acc_0_0': 0.9649, 'acc_0_1': 0.9978, 'acc_1_0': 0.7884, 'acc_1_1': 0.2198, 'mean_acc': 0.9462}\n",
      "Test(class): {'weighted_mean_acc': 0.949, 'worst_acc': 0.2444, 'acc_0_0': 0.9704, 'acc_0_1': 0.9973, 'acc_1_0': 0.7835, 'acc_1_1': 0.2444, 'mean_acc': 0.9508}\n",
      "--- Epoch 20 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5833, 'acc_0_0': 0.9633, 'acc_0_1': 0.9958, 'acc_1_0': 0.8822, 'acc_1_1': 0.5833, 'mean_acc': 0.962}\n",
      "Val(class): {'weighted_mean_acc': 0.9483, 'worst_acc': 0.3297, 'acc_0_0': 0.9342, 'acc_0_1': 0.9981, 'acc_1_0': 0.8845, 'acc_1_1': 0.3297, 'mean_acc': 0.9481}\n",
      "Test(class): {'weighted_mean_acc': 0.9511, 'worst_acc': 0.3167, 'acc_0_0': 0.9442, 'acc_0_1': 0.9964, 'acc_1_0': 0.8786, 'acc_1_1': 0.3167, 'mean_acc': 0.9501}\n",
      "--- Epoch 21 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5926, 'acc_0_0': 0.9629, 'acc_0_1': 0.9959, 'acc_1_0': 0.8868, 'acc_1_1': 0.5926, 'mean_acc': 0.9626}\n",
      "Val(class): {'weighted_mean_acc': 0.9475, 'worst_acc': 0.2967, 'acc_0_0': 0.9515, 'acc_0_1': 0.9959, 'acc_1_0': 0.833, 'acc_1_1': 0.2967, 'mean_acc': 0.9468}\n",
      "Test(class): {'weighted_mean_acc': 0.9514, 'worst_acc': 0.3556, 'acc_0_0': 0.9622, 'acc_0_1': 0.9947, 'acc_1_0': 0.8274, 'acc_1_1': 0.3556, 'mean_acc': 0.9523}\n",
      "--- Epoch 22 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5941, 'acc_0_0': 0.9637, 'acc_0_1': 0.9957, 'acc_1_0': 0.8863, 'acc_1_1': 0.5941, 'mean_acc': 0.9629}\n",
      "Val(class): {'weighted_mean_acc': 0.9451, 'worst_acc': 0.4286, 'acc_0_0': 0.937, 'acc_0_1': 0.9918, 'acc_1_0': 0.8657, 'acc_1_1': 0.4286, 'mean_acc': 0.9448}\n",
      "Test(class): {'weighted_mean_acc': 0.9501, 'worst_acc': 0.4722, 'acc_0_0': 0.9462, 'acc_0_1': 0.9906, 'acc_1_0': 0.873, 'acc_1_1': 0.4722, 'mean_acc': 0.9496}\n",
      "--- Epoch 23 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.5955, 'acc_0_0': 0.9645, 'acc_0_1': 0.996, 'acc_1_0': 0.8858, 'acc_1_1': 0.5955, 'mean_acc': 0.9632}\n",
      "Val(class): {'weighted_mean_acc': 0.9488, 'worst_acc': 0.2418, 'acc_0_0': 0.9358, 'acc_0_1': 0.9986, 'acc_1_0': 0.8873, 'acc_1_1': 0.2418, 'mean_acc': 0.9486}\n",
      "Test(class): {'weighted_mean_acc': 0.9518, 'worst_acc': 0.3222, 'acc_0_0': 0.9446, 'acc_0_1': 0.9963, 'acc_1_0': 0.8827, 'acc_1_1': 0.3222, 'mean_acc': 0.9508}\n",
      "--- Epoch 24 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.62, 'acc_0_0': 0.9645, 'acc_0_1': 0.9959, 'acc_1_0': 0.8903, 'acc_1_1': 0.62, 'mean_acc': 0.964}\n",
      "Val(class): {'weighted_mean_acc': 0.95, 'worst_acc': 0.4066, 'acc_0_0': 0.9545, 'acc_0_1': 0.9899, 'acc_1_0': 0.8525, 'acc_1_1': 0.4066, 'mean_acc': 0.9495}\n",
      "Test(class): {'weighted_mean_acc': 0.9504, 'worst_acc': 0.4889, 'acc_0_0': 0.9599, 'acc_0_1': 0.987, 'acc_1_0': 0.8419, 'acc_1_1': 0.4889, 'mean_acc': 0.9512}\n",
      "--- Epoch 25 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6345, 'acc_0_0': 0.9647, 'acc_0_1': 0.9957, 'acc_1_0': 0.8888, 'acc_1_1': 0.6345, 'mean_acc': 0.9639}\n",
      "Val(class): {'weighted_mean_acc': 0.9492, 'worst_acc': 0.3077, 'acc_0_0': 0.9456, 'acc_0_1': 0.9964, 'acc_1_0': 0.8615, 'acc_1_1': 0.3077, 'mean_acc': 0.9488}\n",
      "Test(class): {'weighted_mean_acc': 0.9508, 'worst_acc': 0.3778, 'acc_0_0': 0.9545, 'acc_0_1': 0.9932, 'acc_1_0': 0.85, 'acc_1_1': 0.3778, 'mean_acc': 0.951}\n",
      "--- Epoch 26 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6244, 'acc_0_0': 0.9654, 'acc_0_1': 0.9962, 'acc_1_0': 0.8895, 'acc_1_1': 0.6244, 'mean_acc': 0.9645}\n",
      "Val(class): {'weighted_mean_acc': 0.9474, 'worst_acc': 0.3846, 'acc_0_0': 0.9571, 'acc_0_1': 0.9932, 'acc_1_0': 0.817, 'acc_1_1': 0.3846, 'mean_acc': 0.9466}\n",
      "Test(class): {'weighted_mean_acc': 0.9518, 'worst_acc': 0.4556, 'acc_0_0': 0.9625, 'acc_0_1': 0.992, 'acc_1_0': 0.831, 'acc_1_1': 0.4556, 'mean_acc': 0.9528}\n",
      "--- Epoch 27 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6482, 'acc_0_0': 0.9648, 'acc_0_1': 0.996, 'acc_1_0': 0.89, 'acc_1_1': 0.6482, 'mean_acc': 0.9644}\n",
      "Val(class): {'weighted_mean_acc': 0.9471, 'worst_acc': 0.4725, 'acc_0_0': 0.9302, 'acc_0_1': 0.9906, 'acc_1_0': 0.9019, 'acc_1_1': 0.4725, 'mean_acc': 0.9471}\n",
      "Test(class): {'weighted_mean_acc': 0.9516, 'worst_acc': 0.4222, 'acc_0_0': 0.9433, 'acc_0_1': 0.991, 'acc_1_0': 0.8948, 'acc_1_1': 0.4222, 'mean_acc': 0.9506}\n",
      "--- Epoch 28 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.646, 'acc_0_0': 0.9653, 'acc_0_1': 0.9963, 'acc_1_0': 0.8909, 'acc_1_1': 0.646, 'mean_acc': 0.9648}\n",
      "Val(class): {'weighted_mean_acc': 0.9503, 'worst_acc': 0.4505, 'acc_0_0': 0.9482, 'acc_0_1': 0.9915, 'acc_1_0': 0.8664, 'acc_1_1': 0.4505, 'mean_acc': 0.9499}\n",
      "Test(class): {'weighted_mean_acc': 0.9501, 'worst_acc': 0.4722, 'acc_0_0': 0.9534, 'acc_0_1': 0.9893, 'acc_1_0': 0.8544, 'acc_1_1': 0.4722, 'mean_acc': 0.9503}\n",
      "--- Epoch 29 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6467, 'acc_0_0': 0.9658, 'acc_0_1': 0.9959, 'acc_1_0': 0.8924, 'acc_1_1': 0.6467, 'mean_acc': 0.9651}\n",
      "Val(class): {'weighted_mean_acc': 0.951, 'worst_acc': 0.1978, 'acc_0_0': 0.9618, 'acc_0_1': 0.9954, 'acc_1_0': 0.833, 'acc_1_1': 0.1978, 'mean_acc': 0.9502}\n",
      "Test(class): {'weighted_mean_acc': 0.9484, 'worst_acc': 0.2944, 'acc_0_0': 0.9639, 'acc_0_1': 0.995, 'acc_1_0': 0.8032, 'acc_1_1': 0.2944, 'mean_acc': 0.9496}\n",
      "--- Epoch 30 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6568, 'acc_0_0': 0.9668, 'acc_0_1': 0.9961, 'acc_1_0': 0.8913, 'acc_1_1': 0.6568, 'mean_acc': 0.9656}\n",
      "Val(class): {'weighted_mean_acc': 0.9523, 'worst_acc': 0.4176, 'acc_0_0': 0.9503, 'acc_0_1': 0.9949, 'acc_1_0': 0.8664, 'acc_1_1': 0.4176, 'mean_acc': 0.9519}\n",
      "Test(class): {'weighted_mean_acc': 0.9515, 'worst_acc': 0.3611, 'acc_0_0': 0.955, 'acc_0_1': 0.9931, 'acc_1_0': 0.8548, 'acc_1_1': 0.3611, 'mean_acc': 0.9516}\n",
      "--- Epoch 31 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6676, 'acc_0_0': 0.9661, 'acc_0_1': 0.9964, 'acc_1_0': 0.8928, 'acc_1_1': 0.6676, 'mean_acc': 0.9657}\n",
      "Val(class): {'weighted_mean_acc': 0.9463, 'worst_acc': 0.3407, 'acc_0_0': 0.9299, 'acc_0_1': 0.9957, 'acc_1_0': 0.89, 'acc_1_1': 0.3407, 'mean_acc': 0.9461}\n",
      "Test(class): {'weighted_mean_acc': 0.9487, 'worst_acc': 0.3889, 'acc_0_0': 0.9396, 'acc_0_1': 0.9942, 'acc_1_0': 0.8786, 'acc_1_1': 0.3889, 'mean_acc': 0.9477}\n",
      "--- Epoch 32 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6583, 'acc_0_0': 0.9664, 'acc_0_1': 0.9959, 'acc_1_0': 0.8948, 'acc_1_1': 0.6583, 'mean_acc': 0.9659}\n",
      "Val(class): {'weighted_mean_acc': 0.947, 'worst_acc': 0.1648, 'acc_0_0': 0.9515, 'acc_0_1': 0.9981, 'acc_1_0': 0.8309, 'acc_1_1': 0.1648, 'mean_acc': 0.9462}\n",
      "Test(class): {'weighted_mean_acc': 0.95, 'worst_acc': 0.3, 'acc_0_0': 0.9562, 'acc_0_1': 0.9971, 'acc_1_0': 0.8323, 'acc_1_1': 0.3, 'mean_acc': 0.9503}\n",
      "--- Epoch 33 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6683, 'acc_0_0': 0.967, 'acc_0_1': 0.9965, 'acc_1_0': 0.8934, 'acc_1_1': 0.6683, 'mean_acc': 0.9662}\n",
      "Val(class): {'weighted_mean_acc': 0.9492, 'worst_acc': 0.4286, 'acc_0_0': 0.9616, 'acc_0_1': 0.9894, 'acc_1_0': 0.8246, 'acc_1_1': 0.4286, 'mean_acc': 0.9485}\n",
      "Test(class): {'weighted_mean_acc': 0.9471, 'worst_acc': 0.4833, 'acc_0_0': 0.9633, 'acc_0_1': 0.987, 'acc_1_0': 0.8077, 'acc_1_1': 0.4833, 'mean_acc': 0.9486}\n",
      "--- Epoch 34 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6741, 'acc_0_0': 0.9669, 'acc_0_1': 0.9962, 'acc_1_0': 0.8967, 'acc_1_1': 0.6741, 'mean_acc': 0.9666}\n",
      "Val(class): {'weighted_mean_acc': 0.949, 'worst_acc': 0.4945, 'acc_0_0': 0.9447, 'acc_0_1': 0.9915, 'acc_1_0': 0.8657, 'acc_1_1': 0.4945, 'mean_acc': 0.9487}\n",
      "Test(class): {'weighted_mean_acc': 0.9507, 'worst_acc': 0.4833, 'acc_0_0': 0.9502, 'acc_0_1': 0.9894, 'acc_1_0': 0.8677, 'acc_1_1': 0.4833, 'mean_acc': 0.9506}\n",
      "--- Epoch 35 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6669, 'acc_0_0': 0.9671, 'acc_0_1': 0.9964, 'acc_1_0': 0.895, 'acc_1_1': 0.6669, 'mean_acc': 0.9665}\n",
      "Val(class): {'weighted_mean_acc': 0.9481, 'worst_acc': 0.2857, 'acc_0_0': 0.9351, 'acc_0_1': 0.9957, 'acc_1_0': 0.89, 'acc_1_1': 0.2857, 'mean_acc': 0.9479}\n",
      "Test(class): {'weighted_mean_acc': 0.9519, 'worst_acc': 0.3944, 'acc_0_0': 0.9425, 'acc_0_1': 0.9944, 'acc_1_0': 0.8911, 'acc_1_1': 0.3944, 'mean_acc': 0.9508}\n",
      "--- Epoch 36 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6727, 'acc_0_0': 0.9672, 'acc_0_1': 0.9962, 'acc_1_0': 0.897, 'acc_1_1': 0.6727, 'mean_acc': 0.9667}\n",
      "Val(class): {'weighted_mean_acc': 0.9422, 'worst_acc': 0.5165, 'acc_0_0': 0.9182, 'acc_0_1': 0.9889, 'acc_1_0': 0.9068, 'acc_1_1': 0.5165, 'mean_acc': 0.9423}\n",
      "Test(class): {'weighted_mean_acc': 0.946, 'worst_acc': 0.5222, 'acc_0_0': 0.9333, 'acc_0_1': 0.9867, 'acc_1_0': 0.8923, 'acc_1_1': 0.5222, 'mean_acc': 0.9447}\n",
      "--- Epoch 37 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6907, 'acc_0_0': 0.9675, 'acc_0_1': 0.996, 'acc_1_0': 0.8972, 'acc_1_1': 0.6907, 'mean_acc': 0.967}\n",
      "Val(class): {'weighted_mean_acc': 0.9492, 'worst_acc': 0.1978, 'acc_0_0': 0.9597, 'acc_0_1': 0.9957, 'acc_1_0': 0.826, 'acc_1_1': 0.1978, 'mean_acc': 0.9484}\n",
      "Test(class): {'weighted_mean_acc': 0.9485, 'worst_acc': 0.3222, 'acc_0_0': 0.9622, 'acc_0_1': 0.9959, 'acc_1_0': 0.8052, 'acc_1_1': 0.3222, 'mean_acc': 0.9497}\n",
      "--- Epoch 38 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6849, 'acc_0_0': 0.9681, 'acc_0_1': 0.9965, 'acc_1_0': 0.8984, 'acc_1_1': 0.6849, 'mean_acc': 0.9676}\n",
      "Val(class): {'weighted_mean_acc': 0.9455, 'worst_acc': 0.3956, 'acc_0_0': 0.9405, 'acc_0_1': 0.9886, 'acc_1_0': 0.8685, 'acc_1_1': 0.3956, 'mean_acc': 0.9451}\n",
      "Test(class): {'weighted_mean_acc': 0.9489, 'worst_acc': 0.4333, 'acc_0_0': 0.9455, 'acc_0_1': 0.9894, 'acc_1_0': 0.8722, 'acc_1_1': 0.4333, 'mean_acc': 0.9484}\n",
      "--- Epoch 39 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.682, 'acc_0_0': 0.9676, 'acc_0_1': 0.9964, 'acc_1_0': 0.8993, 'acc_1_1': 0.682, 'mean_acc': 0.9674}\n",
      "Val(class): {'weighted_mean_acc': 0.9497, 'worst_acc': 0.3516, 'acc_0_0': 0.9583, 'acc_0_1': 0.9932, 'acc_1_0': 0.8316, 'acc_1_1': 0.3516, 'mean_acc': 0.949}\n",
      "Test(class): {'weighted_mean_acc': 0.9482, 'worst_acc': 0.3944, 'acc_0_0': 0.9607, 'acc_0_1': 0.9923, 'acc_1_0': 0.8137, 'acc_1_1': 0.3944, 'mean_acc': 0.9493}\n",
      "--- Epoch 40 ---\n",
      "Train-1 (Feature Learning): {'worst_acc': 0.6957, 'acc_0_0': 0.9681, 'acc_0_1': 0.9964, 'acc_1_0': 0.8997, 'acc_1_1': 0.6957, 'mean_acc': 0.9678}\n",
      "Val(class): {'weighted_mean_acc': 0.946, 'worst_acc': 0.3297, 'acc_0_0': 0.9585, 'acc_0_1': 0.9942, 'acc_1_0': 0.8031, 'acc_1_1': 0.3297, 'mean_acc': 0.9451}\n",
      "Test(class): {'weighted_mean_acc': 0.9468, 'worst_acc': 0.3333, 'acc_0_0': 0.9609, 'acc_0_1': 0.9939, 'acc_1_0': 0.802, 'acc_1_1': 0.3333, 'mean_acc': 0.948}\n",
      "--- Epoch 41 ---\n",
      "================== Stage 2) New adapter for Balanced-Text-Prompt ==================\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.3626, 'acc_0_0': 0.989, 'acc_0_1': 1.0, 'acc_1_0': 0.6813, 'acc_1_1': 0.3626, 'mean_acc': 0.7582}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9377, 'worst_acc': 0.7143, 'acc_0_0': 0.9276, 'acc_0_1': 0.9763, 'acc_1_0': 0.8699, 'acc_1_1': 0.7143, 'mean_acc': 0.9376}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9401, 'worst_acc': 0.6333, 'acc_0_0': 0.938, 'acc_0_1': 0.9748, 'acc_1_0': 0.8641, 'acc_1_1': 0.6333, 'mean_acc': 0.9399}\n",
      "--- Epoch 42 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.8681, 'acc_0_0': 0.9121, 'acc_0_1': 0.9341, 'acc_1_0': 0.8681, 'acc_1_1': 0.8791, 'mean_acc': 0.8984}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.927, 'worst_acc': 0.8132, 'acc_0_0': 0.914, 'acc_0_1': 0.9509, 'acc_1_0': 0.9047, 'acc_1_1': 0.8132, 'mean_acc': 0.9271}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9289, 'worst_acc': 0.7667, 'acc_0_0': 0.9241, 'acc_0_1': 0.9486, 'acc_1_0': 0.896, 'acc_1_1': 0.7667, 'mean_acc': 0.9285}\n",
      "--- Epoch 43 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.8791, 'acc_0_0': 0.9121, 'acc_0_1': 0.8791, 'acc_1_0': 0.9231, 'acc_1_1': 0.9121, 'mean_acc': 0.9066}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8561, 'worst_acc': 0.8093, 'acc_0_0': 0.8093, 'acc_0_1': 0.8673, 'acc_1_0': 0.9652, 'acc_1_1': 0.9341, 'mean_acc': 0.8572}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8671, 'worst_acc': 0.8434, 'acc_0_0': 0.8434, 'acc_0_1': 0.8616, 'acc_1_0': 0.954, 'acc_1_1': 0.9222, 'mean_acc': 0.8647}\n",
      "--- Epoch 44 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.8462, 'acc_0_0': 0.8901, 'acc_0_1': 0.8681, 'acc_1_0': 0.8462, 'acc_1_1': 0.9231, 'mean_acc': 0.8819}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9207, 'worst_acc': 0.4066, 'acc_0_0': 0.8559, 'acc_0_1': 0.9952, 'acc_1_0': 0.9374, 'acc_1_1': 0.4066, 'mean_acc': 0.9216}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9342, 'worst_acc': 0.2722, 'acc_0_0': 0.8886, 'acc_0_1': 0.9967, 'acc_1_0': 0.9343, 'acc_1_1': 0.2722, 'mean_acc': 0.9295}\n",
      "--- Epoch 45 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.8901, 'acc_0_0': 0.967, 'acc_0_1': 0.956, 'acc_1_0': 0.956, 'acc_1_1': 0.8901, 'mean_acc': 0.9423}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8979, 'worst_acc': 0.8636, 'acc_0_0': 0.8636, 'acc_0_1': 0.9154, 'acc_1_0': 0.952, 'acc_1_1': 0.9341, 'mean_acc': 0.8986}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9029, 'worst_acc': 0.8888, 'acc_0_0': 0.8888, 'acc_0_1': 0.905, 'acc_1_0': 0.9411, 'acc_1_1': 0.9, 'mean_acc': 0.9015}\n",
      "--- Epoch 46 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.8791, 'acc_0_0': 0.8791, 'acc_0_1': 0.9341, 'acc_1_0': 0.9121, 'acc_1_1': 0.978, 'mean_acc': 0.9258}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9355, 'worst_acc': 0.7473, 'acc_0_0': 0.9213, 'acc_0_1': 0.9737, 'acc_1_0': 0.8796, 'acc_1_1': 0.7473, 'mean_acc': 0.9355}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9356, 'worst_acc': 0.6, 'acc_0_0': 0.9314, 'acc_0_1': 0.9731, 'acc_1_0': 0.8597, 'acc_1_1': 0.6, 'mean_acc': 0.9352}\n",
      "--- Epoch 47 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9121, 'acc_0_0': 0.9121, 'acc_0_1': 0.978, 'acc_1_0': 0.9341, 'acc_1_1': 0.967, 'mean_acc': 0.9478}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9096, 'worst_acc': 0.8896, 'acc_0_0': 0.8896, 'acc_0_1': 0.9203, 'acc_1_0': 0.9402, 'acc_1_1': 0.9231, 'mean_acc': 0.91}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9107, 'worst_acc': 0.8889, 'acc_0_0': 0.9028, 'acc_0_1': 0.9141, 'acc_1_0': 0.9266, 'acc_1_1': 0.8889, 'mean_acc': 0.9099}\n",
      "--- Epoch 48 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9231, 'acc_0_0': 0.956, 'acc_0_1': 0.989, 'acc_1_0': 0.9231, 'acc_1_1': 0.989, 'mean_acc': 0.9643}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.938, 'worst_acc': 0.8462, 'acc_0_0': 0.9356, 'acc_0_1': 0.9654, 'acc_1_0': 0.8713, 'acc_1_1': 0.8462, 'mean_acc': 0.9379}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9356, 'worst_acc': 0.7444, 'acc_0_0': 0.94, 'acc_0_1': 0.9587, 'acc_1_0': 0.8657, 'acc_1_1': 0.7444, 'mean_acc': 0.9361}\n",
      "--- Epoch 49 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9451, 'acc_0_0': 0.9451, 'acc_0_1': 0.967, 'acc_1_0': 0.956, 'acc_1_1': 0.978, 'mean_acc': 0.9615}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8908, 'worst_acc': 0.8194, 'acc_0_0': 0.8194, 'acc_0_1': 0.9381, 'acc_1_0': 0.9763, 'acc_1_1': 0.8901, 'mean_acc': 0.8922}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8994, 'worst_acc': 0.8478, 'acc_0_0': 0.8478, 'acc_0_1': 0.9293, 'acc_1_0': 0.9758, 'acc_1_1': 0.8722, 'mean_acc': 0.8946}\n",
      "--- Epoch 50 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.967, 'acc_0_0': 0.967, 'acc_0_1': 0.967, 'acc_1_0': 0.967, 'acc_1_1': 0.989, 'mean_acc': 0.9725}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9305, 'worst_acc': 0.7582, 'acc_0_0': 0.896, 'acc_0_1': 0.9674, 'acc_1_0': 0.9415, 'acc_1_1': 0.7582, 'mean_acc': 0.931}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9345, 'worst_acc': 0.6611, 'acc_0_0': 0.9126, 'acc_0_1': 0.9646, 'acc_1_0': 0.9319, 'acc_1_1': 0.6611, 'mean_acc': 0.9323}\n",
      "--- Epoch 51 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.978, 'acc_0_0': 0.978, 'acc_0_1': 0.989, 'acc_1_0': 0.989, 'acc_1_1': 0.978, 'mean_acc': 0.9835}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9081, 'worst_acc': 0.8775, 'acc_0_0': 0.8775, 'acc_0_1': 0.927, 'acc_1_0': 0.9506, 'acc_1_1': 0.8791, 'mean_acc': 0.9087}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9137, 'worst_acc': 0.8167, 'acc_0_0': 0.8973, 'acc_0_1': 0.9226, 'acc_1_0': 0.9452, 'acc_1_1': 0.8167, 'mean_acc': 0.9121}\n",
      "--- Epoch 52 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9341, 'acc_0_0': 0.9341, 'acc_0_1': 0.989, 'acc_1_0': 0.956, 'acc_1_1': 0.989, 'mean_acc': 0.967}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8877, 'worst_acc': 0.8034, 'acc_0_0': 0.8034, 'acc_0_1': 0.9447, 'acc_1_0': 0.9861, 'acc_1_1': 0.8681, 'mean_acc': 0.8893}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9028, 'worst_acc': 0.8333, 'acc_0_0': 0.8465, 'acc_0_1': 0.9388, 'acc_1_0': 0.9778, 'acc_1_1': 0.8333, 'mean_acc': 0.8976}\n",
      "--- Epoch 53 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9341, 'acc_0_0': 0.9341, 'acc_0_1': 0.978, 'acc_1_0': 0.9451, 'acc_1_1': 0.978, 'mean_acc': 0.9588}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9263, 'worst_acc': 0.8791, 'acc_0_0': 0.9297, 'acc_0_1': 0.9384, 'acc_1_0': 0.8831, 'acc_1_1': 0.8791, 'mean_acc': 0.9261}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9239, 'worst_acc': 0.8611, 'acc_0_0': 0.9439, 'acc_0_1': 0.9214, 'acc_1_0': 0.8722, 'acc_1_1': 0.8611, 'mean_acc': 0.9258}\n",
      "--- Epoch 54 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9341, 'acc_0_0': 0.967, 'acc_0_1': 0.9341, 'acc_1_0': 0.978, 'acc_1_1': 0.978, 'mean_acc': 0.9643}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9142, 'worst_acc': 0.8571, 'acc_0_0': 0.877, 'acc_0_1': 0.9427, 'acc_1_0': 0.9506, 'acc_1_1': 0.8571, 'mean_acc': 0.9148}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9162, 'worst_acc': 0.85, 'acc_0_0': 0.8947, 'acc_0_1': 0.9328, 'acc_1_0': 0.9387, 'acc_1_1': 0.85, 'mean_acc': 0.9142}\n",
      "--- Epoch 55 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9121, 'acc_0_0': 0.9121, 'acc_0_1': 1.0, 'acc_1_0': 0.956, 'acc_1_1': 0.989, 'mean_acc': 0.9643}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9271, 'worst_acc': 0.8791, 'acc_0_0': 0.9269, 'acc_0_1': 0.9369, 'acc_1_0': 0.9019, 'acc_1_1': 0.8791, 'mean_acc': 0.927}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9237, 'worst_acc': 0.85, 'acc_0_0': 0.9347, 'acc_0_1': 0.9282, 'acc_1_0': 0.8806, 'acc_1_1': 0.85, 'mean_acc': 0.9248}\n",
      "--- Epoch 56 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9121, 'acc_0_0': 0.9121, 'acc_0_1': 0.956, 'acc_1_0': 0.9121, 'acc_1_1': 0.978, 'mean_acc': 0.9396}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9007, 'worst_acc': 0.8318, 'acc_0_0': 0.8318, 'acc_0_1': 0.949, 'acc_1_0': 0.9784, 'acc_1_1': 0.8462, 'mean_acc': 0.902}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9114, 'worst_acc': 0.7722, 'acc_0_0': 0.8627, 'acc_0_1': 0.9468, 'acc_1_0': 0.969, 'acc_1_1': 0.7722, 'mean_acc': 0.9068}\n",
      "--- Epoch 57 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9451, 'acc_0_0': 0.9451, 'acc_0_1': 0.978, 'acc_1_0': 0.9451, 'acc_1_1': 0.989, 'mean_acc': 0.9643}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9134, 'worst_acc': 0.8681, 'acc_0_0': 0.9065, 'acc_0_1': 0.9244, 'acc_1_0': 0.9054, 'acc_1_1': 0.8681, 'mean_acc': 0.9134}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9181, 'worst_acc': 0.8333, 'acc_0_0': 0.923, 'acc_0_1': 0.9192, 'acc_1_0': 0.9048, 'acc_1_1': 0.8333, 'mean_acc': 0.9185}\n",
      "--- Epoch 58 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9451, 'acc_0_0': 0.9451, 'acc_0_1': 0.967, 'acc_1_0': 0.978, 'acc_1_1': 0.978, 'mean_acc': 0.967}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.8681, 'worst_acc': 0.7748, 'acc_0_0': 0.7748, 'acc_0_1': 0.9265, 'acc_1_0': 0.9889, 'acc_1_1': 0.8791, 'mean_acc': 0.8699}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8831, 'worst_acc': 0.8159, 'acc_0_0': 0.8159, 'acc_0_1': 0.922, 'acc_1_0': 0.9839, 'acc_1_1': 0.8222, 'mean_acc': 0.8769}\n",
      "--- Epoch 59 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9231, 'acc_0_0': 0.9231, 'acc_0_1': 0.989, 'acc_1_0': 0.956, 'acc_1_1': 0.967, 'mean_acc': 0.9588}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9292, 'worst_acc': 0.8571, 'acc_0_0': 0.9356, 'acc_0_1': 0.942, 'acc_1_0': 0.8761, 'acc_1_1': 0.8571, 'mean_acc': 0.9289}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9271, 'worst_acc': 0.7556, 'acc_0_0': 0.9418, 'acc_0_1': 0.9382, 'acc_1_0': 0.8589, 'acc_1_1': 0.7556, 'mean_acc': 0.9285}\n",
      "--- Epoch 60 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9451, 'acc_0_0': 0.9451, 'acc_0_1': 0.956, 'acc_1_0': 0.9451, 'acc_1_1': 0.9451, 'mean_acc': 0.9478}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.881, 'worst_acc': 0.8308, 'acc_0_0': 0.8308, 'acc_0_1': 0.9014, 'acc_1_0': 0.9756, 'acc_1_1': 0.9231, 'mean_acc': 0.882}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.8923, 'worst_acc': 0.8599, 'acc_0_0': 0.8599, 'acc_0_1': 0.8991, 'acc_1_0': 0.9746, 'acc_1_1': 0.8722, 'mean_acc': 0.8891}\n",
      "--- Epoch 61 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9231, 'acc_0_0': 0.9231, 'acc_0_1': 0.978, 'acc_1_0': 0.9341, 'acc_1_1': 0.989, 'mean_acc': 0.956}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9201, 'worst_acc': 0.8782, 'acc_0_0': 0.9299, 'acc_0_1': 0.9241, 'acc_1_0': 0.8782, 'acc_1_1': 0.9121, 'mean_acc': 0.9199}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9202, 'worst_acc': 0.8389, 'acc_0_0': 0.9389, 'acc_0_1': 0.9232, 'acc_1_0': 0.8577, 'acc_1_1': 0.8389, 'mean_acc': 0.922}\n",
      "--- Epoch 62 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.9451, 'acc_0_0': 0.956, 'acc_0_1': 0.956, 'acc_1_0': 0.9451, 'acc_1_1': 0.956, 'mean_acc': 0.9533}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9438, 'worst_acc': 0.7473, 'acc_0_0': 0.9466, 'acc_0_1': 0.9756, 'acc_1_0': 0.8539, 'acc_1_1': 0.7473, 'mean_acc': 0.9434}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9455, 'worst_acc': 0.6167, 'acc_0_0': 0.9539, 'acc_0_1': 0.9792, 'acc_1_0': 0.8407, 'acc_1_1': 0.6167, 'mean_acc': 0.9463}\n",
      "--- Epoch 63 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9121, 'acc_0_0': 0.956, 'acc_0_1': 0.989, 'acc_1_0': 0.9121, 'acc_1_1': 0.989, 'mean_acc': 0.9615}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9154, 'worst_acc': 0.8571, 'acc_0_0': 0.8709, 'acc_0_1': 0.9495, 'acc_1_0': 0.9582, 'acc_1_1': 0.8571, 'mean_acc': 0.9161}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9229, 'worst_acc': 0.8111, 'acc_0_0': 0.8946, 'acc_0_1': 0.9452, 'acc_1_0': 0.9532, 'acc_1_1': 0.8111, 'mean_acc': 0.9202}\n",
      "--- Epoch 64 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Group prompt): {'worst_acc': 0.967, 'acc_0_0': 0.967, 'acc_0_1': 0.978, 'acc_1_0': 0.978, 'acc_1_1': 1.0, 'mean_acc': 0.9808}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9343, 'worst_acc': 0.8022, 'acc_0_0': 0.9105, 'acc_0_1': 0.9635, 'acc_1_0': 0.9311, 'acc_1_1': 0.8022, 'mean_acc': 0.9346}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9364, 'worst_acc': 0.7556, 'acc_0_0': 0.9252, 'acc_0_1': 0.9606, 'acc_1_0': 0.9121, 'acc_1_1': 0.7556, 'mean_acc': 0.9354}\n",
      "--- Epoch 65 ---\n",
      "Train-2 (Balanced Learning)(new adapter)(Class prompt): {'worst_acc': 0.9011, 'acc_0_0': 1.0, 'acc_0_1': 0.967, 'acc_1_0': 0.9011, 'acc_1_1': 0.956, 'mean_acc': 0.956}\n",
      "Val(class)(new adapter): {'weighted_mean_acc': 0.9367, 'worst_acc': 0.8022, 'acc_0_0': 0.91, 'acc_0_1': 0.9703, 'acc_1_0': 0.9304, 'acc_1_1': 0.8022, 'mean_acc': 0.9371}\n",
      "Test(class)(new adapter): {'weighted_mean_acc': 0.9388, 'worst_acc': 0.6944, 'acc_0_0': 0.9235, 'acc_0_1': 0.9692, 'acc_1_0': 0.9125, 'acc_1_1': 0.6944, 'mean_acc': 0.9373}\n",
      "========================================================================\n",
      "> end of training. \n",
      "\n",
      "best epoch : 47\n",
      "best training accuracy on [class]: {'worst_acc': 0.9121, 'acc_0_0': 0.9121, 'acc_0_1': 0.978, 'acc_1_0': 0.9341, 'acc_1_1': 0.967, 'mean_acc': 0.9478}\n",
      "best validation accuracy on [class]: {'weighted_mean_acc': 0.9096, 'worst_acc': 0.8896, 'acc_0_0': 0.8896, 'acc_0_1': 0.9203, 'acc_1_0': 0.9402, 'acc_1_1': 0.9231, 'mean_acc': 0.91}\n",
      "best test accuracy on [class]: {'weighted_mean_acc': 0.9107, 'worst_acc': 0.8889, 'acc_0_0': 0.9028, 'acc_0_1': 0.9141, 'acc_1_0': 0.9266, 'acc_1_1': 0.8889, 'mean_acc': 0.9099}\n",
      "========================================================================\n",
      "> start evaluating feature quality of best model. (using zero-shot prediction)\n",
      "\n",
      "zero-shot prediction (test) (class): {'weighted_mean_acc': 0.9107, 'worst_acc': 0.8889, 'acc_0_0': 0.9028, 'acc_0_1': 0.9141, 'acc_1_0': 0.9266, 'acc_1_1': 0.8889, 'mean_acc': 0.9099}\n",
      "zero-shot prediction (test) (spurious): {'weighted_mean_acc': 0.9837, 'worst_acc': 0.8778, 'acc_0_0': 0.9974, 'acc_0_1': 0.9667, 'acc_1_0': 0.9968, 'acc_1_1': 0.8778, 'mean_acc': 0.9847}\n",
      " ㄴ Note that it is related to [richness of non-target (spurious) information] (-> 'mean_acc' is important)\n",
      "========================================================================\n",
      "========================================================================\n",
      "> end\n",
      "Final Results:               weighted_mean_acc  worst_acc  acc_0_0  acc_0_1  acc_1_0  acc_1_1   \n",
      "1                       0.9142     0.9000   0.9172   0.9111   0.9149   0.9000  \\\n",
      "2                       0.9107     0.8889   0.9028   0.9141   0.9266   0.8889   \n",
      "test_mean               0.9124     0.8944   0.9100   0.9126   0.9208   0.8944   \n",
      "test_std                0.0018     0.0055   0.0072   0.0015   0.0058   0.0055   \n",
      "1                       0.9643     0.9358   0.9358   0.9895   0.9786   0.9833   \n",
      "2                       0.9837     0.8778   0.9974   0.9667   0.9968   0.8778   \n",
      "zs_spu_mean             0.9740     0.9068   0.9666   0.9781   0.9877   0.9306   \n",
      "zs_spu_std              0.0097     0.0290   0.0308   0.0114   0.0091   0.0527   \n",
      "1                          NaN     0.9560   0.9560   0.9780   0.9670   0.9780   \n",
      "2                          NaN     0.9121   0.9121   0.9780   0.9341   0.9670   \n",
      "tr_mean                    NaN     0.9340   0.9340   0.9780   0.9506   0.9725   \n",
      "tr_std                     NaN     0.0219   0.0219   0.0000   0.0164   0.0055   \n",
      "1                       0.9108     0.8988   0.8988   0.9149   0.9367   0.9011   \n",
      "2                       0.9096     0.8896   0.8896   0.9203   0.9402   0.9231   \n",
      "val_mean                0.9102     0.8942   0.8942   0.9176   0.9384   0.9121   \n",
      "val_std                 0.0006     0.0046   0.0046   0.0027   0.0018   0.0110   \n",
      "1                       0.9142     0.9000   0.9172   0.9111   0.9149   0.9000   \n",
      "2                       0.9107     0.8889   0.9028   0.9141   0.9266   0.8889   \n",
      "zs_tg_mean              0.9124     0.8944   0.9100   0.9126   0.9208   0.8944   \n",
      "zs_tg_std               0.0018     0.0055   0.0072   0.0015   0.0058   0.0055   \n",
      "\n",
      "             mean_acc  \n",
      "1              0.9144  \n",
      "2              0.9099  \n",
      "test_mean      0.9122  \n",
      "test_std       0.0022  \n",
      "1              0.9618  \n",
      "2              0.9847  \n",
      "zs_spu_mean    0.9732  \n",
      "zs_spu_std     0.0115  \n",
      "1              0.9698  \n",
      "2              0.9478  \n",
      "tr_mean        0.9588  \n",
      "tr_std         0.0110  \n",
      "1              0.9110  \n",
      "2              0.9100  \n",
      "val_mean       0.9105  \n",
      "val_std        0.0005  \n",
      "1              0.9144  \n",
      "2              0.9099  \n",
      "zs_tg_mean     0.9122  \n",
      "zs_tg_std      0.0022  \n",
      "Save to:  results_iterative/ds_celeba_tl_adapter_reg_seq_alter_bs_1024_lr_0.1_lrr1.0_bsr4_balval_MA+rn.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' :\n",
    "    opt = parse_option()\n",
    "    \n",
    "    epochs=65\n",
    "    epochs_feature_learning=40 \n",
    "    lr_decay_epochs='62,64'\n",
    "    dataset=\"celeba\"\n",
    "    data_dir=\"/home/jinsu/workstation/project/debiasing-multi-modal/data/celeba\"\n",
    "    \n",
    "\n",
    "    tl_method=\"adapter_reg_seq_alter\"\n",
    "    target=\"class\" \n",
    "    non_target=\"spurious\" \n",
    "\n",
    "    opt.bs_list=\"1024\"\n",
    "    opt.bsr_list=\"4\" \n",
    "    opt.lr_list=\"1e-1\"\n",
    "\n",
    "    opt.epochs = epochs\n",
    "    opt.epochs_feature_learning = epochs_feature_learning\n",
    "    opt.dataset=dataset\n",
    "    opt.text_embedding_dir = f\"data/embeddings_unnormalized/{dataset}/clip_{target}.json\"\n",
    "    opt.text_spurious_embedding_dir = f\"data/embeddings_unnormalized/{dataset}/clip_{non_target}.json\"\n",
    "    opt.text_group_embedding_dir = f\"data/embeddings_unnormalized/{dataset}/clip_group.json\"\n",
    "    opt.image_embedding_dir = f\"data/embeddings_unnormalized/{dataset}/RN50/clip.json\"\n",
    "    opt.data_dir=data_dir\n",
    "    opt.tl_method=tl_method\n",
    "    opt.train_target=target\n",
    "    opt.warm_reg = True\n",
    "    opt.lr_decay_rate=0.1\n",
    "    opt.lr_decay_epochs=lr_decay_epochs\n",
    "    opt.add_adapter=True\n",
    "    \n",
    "    opt.save_results = True\n",
    "    opt.balance_val = True\n",
    "    opt.lr_multiple = 10.0\n",
    "    opt.num_iter=2\n",
    "    \n",
    "    \n",
    "    \n",
    "    opt = refine_option(opt)\n",
    "    # Data Loader: Model 당 한 번만 수행하자. (CPU 아파 죽는 것 같음. num_workers 16 / \n",
    "    \n",
    "    print(f\"> Start Transfer Learning using [{opt.tl_method}]\")\n",
    "    print('========================================================================')\n",
    "    if opt.dataset == 'waterbirds':\n",
    "        # build data loader                \n",
    "        if opt.tl_method in [\"adapter_reg\", \"adapter_reg_seq\", 'adapter_reg_seq_alter'] :\n",
    "            from data.waterbirds_embeddings_reg import WaterbirdsEmbeddings, load_waterbirds_embeddings\n",
    "            print(f\"Load image embedding of Waterbirds: {opt.image_embedding_dir}\")\n",
    "            opt.trainset = WaterbirdsEmbeddings(opt.data_dir, 'train', opt.image_embedding_dir, None)\n",
    "            print(\"Load Data Loader (train, validation, test)\")\n",
    "            opt.train_loader, opt.reg_loader, opt.val_loader, opt.test_loader = load_waterbirds_embeddings(opt.data_dir, opt.image_embedding_dir, opt.batch_size, opt.batch_size_reg)\n",
    "        else:\n",
    "            from data.waterbirds_embeddings import WaterbirdsEmbeddings, load_waterbirds_embeddings\n",
    "            print(f\"Load image embedding of Waterbirds: {opt.image_embedding_dir}\")\n",
    "            trainset = WaterbirdsEmbeddings(opt.data_dir, 'train', opt.image_embedding_dir, None)\n",
    "            print(f\"ㄴ Corresponding text embedding of Waterbirds: {opt.text_embedding_dir}\")\n",
    "            opt.train_loader, opt.val_loader, opt.test_loader = load_waterbirds_embeddings(opt.data_dir, opt.image_embedding_dir, opt.batch_size, opt.batch_size)\n",
    "        \n",
    "        if opt.train_target == \"class\":\n",
    "            print(f\"Training target : {opt.train_target} (Land bird(0) / Water bird(1))\")\n",
    "        elif opt.train_target == \"spurious\":\n",
    "            print(f\"Training target : {opt.train_target} (Land background(0) / Water background(1))\")\n",
    "        \n",
    "    elif opt.dataset == 'celeba':\n",
    "        # build dataset example.\n",
    "        from data.celeba_embeddings import CelebaEmbeddings, load_celeba_embeddings # 버근가.. 왜 인식을 몬하지\n",
    "        print(f\"Load embedding of CelebA: {opt.image_embedding_dir}\")\n",
    "        \n",
    "        opt.trainset = CelebaEmbeddings(opt.data_dir, 'train', opt.image_embedding_dir, None)\n",
    "        \n",
    "        # build data loader\n",
    "        print(\"Load Data Loader (train, validation, test)\")\n",
    "        \n",
    "        train_loader, val_loader, test_loader = load_celeba_embeddings(opt.data_dir, opt.image_embedding_dir, opt.batch_size, opt.batch_size)\n",
    "        if opt.tl_method in [\"adapter_reg\", \"adapter_reg_seq\", 'adapter_reg_seq_alter'] :\n",
    "            from data.celeba_embeddings_reg import CelebaEmbeddings, load_celeba_embeddings\n",
    "            print(f\"Load embedding of CelebA: {opt.image_embedding_dir}\")\n",
    "            opt.trainset = CelebaEmbeddings(opt.data_dir, 'train', opt.image_embedding_dir, None)\n",
    "            print(\"Load Data Loader (train, validation, test)\")\n",
    "            opt.train_loader, opt.reg_loader, opt.val_loader, opt.test_loader = load_celeba_embeddings(opt.data_dir, opt.image_embedding_dir, opt.batch_size, opt.batch_size_reg)\n",
    "        else:\n",
    "            print(f\"Load image embedding of Waterbirds: {opt.image_embedding_dir}\")\n",
    "            opt.trainset = CelebaEmbeddings(opt.data_dir, 'train', opt.image_embedding_dir, None)\n",
    "            print(f\"ㄴ Corresponding text embedding of Waterbirds: {opt.text_embedding_dir}\")\n",
    "            opt.train_loader, opt.val_loader, opt.test_loader = load_celeba_embeddings(opt.data_dir, opt.image_embedding_dir, opt.batch_size, opt.batch_size)\n",
    "        \n",
    "        # print training target\n",
    "        if opt.train_target == \"class\":\n",
    "            print(f\"Training target : {opt.train_target} (non-blond hair(0) / blond hair(1))\")\n",
    "        elif opt.train_target == \"spurious\":\n",
    "            print(f\"Training target : {opt.train_target} (female(0) / male(1))\")\n",
    "    \n",
    "    # set the path according to the environment\n",
    "    lr_list = opt.lr_list.split(',')\n",
    "    lr_list = [float(lr) for lr in lr_list]\n",
    "    bs_list = opt.bs_list.split(',')\n",
    "    bs_list = [int(bs) for bs in bs_list]\n",
    "    \n",
    "    # for adapter_reg, adapter_reg_seq, adapter_reg_seq_ma.\n",
    "    bsr_list = opt.bsr_list.split(',')\n",
    "    bsr_list = [int(bsr) for bsr in bsr_list]\n",
    "    \n",
    "    if opt.tl_method ==\"adapter\":\n",
    "        bsr_list = [128] # 혹시 argument 잘못 넣었을 때를 대비.\n",
    "        \n",
    "    for lr in lr_list:\n",
    "        for bs in bs_list:\n",
    "            for bsr in bsr_list:\n",
    "                opt.learning_rate = lr\n",
    "                opt.learning_rate_reg = lr * opt.lr_multiple\n",
    "                opt.batch_size = bs\n",
    "                opt.batch_size_reg = bsr\n",
    "    \n",
    "                for iter in range(1, opt.num_iter+1):     \n",
    "                    print(f\"f=============Iteration : {iter}/{opt.num_iter}=============\")        \n",
    "                    \n",
    "                    set_seed(opt.random_seeds[iter-1])\n",
    "                    \n",
    "                    (tr_group_acc, val_group_acc, test_group_acc), (zs_target, zs_spurious)= train_all_epochs(opt)\n",
    "                    \n",
    "                    if iter==1:\n",
    "                        tr_df = pd.DataFrame(tr_group_acc, index=[iter])\n",
    "                        val_df = pd.DataFrame(val_group_acc, index=[iter])\n",
    "                        test_df = pd.DataFrame(test_group_acc, index=[iter])\n",
    "                        zs_target_df = pd.DataFrame(zs_target, index=[iter])\n",
    "                        zs_spurious_df = pd.DataFrame(zs_spurious, index=[iter])\n",
    "                    else:\n",
    "                        tr_df = pd.concat([tr_df, pd.DataFrame(tr_group_acc, index=[iter])])\n",
    "                        val_df = pd.concat([val_df, pd.DataFrame(val_group_acc, index=[iter])])\n",
    "                        test_df = pd.concat([test_df, pd.DataFrame(test_group_acc, index=[iter])])\n",
    "                        zs_target_df = pd.concat([zs_target_df, pd.DataFrame(zs_target, index=[iter])])\n",
    "                        zs_spurious_df = pd.concat([zs_spurious_df, pd.DataFrame(zs_spurious, index=[iter])])\n",
    "                    \n",
    "                tr_df = pd.concat([tr_df, pd.DataFrame(tr_df.mean().to_dict(), index=['tr_mean'])])\n",
    "                tr_df = pd.concat([tr_df, pd.DataFrame(tr_df.std().to_dict(), index=['tr_std'])])\n",
    "                val_df = pd.concat([val_df, pd.DataFrame(val_df.mean().to_dict(), index=['val_mean'])])\n",
    "                val_df = pd.concat([val_df, pd.DataFrame(val_df.std().to_dict(), index=['val_std'])])\n",
    "                test_df = pd.concat([test_df, pd.DataFrame(test_df.mean().to_dict(), index=['test_mean'])])\n",
    "                test_df = pd.concat([test_df, pd.DataFrame(test_df.std().to_dict(), index=['test_std'])])\n",
    "                zs_target_df = pd.concat([zs_target_df, pd.DataFrame(zs_target_df.mean().to_dict(), index=['zs_tg_mean'])])\n",
    "                zs_target_df = pd.concat([zs_target_df, pd.DataFrame(zs_target_df.std().to_dict(), index=['zs_tg_std'])])\n",
    "                zs_spurious_df = pd.concat([zs_spurious_df, pd.DataFrame(zs_spurious_df.mean().to_dict(), index=['zs_spu_mean'])])\n",
    "                zs_spurious_df = pd.concat([zs_spurious_df, pd.DataFrame(zs_spurious_df.std().to_dict(), index=['zs_spu_std'])])\n",
    "                \n",
    "                \n",
    "                final_df = pd.concat([test_df, zs_spurious_df, tr_df, val_df, zs_target_df])\n",
    "                result_root = \"results_iterative\"\n",
    "                if not os.path.exists(result_root):\n",
    "                    os.mkdir(result_root)\n",
    "                \n",
    "                final_result_file_path = f\"ds_{opt.dataset}_tl_{opt.tl_method}_bs_{opt.batch_size}_lr_{opt.learning_rate}\"\n",
    "                \n",
    "                if \"reg\" in opt.tl_method:\n",
    "                    final_result_file_path += f\"_lrr{opt.learning_rate_reg}_bsr{opt.batch_size_reg}\"\n",
    "                \n",
    "                    if opt.balance_val:\n",
    "                        final_result_file_path += \"_balval\"\n",
    "                    \n",
    "                    if opt.tl_method != \"adapter_reg_seq_alter\":      \n",
    "                        if opt.use_cls_prompt_in_reg:\n",
    "                            final_result_file_path += f\"_CP\"\n",
    "                        else:\n",
    "                            final_result_file_path += f\"_GP\"\n",
    "                        \n",
    "                    if opt.add_adapter:\n",
    "                        final_result_file_path += f\"_MA\"\n",
    "                        if opt.init_near_identity:\n",
    "                            final_result_file_path += \"+ni\"\n",
    "                        else:\n",
    "                            final_result_file_path += \"+rn\"\n",
    "                    \n",
    "                    if opt.continue_from_best and ('seq' in opt.tl_method):\n",
    "                        final_result_file_name+=\"_cont\"\n",
    "                \n",
    "                \n",
    "                if opt.resample_ce:\n",
    "                    final_result_file_path+=\"_rs\"\n",
    "                \n",
    "                final_df = final_df.round(4)\n",
    "                print(\"Final Results: \", final_df)\n",
    "                print(\"Save to: \", os.path.join(result_root, final_result_file_path)+'.csv')\n",
    "                final_df.to_csv(os.path.join(result_root, final_result_file_path)+'.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

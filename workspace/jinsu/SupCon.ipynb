{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinsu/anaconda3/envs/dl_mmd/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append(\"/home/jinsu/workstation/project/debiasing-multi-modal\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from util import AverageMeter\n",
    "from util import adjust_learning_rate, warmup_learning_rate, accuracy\n",
    "from util import set_optimizer\n",
    "\n",
    "from data.waterbirds_embeddings import WaterbirdsEmbeddings, load_waterbirds_embeddings\n",
    "from data.celeba_embeddings import CelebaEmbeddings, load_celeba_embeddings\n",
    "model_dict = {'resnet50': [None, 1024]} # (nn.module, 1024)\n",
    "new_order_for_print = [\n",
    "    'weighted_mean_acc',\n",
    "    'worst_acc',\n",
    "    'acc_0_0',\n",
    "    'acc_0_1',\n",
    "    'acc_1_0',\n",
    "    'acc_1_1',\n",
    "    'mean_acc'\n",
    "]\n",
    "from functools import partial\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.fc(features)\n",
    "\n",
    "\n",
    "\n",
    "class CustomCLIP(nn.Module):\n",
    "    def __init__(self, adapter, text_embedding_dir, text_spurious_embedding_dir, temperature=0.01, head = None, ca_feat_dim = 128):\n",
    "        super().__init__()\n",
    "        self.text_embedding_dir = text_embedding_dir\n",
    "        self.text_spurious_embedding_dir = text_spurious_embedding_dir\n",
    "        self.adapter = adapter\n",
    "        self.ca_feat_dim = ca_feat_dim\n",
    "        \n",
    "        # Contrastive Adapter\n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(self.adapter.input_dim, ca_feat_dim)\n",
    "        \n",
    "        self.temperature = temperature # CA default : 0.01, B2T default : 0.02 (?) NOTE\n",
    "        \n",
    "        self.text_features = get_text_embedding(self.text_embedding_dir)\n",
    "        self.text_spurious_features = get_text_embedding(self.text_spurious_embedding_dir)\n",
    "        \n",
    "    def forward(self, features): \n",
    "        image_features =  self.adapter(features) # Un-normalized (B, 1024)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "\n",
    "        text_features = self.text_features # (Pre) Normalized (B, 2, 1024)\n",
    "        \n",
    "        logits = image_features @ text_features / self.temperature # (B, 1024) X (B, 2, 1024) = # (B, 2)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def forward_spurious(self, features): \n",
    "        image_features =  self.adapter(features) # Un-normalized (B, 1024)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "\n",
    "        text_spurious_features = self.text_spurious_features # (Pre) Normalized (B, 2, 1024)\n",
    "        \n",
    "        logits = image_features @ text_spurious_features / self.temperature # (B, 1024) X (B, 2, 1024) = # (B, 2)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def forward_ca(self, x):\n",
    "        x = x / x.norm(dim=-1, keepdim=True)\n",
    "        feat = self.adapter(x)\n",
    "        proj = self.head(feat)\n",
    "        proj = proj / proj.norm(dim=-1, keepdim=True)\n",
    "        return proj\n",
    "        \n",
    "class Adapter(nn.Module):\n",
    "    \"\"\"\n",
    "    - Residual connetion : 제외 (original Adapter - 0.2*images + 0.8*adapter)\n",
    "    - Hidden dimension : args.adapter_feat_dim (original Adatper - input_dim // 4)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    def forward(self, features):\n",
    "        return self.layers(features)\n",
    "\n",
    "def parse_option():\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--print_freq', type=int, default=10,\n",
    "                        help='print frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=50,\n",
    "                        help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=16,\n",
    "                        help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='number of training epochs')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-3, \n",
    "                        help='learning rate') # Tuning needed. \n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',\n",
    "                        help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=1,\n",
    "                        help='decay rate for learning rate') \n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-5,\n",
    "                        help='weight decay') # Tuning needed. \n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='momentum')\n",
    "\n",
    "    # model dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet50')\n",
    "    parser.add_argument('--dataset', type=str, default='waterbirds',\n",
    "                        choices=['celeba', 'waterbirds'], help='dataset')\n",
    "\n",
    "    # other setting\n",
    "    parser.add_argument('--cosine', action='store_true',\n",
    "                        help='using cosine annealing') # Tuning needed. \n",
    "    parser.add_argument('--warm', action='store_true',\n",
    "                        help='warm-up for large batch training') # Tuning needed. \n",
    "\n",
    "    parser.add_argument('--image_embedding_dir', type=str,\n",
    "                        help='extracted image embedding')\n",
    "    parser.add_argument('--text_embedding_dir', type=str,\n",
    "                        help='extracted text embedding')\n",
    "    parser.add_argument('--text_spurious_embedding_dir', type=str,\n",
    "                        help='extracted text embedding (about spurious attributes)')\n",
    "    parser.add_argument('--train_target', type=str, default=\"class\", choices=[\"class\", \"spurious\", \"group\"]) # label for prediction.\n",
    "    parser.add_argument('--data_dir', type=str,\n",
    "                    help='folder, in which metadata.csv] exist')\n",
    "    parser.add_argument('--tl_method', type=str, default= \"linear_probing\", choices=[\"linear_probing\", \"adapter\", \"contrastive_adapter\", \"ETC\"]\n",
    "                        ,help='transfer learning method')\n",
    "    parser.add_argument('--adapter_feat_dim', type=int, default= 128, help='reduced dimension in adapter')\n",
    "    parser.add_argument('--zs_temperature', type=float, default= 0.01, help='Temperature in zero-shot prediction')\n",
    "    parser.add_argument('--watch_batch_results', type=bool, default=False, help='Print results in each bach by [opt.print_freq]. Recommdned: True when single-run of CelebA(Large # of batch), False others')\n",
    "    parser.add_argument('--save_results', type=bool, default=True, help='Save the results of transfer learning (and final feature quality) in the folder where ')\n",
    "    \n",
    "\n",
    "    # parser.add_argument('--lr_linear_probing', type=float, default=1e-3, chocies=[1e-3, 1e-2, 1e-1, 1, 3, 10], help='learning rate for linear probing') # Tuning needed. \n",
    "      # -> Zero-shot으로 대체하는 게 맞을듯.\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "\n",
    "    # set the path according to the environment\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "    if opt.warm:\n",
    "        opt.warmup_from = 0.01\n",
    "        opt.warm_epochs = 10\n",
    "        if opt.cosine:\n",
    "            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "        else:\n",
    "            opt.warmup_to = opt.learning_rate\n",
    "            \n",
    "    if opt.dataset == 'celeba':\n",
    "        opt.n_cls = 2\n",
    "    elif opt.dataset == 'waterbirds':\n",
    "        opt.n_cls = 2\n",
    "    else:\n",
    "        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "def set_model(opt):\n",
    "    # model = SupConResNet(name=opt.model)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    _ , input_dim = model_dict[opt.model] # (Encoder(not use), feature dim)\n",
    "    \n",
    "    if opt.tl_method =='linear_probing':\n",
    "        print(\"Off-the-shelf classifier : [Linear Classifier]\")\n",
    "        classifier = LinearClassifier(input_dim = input_dim, num_classes = opt.n_cls)\n",
    "    elif opt.tl_method =='adapter':\n",
    "        print(\"Off-the-shelf classifier : [Adapter + (temperatured) image-text jointly normalized prediction]\")\n",
    "        adapter = Adapter(input_dim = input_dim, hidden_dim = opt.adapter_feat_dim) # Fixed by heuristics\n",
    "        classifier = CustomCLIP(adapter, opt.text_embedding_dir, opt.text_spurious_embedding_dir, temperature=opt.zs_temperature)\n",
    "    elif opt.tl_method =='contrastive_adapter':\n",
    "        print(\"Off-the-shelf classifier : Contrastive Adapter\")\n",
    "        adapter = Adapter(input_dim = input_dim, hidden_dim = opt.adapter_feat_dim) # Fixed by heuristics\n",
    "        classifier = CustomCLIP(adapter, opt.text_embedding_dir, opt.text_spurious_embedding_dir, temperature=opt.zs_temperature, head=opt.ca_head, ca_feat_dim=opt.ca_feat_dim)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        classifier = classifier.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    return classifier, criterion # model, \n",
    "\n",
    "# Group-wise Accuracy Update.\n",
    "def update_dict(acc_groups, y, g, logits):\n",
    "    preds = torch.argmax(logits, axis=1)\n",
    "    correct_batch = (preds == y)\n",
    "    g = g.cpu()\n",
    "    for g_val in np.unique(g):\n",
    "        mask = g == g_val\n",
    "        n = mask.sum().item()\n",
    "        corr = correct_batch[mask].sum().item()\n",
    "        acc_groups[g_val].update(corr / n, n) \n",
    "\n",
    "\n",
    "# Mean/Worst acc (not weighted average)\n",
    "def get_results(acc_groups, get_yp_func): # Input 중 acc_groups : AverageMeter()를 담고있는 dict. get_yp_func : 미리 partial을 이용해 n_groups를 저장해놓음. \n",
    "    groups = acc_groups.keys() # 0, 1, 2, 3\n",
    "    results = {\n",
    "            f\"acc_{get_yp_func(g)[0]}_{get_yp_func(g)[1]}\": acc_groups[g].avg\n",
    "            for g in groups\n",
    "    }\n",
    "    all_correct = sum([acc_groups[g].sum for g in groups])\n",
    "    all_total = sum([acc_groups[g].count for g in groups])\n",
    "    results.update({\"mean_acc\" : all_correct / all_total})\n",
    "    results.update({\"worst_acc\" : min(results.values())})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Group -> class / spurious attributes\n",
    "def get_y_p(g, n_places):\n",
    "    y = g // n_places\n",
    "    p = g % n_places\n",
    "    return y, p\n",
    "\n",
    "def get_text_embedding(text_embedding_dir):\n",
    "    with open(text_embedding_dir, 'r') as f:\n",
    "        text_embeddings = json.load(f)\n",
    "\n",
    "    text_features = []\n",
    "    for class_template, class_embedding in text_embeddings.items():\n",
    "        text_features.append(torch.tensor(class_embedding))\n",
    "    text_features = torch.stack(text_features, dim=1).cuda() # (B, 2, 1024)\n",
    "    \n",
    "    \n",
    "    return text_features\n",
    "\n",
    "def train_one_epoch(opt, train_loader, classifier, criterion, optimizer, epoch, get_yp_func, target, print_label='Train'): # model,\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    # model.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    acc_groups = {g_idx : AverageMeter() for g_idx in range(train_loader.dataset.n_groups)}\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, data in enumerate(train_loader):  \n",
    "        \n",
    "        embeddings, all_labels, img_filenames = data # all_labels.keys() : ['class', 'group', 'spurious', 'ebd_pred'(CLIP-zeroshot)] \n",
    "        labels = all_labels[target] # target : one of [y, spurious, group]\n",
    "        groups = all_labels['group'] # For evaluating group accuracy (and further developing group-information-aware approaches)\n",
    "    \n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        embeddings = embeddings.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # warm-up learning rate\n",
    "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
    "\n",
    "        # compute loss\n",
    "        output = classifier(embeddings.detach()) \n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "        acc1 = accuracy(output, labels, bsz)\n",
    "        acc.update(acc1, bsz)\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # optimizer.step() # NOTE\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Update acc dict\n",
    "        update_dict(acc_groups, labels, groups, output)\n",
    "        \n",
    "        if opt.watch_batch_results:\n",
    "            if (idx + 1) % opt.print_freq == 0:\n",
    "                print(f'{print_label}: [{0}][{1}/{2}]\\t'\n",
    "                    'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                    'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
    "                    'Acc@1 {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                    epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                    data_time=data_time, loss=losses, acc=acc))\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "    group_acc = get_results(acc_groups, get_yp_func) # NOTE declared in [def main]\n",
    "    group_acc = {key: group_acc[key] for key in new_order_for_print[1:]}\n",
    "    group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "    print(f\"{print_label}:\", str(group_acc))\n",
    "    \n",
    "    return losses.avg, acc.avg, group_acc\n",
    "\n",
    "def train_one_epoch_cl(opt, train_loader, classifier, contrastive_loss, optimizer, epoch, print_label='Train'):\n",
    "    \"\"\"\n",
    "    Train contrastive epoch\n",
    "    \"\"\"\n",
    "    classifier.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    # acc = AverageMeter()\n",
    "    # acc_groups = {g_idx : AverageMeter() for g_idx in range(train_loader.dataset.n_groups)}\n",
    "\n",
    "    # contrastive_weight = args.contrastive_weight\n",
    "    loss_compute_size = int(opt.num_anchor +\n",
    "                            opt.num_negative +\n",
    "                            opt.num_positive)\n",
    "    \n",
    "    end = time.time()\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        # Setup main contrastive batch\n",
    "        ## 순서대로 임베딩, \n",
    "        all_batch_inputs, all_batch_labels, _ = batch_data\n",
    "        \n",
    "        all_batch_inputs = all_batch_inputs.cuda(non_blocking=True) \n",
    "        # 각각 1개의 Anchor, N개의 Positive , M개의 Negative\n",
    "        # 총 opt.batch_factor 개의 Triplet. \n",
    "        batch_inputs = torch.split(all_batch_inputs,\n",
    "                                  loss_compute_size)\n",
    "        \n",
    "        # 기본적인 CA에선 사용 x\n",
    "        # all_batch_labels, all_batch_group, all_batch_spurious, all_batch_zs_pred = all_batch_labels.values()\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # warm-up learning rate\n",
    "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
    "        \n",
    "        neg_start_ix = opt.num_anchor + opt.num_positive\n",
    "        neg_end_ix = neg_start_ix + opt.num_negative\n",
    "        \n",
    "        for ix, batch_input in enumerate(batch_inputs):\n",
    "            inputs_a = batch_input[:opt.num_anchor]\n",
    "            inputs_p = batch_input[opt.num_anchor:neg_start_ix]\n",
    "            inputs_n = batch_input[neg_start_ix:neg_end_ix]\n",
    "\n",
    "            # Just do contrastive loss against first anchor for now\n",
    "            inputs_a_ = [inputs_a[0]] # [1, 1024]\n",
    "\n",
    "            # in Contrastive Adapter, iterated over only \"single\" anchor\n",
    "            for anchor_ix, input_a in enumerate(inputs_a_):\n",
    "                contrastive_batch = torch.vstack((input_a.unsqueeze(0),\n",
    "                                                  inputs_p, inputs_n))\n",
    "                # compute loss\n",
    "                # loss = contrastive_loss(classifier, contrastive_batch) # anchor 1개에 대한 Loss\n",
    "                loss, pos_numerator_last, neg_numerator_last, denominator_last = contrastive_loss(classifier, contrastive_batch) # anchor 1개에 대한 Loss # 관찰용\n",
    "                contrastive_batch = contrastive_batch.detach().cpu()\n",
    "                \n",
    "            # update metric\n",
    "            \n",
    "            \n",
    "            losses.update(loss.item(), 1)\n",
    "            # acc1 = accuracy(output, labels, 1)\n",
    "            # acc.update(acc1, 1)\n",
    "\n",
    "            # SGD\n",
    "            \n",
    "            # for n, p in classifier.named_parameters():\n",
    "            #     if 'bias' not in n:\n",
    "            #         if len(p.shape)>1:\n",
    "            #             print(f\"Before({n})\", p[8, :3])\n",
    "            #         else:\n",
    "            #             print(f\"Before({n})\", p[8])            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # for n, p in classifier.named_parameters():\n",
    "            #     if 'bias' not in n:\n",
    "            #         if len(p.shape)>1:\n",
    "            #             print(f\"After({n})\", p[8, :3])\n",
    "            #         else:\n",
    "            #             print(f\"After({n})\", p[8])\n",
    "            \n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            # Update acc dict\n",
    "            # update_dict(acc_groups, labels, groups, output)\n",
    "\n",
    "        if opt.watch_batch_results:\n",
    "            if (idx + 1) % (opt.print_freq//4) == 0:\n",
    "                print(f'{print_label}: [{0}][{1}/{2}]\\t'\n",
    "                    'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                    'loss {loss.val:.3f} ({loss.avg:.3f})\\t'.format(\n",
    "                    epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                    data_time=data_time, loss=losses))\n",
    "\n",
    "    # group_acc = get_results(acc_groups, get_yp_func) # NOTE declared in [def main]\n",
    "    # group_acc = {key: group_acc[key] for key in new_order_for_print[1:]}\n",
    "    # group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "    \n",
    "    \n",
    "    print(f\"(Last) Averaged Pos-term in {print_label}:\", str(pos_numerator_last.item()))\n",
    "    print(f\"(Last) Averaged Neg-term in {print_label}:\", str(neg_numerator_last.item()))\n",
    "    print(f\"(Last) Denominator in {print_label}:\", str(denominator_last.item()))\n",
    "    print(f\"Loss in {print_label}:\", str(losses.avg))\n",
    "    \n",
    "\n",
    "\n",
    "    return losses.avg # , acc.avg, group_acc\n",
    "\n",
    "\n",
    "def validate(opt, val_loader, classifier, criterion, get_yp_func, train_group_ratio, target, print_label='Test'):\n",
    "    \"\"\"validation\"\"\"\n",
    "    \n",
    "    classifier.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    acc_groups = {g_idx : AverageMeter() for g_idx in range(val_loader.dataset.n_groups)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, data in enumerate(val_loader):\n",
    "            embeddings, all_labels, img_filenames = data # all_labels.keys() : ['class', 'group', 'spurious', 'ebd_pred'(CLIP-zeroshot)] \n",
    "            labels = all_labels[target] # target : one of [class, spurious, group]\n",
    "            groups = all_labels['group'] # For evaluating group accuracy (and further developing group-information-aware approaches)\n",
    "            \n",
    "            embeddings = embeddings.float().cuda()\n",
    "            labels = labels.cuda()\n",
    "            bsz = labels.shape[0]\n",
    "\n",
    "            # forward\n",
    "            output = classifier(embeddings)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # update metric\n",
    "            losses.update(loss.item(), bsz)\n",
    "            acc1 = accuracy(output, labels, bsz)\n",
    "            acc.update(acc1, bsz)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            # Update acc dict\n",
    "            update_dict(acc_groups, labels, groups, output)\n",
    "        \n",
    "            if opt.watch_batch_results:\n",
    "                if (idx+1) % opt.print_freq == 0:\n",
    "                    print(f'{print_label}: [{0}/{1}]\\t'\n",
    "                        'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                        'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                        'Acc@1 {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                        idx, len(val_loader), batch_time=batch_time,\n",
    "                        loss=losses, acc=acc))\n",
    "                    \n",
    "    group_acc = get_results(acc_groups, get_yp_func)\n",
    "\n",
    "    # NOTE Add Weighted mean acc.\n",
    "    groups = range(val_loader.dataset.n_groups) # 0, 1, 2, 3\n",
    "    group_acc_indiv =  [group_acc[f\"acc_{get_yp_func(g)[0]}_{get_yp_func(g)[1]}\"] for g in groups]\n",
    "    weighted_mean_acc = (np.array(group_acc_indiv) * np.array(train_group_ratio)).sum() # Weighted Sum \\\n",
    "    \n",
    "    group_acc[\"weighted_mean_acc\"] = weighted_mean_acc\n",
    "    group_acc = {key: group_acc[key] for key in new_order_for_print}\n",
    "    group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "    print(f\"{print_label}:\", str(group_acc))\n",
    "\n",
    "    return losses.avg, acc.avg, group_acc    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def validate_zs(opt, val_loader, classifier, criterion, get_yp_func, train_group_ratio, target, print_label='Zero-shot Prediction (Test) (Class)'):\n",
    "    \"\"\"(Feature quality) validation using zeroshot-prediction\"\"\"\n",
    "\n",
    "    classifier.eval()\n",
    "\n",
    "\n",
    "    if opt.tl_method in [\"linear_probing\"]:\n",
    "        temperature = opt.zs_temperature\n",
    "        \n",
    "        if target==\"class\":\n",
    "            text_embeddings = get_text_embedding(opt.text_embedding_dir)\n",
    "        elif target=='spurious':\n",
    "            text_embeddings = get_text_embedding(opt.text_spurious_embedding_dir)\n",
    "        \n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    acc_groups = {g_idx : AverageMeter() for g_idx in range(val_loader.dataset.n_groups)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, data in enumerate(val_loader):\n",
    "            image_embeddings, all_labels, img_filenames = data # all_labels.keys() : ['class', 'group', 'spurious', 'ebd_pred'(CLIP-zeroshot)] \n",
    "            labels = all_labels[target] # target : one of [class, spurious, group]\n",
    "            groups = all_labels['group'] # For evaluating group accuracy (and further developing group-information-aware approaches)\n",
    "            \n",
    "            image_embeddings = image_embeddings.float().cuda()\n",
    "            labels = labels.cuda()\n",
    "            bsz = labels.shape[0]\n",
    "            \n",
    "            if opt.tl_method in ['linear_probing']: # same to CLIP Embedding\n",
    "                image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True) # Normalized (B, 1024)\n",
    "                output = image_embeddings @ text_embeddings / temperature # (B, 1024) X (B, 2, 1024) = # (B, 2)\n",
    "                \n",
    "            elif opt.tl_method in ['adapter', 'contrastive_adapter']: # Adpater, Contrastive Adapter : Embedding -> (1) (Adapted) Embedding -> (2) ZeroShot prediction as logit    (CustomCLIP.forward : (1)+(2))\n",
    "                # forward\n",
    "                if target=='class':\n",
    "                    output = classifier(image_embeddings)\n",
    "                elif target=='spurious':\n",
    "                    output = classifier.forward_spurious(image_embeddings)\n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # update metric\n",
    "            losses.update(loss.item(), bsz)\n",
    "            acc1 = accuracy(output, labels, bsz)\n",
    "            acc.update(acc1, bsz)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            # Update acc dict\n",
    "            update_dict(acc_groups, labels, groups, output)\n",
    "        \n",
    "            if opt.watch_batch_results:\n",
    "                if (idx+1) % opt.print_freq == 0:\n",
    "                    print(f'{print_label}: [{0}/{1}]\\t'\n",
    "                        'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                        'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                        'Acc@1 {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                        idx, len(val_loader), batch_time=batch_time,\n",
    "                        loss=losses, acc=acc))\n",
    "                    \n",
    "    group_acc = get_results(acc_groups, get_yp_func)\n",
    "\n",
    "    # NOTE Add Weighted mean acc.\n",
    "    groups = range(val_loader.dataset.n_groups) # 0, 1, 2, 3\n",
    "    group_acc_indiv =  [group_acc[f\"acc_{get_yp_func(g)[0]}_{get_yp_func(g)[1]}\"] for g in groups]\n",
    "    weighted_mean_acc = (np.array(group_acc_indiv) * np.array(train_group_ratio)).sum() # Weighted Sum \\\n",
    "    \n",
    "    group_acc[\"weighted_mean_acc\"] = weighted_mean_acc\n",
    "    group_acc = {key: group_acc[key] for key in new_order_for_print}\n",
    "    group_acc = {key: np.round(value, 4) for key, value in group_acc.items()}\n",
    "    print(f\"{print_label}:\", str(group_acc))\n",
    "    \n",
    "    return losses.avg, acc.avg, group_acc    \n",
    "\n",
    "def train_all_epochs(opt):\n",
    "    best_acc = 0\n",
    "    best_epoch = 0\n",
    "    best_model = None\n",
    "    # opt = parse_option()\n",
    "    \n",
    "    \n",
    "    print(f\"> Start Transfer Learning using [{opt.tl_method}]\")\n",
    "    print('========================================================================')\n",
    "    if opt.dataset == 'waterbirds':\n",
    "        # build dataset example.\n",
    "        print(f\"Load image embedding of Waterbirds: {opt.image_embedding_dir}\")\n",
    "        trainset = WaterbirdsEmbeddings(opt.data_dir, 'train', opt.image_embedding_dir, None)\n",
    "        print(f\"ㄴ Corresponding text embedding of Waterbirds: {opt.text_embedding_dir}\")\n",
    "        # build data loader\n",
    "        print(\"Load Data Loader (train, validation, test)\")\n",
    "        train_loader, val_loader, test_loader = load_waterbirds_embeddings(opt.data_dir, opt.image_embedding_dir, opt.batch_size, opt.batch_size)\n",
    "        \n",
    "        # print training target\n",
    "        if opt.train_target == \"class\":\n",
    "            print(f\"Training target : {opt.train_target} (Land bird(0) / Water bird(1))\")\n",
    "        elif opt.train_target == \"spurious\":\n",
    "            print(f\"Training target : {opt.train_target} (Land background(0) / Water background(1))\")\n",
    "        \n",
    "    elif opt.dataset == 'celeba':\n",
    "        # build dataset example.\n",
    "        print(f\"Load embedding of CelebA: {opt.image_embedding_dir}\")\n",
    "        trainset = CelebaEmbeddings(opt.data_dir, 'train', opt.image_embedding_dir, None)\n",
    "        print(f\"ㄴ Corresponding text embedding of Waterbirds: {opt.text_embedding_dir}\")\n",
    "        # build data loader\n",
    "        print(\"Load Data Loader (train, validation, test)\")\n",
    "        train_loader, val_loader, test_loader = load_celeba_embeddings(opt.data_dir, opt.image_embedding_dir, opt.batch_size, opt.batch_size)\n",
    "        \n",
    "        # print training target\n",
    "        if opt.train_target == \"class\":\n",
    "            print(f\"Training target : {opt.train_target} (non-blond hair(0) / blond hair(1))\")\n",
    "        elif opt.train_target == \"spurious\":\n",
    "            print(f\"Training target : {opt.train_target} (female(0) / male(1))\")\n",
    "\n",
    "    # group information\n",
    "    get_yp_func = partial(get_y_p, n_places=trainset.n_places)\n",
    "    train_group_ratio = trainset.group_ratio\n",
    "    \n",
    "    # build model and criterion\n",
    "    classifier, criterion = set_model(opt) # model, \n",
    "\n",
    "    # build optimizer\n",
    "    print(\"Set Optimizer: SGD (default)\")\n",
    "    print('========================================================================')\n",
    "    optimizer = set_optimizer(opt, classifier)\n",
    "    \n",
    "    # training routine\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    train_group_accs = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_group_accs = []\n",
    "    \n",
    "    test_losses = [] # NOTE: Don't peek ! \n",
    "    test_accs = [] # NOTE: Don't peek ! \n",
    "    test_group_accs = [] # NOTE: Don't peek ! \n",
    "    \n",
    "    # entire training\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "        adjust_learning_rate(opt, optimizer, epoch)\n",
    "        print(f'--- Epoch {epoch} ---')\n",
    "        \n",
    "        # train one epoch\n",
    "        loss, acc, group_acc = train_one_epoch(opt, train_loader, classifier, criterion,\n",
    "                          optimizer, epoch, get_yp_func, target=opt.train_target, print_label=f'Train({opt.train_target})')\n",
    "        \n",
    "        train_losses.append(loss); train_accs.append(acc); train_group_accs.append(group_acc)\n",
    "        \n",
    "        # eval for one epoch\n",
    "        val_loss, val_acc, val_group_acc = validate(opt, val_loader, classifier, criterion, get_yp_func, train_group_ratio, target=opt.train_target, print_label=f'Val({opt.train_target})')\n",
    "        val_losses.append(val_loss); val_accs.append(val_acc); val_group_accs.append(val_group_acc)\n",
    "        \n",
    "        # update best epoch by worst_group accuracy (default)\n",
    "        if val_group_acc['worst_acc'] > best_acc:\n",
    "            best_acc = val_group_acc['worst_acc']\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(classifier)\n",
    "        \n",
    "        # test for one epoch\n",
    "        test_loss, test_acc, test_group_acc = validate(opt, test_loader, classifier, criterion, get_yp_func, train_group_ratio, target='class', print_label=f'Test({opt.train_target})')\n",
    "        \n",
    "        test_losses.append(test_loss); test_accs.append(test_acc); test_group_accs.append(test_group_acc)\n",
    "        \n",
    "\n",
    "    print('========================================================================')\n",
    "    print(\"> end of training. \\n\")\n",
    "    print('best epoch : {}'.format(best_epoch))\n",
    "    \n",
    "    best_train_group_acc = train_group_accs[best_epoch-1]\n",
    "    best_val_group_acc = val_group_accs[best_epoch-1]\n",
    "    best_test_group_acc = test_group_accs[best_epoch-1]\n",
    "    \n",
    "    print(f'best training accuracy on [{opt.train_target}]: {best_train_group_acc}')\n",
    "    print(f'best validation accuracy on [{opt.train_target}]: {best_val_group_acc}')\n",
    "    print(f'best test accuracy on [{opt.train_target}]: {best_test_group_acc}')\n",
    "    \n",
    "    # Evaluate Feature Quality using (Embedding-based) Zero-shot Prediction\n",
    "    print('========================================================================')\n",
    "    print(\"> start evaluating feature quality of best model. (using zero-shot prediction)\\n\")\n",
    "    \n",
    "    \n",
    "    # Zero-shot [class] prediction\n",
    "    zs_loss, zs_acc, zs_group_acc = validate_zs(opt, test_loader, best_model, criterion, get_yp_func, train_group_ratio, target=\"class\", print_label='zero-shot prediction (test) (class)')    \n",
    "    \n",
    "    if opt.tl_method in [\"linear_probing\"]:\n",
    "        print(f\" ㄴ Note that it should be same to [CLIP Zero-shot Baselines, of which worst acc is about 39%], in {opt.tl_method}\")\n",
    "    elif opt.tl_method in [\"adapter\", \"contrastive_adapt\"]: \n",
    "        print(f\" ㄴ Note that it should be same to [best test accuracy on [{opt.train_target}]], above, in {opt.tl_method}\")\n",
    "    \n",
    "    # Zero-shot [spurious] prediction\n",
    "    zs_loss_spurious, zs_acc_spurious, zs_group_acc_spurious = validate_zs(opt, test_loader, best_model, criterion, get_yp_func, train_group_ratio, target=\"spurious\", print_label='zero-shot prediction (test) (spurious)')    \n",
    "    print(f\" ㄴ Note that it is related to [richness of non-target (spurious) information] (-> 'mean_acc' is important)\")\n",
    "    \n",
    "    print('========================================================================')\n",
    "    # Recommendation : False when multiple training\n",
    "    if opt.save_results:\n",
    "        print('> Save results\\n')\n",
    "        all_results = {}\n",
    "        \n",
    "        for epoch in range(1, opt.epochs + 1):\n",
    "            all_results[f\"Epoch {epoch}\"] = {\"Train\": train_group_accs[epoch-1], \"Val\": test_group_accs[epoch-1], \"Test\": test_group_accs[epoch-1]}\n",
    "        \n",
    "        final_results = {\"Final Results (best epoch)\":  {f\"Epoch {best_epoch}\": {\"Train\": best_train_group_acc, \"Val\": best_val_group_acc, \"Test\": best_test_group_acc}}, \n",
    "                         \"Feature Quality (using zs)\":  {\"class\":  zs_group_acc, \"spurious\": zs_group_acc_spurious}, \n",
    "                         \"All Results (all epoch)\": all_results}\n",
    "        \n",
    "        # make result folder \n",
    "        final_result_folder = os.path.dirname(opt.image_embedding_dir).replace('data', 'results')\n",
    "        if not os.path.exists(final_result_folder):\n",
    "            os.makedirs(final_result_folder)\n",
    "            \n",
    "        image_ebd_file_name = os.path.basename(opt.image_embedding_dir).split(\".\")[0]\n",
    "        text_ebd_file_name = os.path.basename(opt.text_embedding_dir).split(\".\")[0]\n",
    "        \n",
    "        # result name\n",
    "        final_result_file_name = f\"im_{image_ebd_file_name}_t_{text_ebd_file_name}_tl_{opt.tl_method}_t_{opt.train_target}_lr_{opt.learning_rate}_bs_{opt.batch_size}\"\n",
    "        \n",
    "        # NOTE This file name can be modified when we add baselines\n",
    "        \"\"\"\n",
    "        E.g., if we use [flexable_adapter] and corresponding h.p. [flexable_weight], then, \n",
    "        if opt.tl_method == \"flexable_adpater\":\n",
    "            final_result_file_name += f\"_{opt.flexable_weight}\"\n",
    "        if opt.cosine:\n",
    "            opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "        if opt.warm:\n",
    "            opt.model_name = '{}_warm'.format(opt.model_name)\n",
    "        \"\"\"\n",
    "\n",
    "        # result path\n",
    "        final_result_file_path = os.path.join(final_result_folder, final_result_file_name + \".json\")\n",
    "        final_model_path = os.path.join(final_result_folder, final_result_file_name + \".pth\")\n",
    "        \n",
    "        print('final result path: ', final_result_file_path)\n",
    "        print('final model path: ', final_model_path)\n",
    "        \n",
    "        # save results, as json.\n",
    "        with open(final_result_file_path, \"w\") as f:\n",
    "            json.dump(final_results, f, indent=4)\n",
    "        \n",
    "        # save final model, as pth \n",
    "        torch.save(best_model.state_dict(), final_model_path)    \n",
    "            \n",
    "    \n",
    "    print('========================================================================')\n",
    "    print(\"> end\")\n",
    "    \n",
    "    return (best_train_group_acc, best_val_group_acc, best_test_group_acc), (train_group_accs, val_group_accs, test_group_accs) # (best_results, all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "parser.add_argument('--print_freq', type=int, default=20,\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--save_freq', type=int, default=50,\n",
    "                    help='save frequency')\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                    help='batch_size')\n",
    "parser.add_argument('--num_workers', type=int, default=16,\n",
    "                    help='num of workers to use')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='number of training epochs')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',\n",
    "                    help='where to decay lr, can be a list')\n",
    "parser.add_argument('--lr_decay_rate', type=float, default=1 ,\n",
    "                    help='decay rate for learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-5,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='momentum')\n",
    "\n",
    "# model dataset\n",
    "parser.add_argument('--model', type=str, default='resnet50')\n",
    "parser.add_argument('--dataset', type=str, default='waterbirds',\n",
    "                    choices=['celeba', 'waterbirds'], help='dataset')\n",
    "\n",
    "# other setting\n",
    "parser.add_argument('--cosine', action='store_true',\n",
    "                    help='using cosine annealing')\n",
    "parser.add_argument('--warm', action='store_true',\n",
    "                    help='warm-up for large batch training')\n",
    "\n",
    "parser.add_argument('--image_embedding_dir', type=str, \n",
    "                    help='extracted image embedding')\n",
    "parser.add_argument('--text_embedding_dir', type=str, \n",
    "                    help='extracted text embedding')\n",
    "parser.add_argument('--train_target', type=str, default=\"class\", choices=[\"class\", \"spurious\", \"group\"]) # Label for training.\n",
    "parser.add_argument('--data_dir', type=str,\n",
    "                    help='folder, in which [metadata.csv] exists')\n",
    "parser.add_argument('--tl_method', type=str, default=\"linear_probing\", choices=[\"linear_probing\", \"adapter\", \"contrastive_adapter\"]\n",
    "                        ,help='transfer learning method')\n",
    "parser.add_argument('--adapter_feat_dim', type=int, default= 128, help='reduced dimension in adapter')\n",
    "parser.add_argument('--watch_batch_results', type=bool, default=True, help='Print results in each bach by [opt.print_freq]. Recommdned: True when single-run of CelebA(Large dataset), False otherwises')\n",
    "\n",
    "parser.add_argument('--zs_temperature', type=float, default= 0.01, help='Temperature in zero-shot prediction')\n",
    "parser.add_argument('--save_results', type=bool, default=False, help='Save the results of transfer learning (and final feature quality) in the folder where ')\n",
    "opt = parser.parse_args(args=[])   \n",
    "\n",
    "iterations = opt.lr_decay_epochs.split(',')\n",
    "opt.lr_decay_epochs = list([])\n",
    "for it in iterations:\n",
    "    opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\\\n",
    "    format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n",
    "            opt.batch_size)\n",
    "\n",
    "if opt.cosine:\n",
    "    opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "# warm-up for large-batch training,\n",
    "if opt.warm:\n",
    "    opt.model_name = '{}_warm'.format(opt.model_name)\n",
    "    opt.warmup_from = 0.01\n",
    "    opt.warm_epochs = 10\n",
    "    if opt.cosine:\n",
    "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "        opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "    else:\n",
    "        opt.warmup_to = opt.learning_rate\n",
    "        \n",
    "if opt.dataset == 'celeba':\n",
    "    opt.n_cls = 2\n",
    "elif opt.dataset == 'waterbirds':\n",
    "    opt.n_cls = 2\n",
    "else:\n",
    "    raise ValueError('dataset not supported: {}'.format(opt.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opt.watch_batch_results = True\n",
    "opt.save_results = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.epochs = 100\n",
    "opt.learning_rate = 1e-3\n",
    "opt.batch_size = 128\n",
    "opt.batch_factor = 32 # 튜닝 필요\n",
    "opt.num_anchor = 1\n",
    "opt.num_positive = 2048\n",
    "opt.num_negative = 2048\n",
    "\n",
    "opt.anc_loss_temp = 0.5\n",
    "opt.pos_loss_temp = 0.5\n",
    "opt.neg_loss_temp = 0.5\n",
    "\n",
    "opt.cl_temperature = 0.1\n",
    "opt.ca_head = \"linear\" # linear or mlp\n",
    "opt.ca_feat_dim = 128 \n",
    "\n",
    "opt.dataset = 'waterbirds'\n",
    "\n",
    "opt.tl_method = \"contrastive_adapter\"\n",
    "opt.train_target = \"class\"\n",
    "\n",
    "non_target = \"spurious\"\n",
    "\n",
    "opt.text_embedding_dir = f\"/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings_unnormalized/{opt.dataset}/clip_{opt.train_target}.json\"\n",
    "opt.text_spurious_embedding_dir = f\"/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings_unnormalized/{opt.dataset}/clip_{non_target}.json\"\n",
    "opt.image_embedding_dir = f\"/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings_unnormalized/waterbirds/RN50/clip.json\"\n",
    "opt.data_dir=\"/home/jinsu/workstation/project/debiasing-multi-modal/data/waterbirds/waterbird_complete95_forest2water2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = train_all_epochs(opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CA-realted function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_anchors(anchor_class, anchor_dict, num_anchor):\n",
    "    p = None\n",
    "\n",
    "    num_samples = num_anchor\n",
    "    sample_indices = anchor_dict['ix_by_class'][anchor_class]\n",
    "    replace = True if num_samples > len(sample_indices) else False\n",
    "    sample_indices = np.random.choice(sample_indices,\n",
    "                                      size=num_samples,\n",
    "                                      replace=replace,\n",
    "                                      p=p)\n",
    "    return sample_indices\n",
    "\n",
    "\n",
    "def sample_positives(anchor_class, positives_by_class, num_positive):\n",
    "    positive_dict = positives_by_class[anchor_class]\n",
    "    p = None\n",
    "    num_samples = num_positive\n",
    "    replace = True if num_samples > len(positive_dict['ix']) else False\n",
    "\n",
    "    sample_indices = np.random.choice(np.arange(len(positive_dict['ix'])),\n",
    "                                      size=num_samples,\n",
    "                                      replace=replace,\n",
    "                                      p=p)\n",
    "    sample_slice_sources = positive_dict['source'][sample_indices]\n",
    "    sample_indices = positive_dict['ix'][sample_indices]\n",
    "    return sample_indices, sample_slice_sources\n",
    "\n",
    "\n",
    "def sample_negatives(negative_dict, num_negative):\n",
    "    p = None\n",
    "\n",
    "    num_samples = num_negative\n",
    "    replace = True if num_samples > len(negative_dict['ix']) else False\n",
    "    sample_indices = np.random.choice(negative_dict['ix'],\n",
    "                                      size=num_samples,\n",
    "                                      replace=replace,\n",
    "                                      p=p)\n",
    "    return sample_indices\n",
    "\n",
    "\n",
    "# Adjust number of negatives or positives if > sliced neg / pos\n",
    "def adjust_num_pos_neg_(positives_by_class, slice_negatives,\n",
    "                        args):\n",
    "    \n",
    "    print(f'given number of positives: {args.num_anchor}')\n",
    "    print(f'given number of positives: {args.num_positive}')\n",
    "    print(f'given number of negatives: {args.num_negative}')\n",
    "    num_pos = np.min([len(positives_by_class[c]['target'])\n",
    "                      for c in range(args.n_cls)])\n",
    "    num_neg = np.min([len(negative_dict['target'])\n",
    "                      for negative_dict in slice_negatives])\n",
    "    num_pos = np.min((args.num_positive, num_pos))\n",
    "    num_neg = np.min((args.num_negative, num_neg))\n",
    "\n",
    "    # Tentative\n",
    "    num_anc = np.min((args.num_anchor, np.min((num_pos, num_neg))))\n",
    "\n",
    "\n",
    "    # Adjust arguments\n",
    "    args.num_positive = num_pos\n",
    "    args.num_negative = num_neg\n",
    "    args.num_anchor = num_anc\n",
    "    print(f'Adjusted number of anchors:   {args.num_anchor}')\n",
    "    print(f'Adjusted number of positives: {args.num_positive}')\n",
    "    print(f'Adjusted number of negatives: {args.num_negative}')\n",
    "    \n",
    "# Adjust number of anchors or hard negatives if > sliced anc / neg\n",
    "def adjust_num_anc_neg_(slice_anchors, slice_negatives,\n",
    "                        args):\n",
    "    num_anc = np.min([len(anchor_dict['target'])\n",
    "                      for anchor_dict in slice_anchors])\n",
    "    num_neg = np.min([len(negative_dict['target'])\n",
    "                      for negative_dict in slice_negatives])\n",
    "    num_anc = np.min((args.num_anchor, num_anc))\n",
    "    # num_neg Because now both anchors and negatives are from the nonspurious groups\n",
    "    num_neg = np.min((args.num_negative_easy, num_anc))\n",
    "\n",
    "    # Adjust experiment name to reflect\n",
    "    # Adjust arguments\n",
    "    args.num_anchor = num_anc\n",
    "    args.num_negative_easy = num_neg\n",
    "    print(f'Adjusted number of anchors:   {args.num_anchor}')\n",
    "    print(f'Adjusted number of (hard) negatives: {args.num_negative_easy}')\n",
    "\n",
    "def compute_slice_indices(dataset):\n",
    "    \"\"\"\n",
    "    Get \"slices\" of data belonging to different subgroups from the pre-extracted embeddings\n",
    "    (cf. [data/embeddings_unnormalized/[waterbirds/celeba]/RN50/clip.json])\n",
    "\n",
    "    Args:\n",
    "    - dataset : Custom Dataset (cf. [[waterbirds/celeba]_embedding.py])\n",
    "    Returns:\n",
    "    - sliced_data_indices (int(np.array)[]): List of numpy arrays denoting indices of the dataloader.dataset\n",
    "                                             corresponding to different slices \n",
    "    \"\"\"\n",
    "    # First compute pseudolabels\n",
    "    \n",
    "    # pseudo_labels = torch.hstack(all_predicted) # [N, ]\n",
    "    # correct = torch.hstack(all_correct) # [N, \n",
    "    \n",
    "    embeddings_df = dataset.embeddings_df\n",
    "    train_indices = (embeddings_df.loc[\"split\"]==dataset.split_dict[trainset.split]).values\n",
    "    train_embeddings_df = embeddings_df.T[train_indices]\n",
    "    train_embeddings_df = train_embeddings_df.T\n",
    "    \n",
    "    pseudo_labels = train_embeddings_df.loc[\"y_pred\"].values # [0, 1, 1, 0, 1....]\n",
    "    labels =  train_embeddings_df.loc[\"y\"].values # [1, 1, 0, 0, 1, ....]\n",
    "    correct = (pseudo_labels == labels) # [False, True, False, True, True, ...]\n",
    "    \n",
    "    sliced_data_indices = []\n",
    "    all_correct = []\n",
    "    for label in np.unique(pseudo_labels): # 0으로 예측\n",
    "        group = np.where(pseudo_labels == label)[0] # [0, 3, ...] / [1, 2, 4, ...]\n",
    "        correct_by_group = correct[group] # [False, True, ...] / [True, False, True, ...]\n",
    "        \n",
    "        sliced_data_indices.append(group) # \n",
    "        all_correct.append(correct_by_group) \n",
    "    \n",
    "    \n",
    "    return sliced_data_indices, all_correct # [[0, 3,...], [1, 2, 4, ...]], [[False, True, ...], [True, False, True, ...]]\n",
    "\n",
    "\n",
    "def prepare_contrastive_points(dataset, sliced_data_indices,\n",
    "                               sliced_data_correct,\n",
    "                               ):\n",
    "    train_targets = dataset.y_array\n",
    "    train_spurious = dataset.confounder_array\n",
    "    sliced_data_indices_all = np.concatenate(sliced_data_indices)\n",
    "    sliced_data_correct_all = np.zeros(len(train_targets))\n",
    "    sliced_data_correct_all[sliced_data_indices_all] = np.concatenate(\n",
    "        sliced_data_correct)\n",
    "    \n",
    "    sliced_data_incorrect = []\n",
    "    for slice_ix, boolean_array in enumerate(sliced_data_correct):\n",
    "        sliced_data_incorrect.append(np.array([not bool for bool in boolean_array]))\n",
    "    sliced_data_incorrect = np.array(sliced_data_incorrect)\n",
    "    \n",
    "    all_anchors = {'slice_ix': np.zeros(len(train_targets)).astype(int), # 4765\n",
    "                   'in_slice_ix': np.zeros(len(train_targets)).astype(int)} # 4765\n",
    "\n",
    "    # Store all anchors and negatives\n",
    "    slice_anchors = [None] * len(sliced_data_indices)\n",
    "    slice_negatives = [None] * len(sliced_data_indices)\n",
    "    \n",
    "    # For positives, just specify by the ground-truth NOTE No.\n",
    "    # (These are the same as negatives in another slice, just organized by class) \n",
    "     ## another slice : 1 prediction. 0 class (즉, 그냥 틀린 친구들 in CnC)\n",
    "     \n",
    "     \n",
    "    # Cnc : Anchor -> correct \n",
    "      # Neg : Different class & Same Prediction\n",
    "      # Pos : Different prediction & Same class\n",
    "    # CA : Anchor -> incorrect\n",
    "    positives_by_class = {}\n",
    "\n",
    "    for slice_ix, data_indices in enumerate(sliced_data_indices): # slice_ix = 0 (즉, prediction=0일 때 기준 서술)\n",
    "        \n",
    "        target_class, target_counts = np.unique(train_targets[data_indices],\n",
    "                                                return_counts=True)\n",
    "        \n",
    "        for tc_ix, tc in enumerate(target_class):\n",
    "            print(f'>> Slice {slice_ix}, target: {tc}, counts: {target_counts[tc_ix]}')\n",
    "        \n",
    "        # Anchors are datapoints in the slice that the model got in-correct (False)\n",
    "        ix = np.where(sliced_data_incorrect[slice_ix])[0] # prediction 0, class 1\n",
    "        print(\n",
    "            f'Slice {slice_ix} % incorrect: {len(ix) / len(data_indices) * 100:<.4f} %')\n",
    "\n",
    "        slice_ix_anchors = {'ix': data_indices[ix],\n",
    "                            'target': train_targets[data_indices][ix], # Only 1\n",
    "                            'incorrect': sliced_data_incorrect[slice_ix][ix], # all True\n",
    "                            'source': np.ones(len(data_indices[ix])).astype(int) * slice_ix, # zero_prediction\n",
    "                            'spurious': train_spurious[data_indices][ix], # 0 or 1 (다만 1이 그 전체 양(5%)에 비해선 비교적 많을 것)\n",
    "                            'ix_by_class': {},} # 1: data_indices[ix] (Class 1)\n",
    "        # Zeroshot prediction 값([0/1])에 따른 data indices 들에 대한 정보들\n",
    "        \n",
    "        # anchor: prediction 0 -> class 1 (즉, zero-prediction에 대한 anchor는 모두 class 1) -> indices\n",
    "        for t in np.unique(train_targets[data_indices][ix]):\n",
    "            tix = np.where(train_targets[data_indices][ix] == t)[0]\n",
    "            slice_ix_anchors['ix_by_class'][t] = data_indices[ix][tix] \n",
    "            \n",
    "        # Negatives: prediction 0 -> class 0 (즉, Anchor와 Different Class) -> indices\n",
    "        ## 이 중에 가까운 샘플만 골라서 사용(Water birds에서는 거의 대부분 사용한다고 봐도 무방하다) \n",
    "        nix = np.setdiff1d(np.arange(len(data_indices)), ix) # prediction 0, class 0 (True)\n",
    "        # target_class, target_counts = np.unique(train_targets[data_indices][ix], # Anchor와 같은 Class\n",
    "        #                                         return_counts=True) # (1, 254) (Class 1, anchor 개수)\n",
    "        print(f'Slice {slice_ix} # negative (correct): {len(nix)}')\n",
    "        print(\n",
    "            f'Slice {slice_ix} % negative (correct): {len(nix) / len(data_indices) * 100 :<.4f} %')\n",
    "        \n",
    "        print(\n",
    "            f'Unique negative targets: {np.unique(train_targets[data_indices][nix], return_counts=True)}')\n",
    "\n",
    "        slice_ix_negatives = {'ix': list(data_indices[nix]),\n",
    "                                'target': list(train_targets[data_indices][nix]), # All 0\n",
    "                                'incorrect': list(sliced_data_incorrect[slice_ix][nix]),# All False\n",
    "                                'source': list(np.ones(len(data_indices[nix])).astype(int) * slice_ix), # All 0\n",
    "                                'spurious': list(train_spurious[data_indices][nix])} # 0이 많을 것(Major group) (True)\n",
    "\n",
    "        # Positives: \"Different prediction\" and \"Same Class(True)\"\n",
    "        ## 즉, 0 prediction & 0 class (다른 Slice의 Positive다)\n",
    "        # Positives, for other slices - for here just save by unique class that was also \"correct\"\n",
    "        ## 즉, \n",
    "        target_class, target_counts = np.unique(train_targets[data_indices][nix], # nix : correct\n",
    "                                                return_counts=True) # (0, 3588) (Class 0, # Correct sample )\n",
    "        correct_data_indices = data_indices[nix] # Pred 0 \n",
    "        \n",
    "        print(f\"Slice {slice_ix} # Positive: (for 'other' slice)\", len(correct_data_indices))\n",
    "        \n",
    "        # print(f'Slice {slice_ix} # positive (correct): {len(nix)}')\n",
    "        \n",
    "        for cix, c in enumerate(target_class): # only 0\n",
    "            pix = np.where(train_targets[correct_data_indices] == c)[0]            \n",
    "\n",
    "            pos_data_indices = list(correct_data_indices[pix])\n",
    "            pos_data_targets = list(\n",
    "                train_targets[correct_data_indices][pix])\n",
    "            pos_data_correct = list(\n",
    "                sliced_data_correct[slice_ix][nix][pix])\n",
    "            pos_data_source = list(\n",
    "                np.ones(len(data_indices[nix][pix])).astype(int) * slice_ix)\n",
    "            pos_data_spurious = list(\n",
    "                train_spurious[correct_data_indices][pix])\n",
    "            if c in positives_by_class:\n",
    "                positives_by_class[c]['ix'].extend(pos_data_indices)\n",
    "                positives_by_class[c]['target'].extend(pos_data_targets)\n",
    "                positives_by_class[c]['correct'].extend(pos_data_correct)\n",
    "                positives_by_class[c]['source'].extend(pos_data_source)\n",
    "                positives_by_class[c]['spurious'].extend(pos_data_spurious)\n",
    "            else:\n",
    "                positives_by_class[c] = {'ix': pos_data_indices,\n",
    "                                            'target': pos_data_targets,\n",
    "                                            'correct': pos_data_correct,\n",
    "                                            'source': pos_data_source,\n",
    "                                            'spurious': pos_data_spurious}\n",
    "            \n",
    "        # Save\n",
    "        slice_anchors[slice_ix] = slice_ix_anchors\n",
    "        slice_negatives[slice_ix] = slice_ix_negatives\n",
    "\n",
    "    # Fill in positives if no slices had the class as spurious\n",
    "    for slice_ix, data_indices in enumerate(sliced_data_indices):\n",
    "        target_class, target_counts = np.unique(train_targets[data_indices],\n",
    "                                                return_counts=True)\n",
    "\n",
    "        # Compare average correctness, still use the max_class variable\n",
    "        avg_correct = []\n",
    "        for c in target_class:\n",
    "            class_indices = np.where(train_targets[data_indices] == c)[0]\n",
    "            class_correct = sliced_data_correct[slice_ix][class_indices]\n",
    "            avg_correct.append(np.mean(class_correct))\n",
    "        max_class_ix = np.argmax(avg_correct)\n",
    "\n",
    "        for c in target_class:\n",
    "            if c not in positives_by_class:\n",
    "                print(\n",
    "                    f'> Loading correct datapoints as positives for class {c} from slice {slice_ix}')\n",
    "                ix = np.where(train_targets[data_indices] == c)[0]\n",
    "                positives_by_class[c] = {'ix': list(data_indices[ix]),\n",
    "                                         'target': list(train_targets[data_indices][ix]),\n",
    "                                         'correct': list(sliced_data_correct[slice_ix][ix]),\n",
    "                                         'source': list(np.ones(len(data_indices[ix])).astype(int) * slice_ix),\n",
    "                                         'spurious': list(train_spurious[data_indices][ix])}\n",
    "\n",
    "    # Convert casted lists back to ndarrays\n",
    "    for c, class_dict in positives_by_class.items():\n",
    "        for k, v in class_dict.items():\n",
    "            positives_by_class[c][k] = np.array(v)\n",
    "\n",
    "    for ix, slice_negative in enumerate(slice_negatives):\n",
    "        for k, v in slice_negative.items():\n",
    "            slice_negatives[ix][k] = np.array(v)\n",
    "\n",
    "\n",
    "    return slice_anchors, slice_negatives, positives_by_class, all_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_contrastive_data(train_loader, slice_anchors,\n",
    "                          slice_negatives, positives_by_class, args,\n",
    "                          persistent_workers=True): # Data processing 오류 핸들링.\n",
    "    # Get number of negatives per target class\n",
    "    args.num_negatives_by_target = [0] * args.n_cls\n",
    "\n",
    "    batch_samples = []\n",
    "    batch_samples_old = []\n",
    "\n",
    "    for slice_ix, anchor_dict in enumerate(slice_anchors):\n",
    "        batch_samples_per_slice = []  # First aggregate within\n",
    "        negative_dict = slice_negatives[slice_ix]\n",
    "        # For hard negative\n",
    "        args.num_negatives_by_target[slice_ix] = len(negative_dict['ix'])\n",
    "\n",
    "        anchor_targets = anchor_dict['target']\n",
    "        anchor_indices = anchor_dict['ix']\n",
    "\n",
    "        # 254, 94 (Prediction 0에서의 False, Prediction 1에서의 False )\n",
    "        for aix, anchor_ix in enumerate(tqdm(anchor_indices, desc=f'Generating data from slice {slice_ix}')): \n",
    "            anchor_class = anchor_targets[aix]\n",
    "            # Sample additional positives\n",
    "            anchor_indices = sample_anchors(anchor_class,\n",
    "                                            anchor_dict,\n",
    "                                            args.num_anchor - 1)\n",
    "            anchor_indices = np.concatenate([[anchor_ix], anchor_indices]) # (1, )\n",
    "            \n",
    "            positive_outputs = sample_positives(anchor_class,\n",
    "                                                positives_by_class,\n",
    "                                                args.num_positive)\n",
    "            positive_indices, positive_slice_sources = positive_outputs # (n_positives, )\n",
    "            \n",
    "            # Keep as this, in case want to generate new neg per pos as before\n",
    "            samples = [anchor_indices, positive_indices]\n",
    "            negative_indices = sample_negatives(negative_dict,\n",
    "                                                args.num_negative) # (n_negatives, )\n",
    "            samples.append(negative_indices)\n",
    "    \n",
    "            batch_sample = np.concatenate(samples) # (# positive * # negative)\n",
    "            batch_samples_per_slice.append(batch_sample)\n",
    "            batch_samples_old.append(batch_sample)\n",
    "            \n",
    "        np.random.shuffle(batch_samples_per_slice) # (# anchor, # positive * # negative): (254, 1719) / (94. 1719)\n",
    "        batch_samples.append(batch_samples_per_slice)\n",
    "        print(\"batch_samples_per_slice.shape\", np.array(batch_samples_per_slice).shape) \n",
    "\n",
    "    # print(\"batch_samples.shape\", np.array(batch_samples[0]).shape) # (254, 1719)\n",
    "    # print(\"batch_samples.shape\", np.array(batch_samples[1]).shape) # (94, 1719)\n",
    "    batch_samples = list(zip(*batch_samples)) # (94, 2, 1719) (우선 Prediction 비율에 따라 Balanced로 구성.)\n",
    "    print(np.array(batch_samples).shape)\n",
    "    \n",
    "    batch_samples = np.array(batch_samples).reshape(-1, len(batch_sample)) # (188, 1719)\n",
    "    \n",
    "    contrastive_indices = np.concatenate(batch_samples)\n",
    "    contrastive_train_set = get_resampled_set(train_loader.dataset,\n",
    "                                              contrastive_indices,\n",
    "                                              copy_dataset=True)\n",
    "    \n",
    "    \n",
    "    contrastive_dataloader = DataLoader(contrastive_train_set,\n",
    "                                        batch_size=len(\n",
    "                                            batch_samples[0]) * int(args.batch_factor),\n",
    "                                        shuffle=False, num_workers=args.num_workers, persistent_workers = persistent_workers)\n",
    "    print(\"> batchsize of contrastive data loader :\", len(batch_samples[0]) * int(args.batch_factor)) # (32) * (1719)[1 + pos + neg]\n",
    "    print(\"> len of contrastive dataset :\", len(contrastive_train_set)) # (188*1719, )\n",
    "    return contrastive_dataloader\n",
    "\n",
    "def get_resampled_set(dataset, resampled_set_indices, copy_dataset=False):\n",
    "    \"\"\"\n",
    "    Obtain spurious dataset resampled_set\n",
    "    Args:\n",
    "    - dataset (torch.utils.data.Dataset): Spurious correlations dataset\n",
    "    - resampled_set_indices (int[]): List-like of indices \n",
    "    - deepcopy (bool): If true, copy the dataset\n",
    "    \"\"\"\n",
    "    resampled_set = copy.deepcopy(dataset) if copy_dataset else dataset\n",
    "    try:  # Some dataset classes may not have these attributes (WaterbirdsEmbeddingsDatasets 특화)\n",
    "        resampled_set.y_array = resampled_set.y_array[resampled_set_indices]\n",
    "        resampled_set.group_array = resampled_set.group_array[resampled_set_indices]\n",
    "        resampled_set.split_array = resampled_set.split_array[resampled_set_indices]\n",
    "        resampled_set.targets = resampled_set.targets[resampled_set_indices]\n",
    "        resampled_set.targets_group = resampled_set.targets_group[resampled_set_indices]\n",
    "        resampled_set.targets_spurious = resampled_set.targets_spurious[resampled_set_indices]\n",
    "        try:  # Depending on the dataset these are responsible for the X features\n",
    "            resampled_set.filename_array = resampled_set.filename_array[resampled_set_indices]\n",
    "        except:\n",
    "            resampled_set.x_array = resampled_set.x_array[resampled_set_indices]\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Main Embedding Information을 담고있는 Dataframe 조절 (6, 11760) -> (6, 323171...)\n",
    "    resampled_embeddings_df = resampled_set.embeddings_df.copy()\n",
    "    resampled_embeddings_df = resampled_embeddings_df.T\n",
    "    resampled_embeddings_df = resampled_embeddings_df.iloc[resampled_set_indices]\n",
    "    resampled_set.embeddings_df = resampled_embeddings_df.T.copy()\n",
    "    \n",
    "    # Indexing 방식 수정 (dataframe+key -> dataframe+iloc)\n",
    "    resampled_set.on_contrastive_batch = True\n",
    "\n",
    "    print('len(resampled_set.targets)', len(resampled_set.targets))\n",
    "    return resampled_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SupervisedContrastiveLoss, self).__init__()\n",
    "        self.cl_temperature = args.cl_temperature\n",
    "        self.n_positives = args.num_positive\n",
    "        self.n_negatives = args.num_negative\n",
    "        self.args = args\n",
    "    \n",
    "        self.sim = nn.CosineSimilarity(dim=1)\n",
    "        \n",
    "    def forward(self, model, contrastive_batch):\n",
    "        \n",
    "        # contrastive_batch [anc; pos; neg] : (1+N+M(=N), )\n",
    "        # Compute negative similarities\n",
    "        neg_indices = [0] + list(range(len(contrastive_batch))[\n",
    "            -self.n_negatives:])\n",
    "        anchor_negatives = contrastive_batch[neg_indices]\n",
    "        \n",
    "        # Compute positive similarities\n",
    "        anchor_positives = contrastive_batch[:1 + self.n_positives]\n",
    "        pos = self.compute_sim(model, anchor_positives, \n",
    "                                       return_sum=False)\n",
    "        max_pos, _ = torch.max(pos, dim=0, keepdim=True)\n",
    "        pos = pos - max_pos.detach() # 그래디언트 흐를듯\n",
    "        exp_pos = torch.exp(pos)\n",
    "        anchors_positives = anchor_positives.to(torch.device('cpu'))\n",
    "        \n",
    "        # M(=N)개의 exp(sim) score\n",
    "        neg = self.compute_sim(model, anchor_negatives,\n",
    "                                       return_sum=False)\n",
    "        \n",
    "        neg = neg - max_pos.detach() # 같은 Scaling 먹여야.\n",
    "        exp_neg = torch.exp(neg)\n",
    "        \n",
    "        anchors_negatives = anchor_negatives.to(torch.device('cpu'))\n",
    "        \n",
    "        sum_exp_neg = exp_neg.sum(0, keepdim=True)\n",
    "        pos_numerator = exp_pos.mean()\n",
    "        neg_numerator = exp_neg.mean()\n",
    "        denominator = sum_exp_neg + exp_pos.sum(0, keepdim=True)\n",
    "        log_probs = (torch.log(exp_pos) - \n",
    "                        torch.log(sum_exp_neg + exp_pos.sum(0, keepdim=True)))\n",
    "        loss = -1 * log_probs\n",
    "        del exp_pos; del exp_neg; del log_probs\n",
    "        \n",
    "        # return loss.mean() # N개의 Positives에 대한 평균. \n",
    "        return loss.mean(), pos_numerator, neg_numerator, denominator # NOTE 학습 추이 관찰용. \n",
    "    \n",
    "    def compute_sim(self, model, features, return_sum=True):\n",
    "        \"\"\"\n",
    "        Compute sum(sim(anchor, pos)) or sum(sim(anchor, neg))\n",
    "        First index : anchor\n",
    "        \"\"\"\n",
    "        # in Contrastive Adapter, features:CLIP, outputs:Adapted-CLIP \n",
    "        \n",
    "        if self.args.tl_method ==\"contrastive_adapter\":\n",
    "            outputs = model.forward_ca(features) # unnormalized: model.adapter; normalized_head: model.forward_ca; (normalized) logits까지: modal.forward\n",
    "        \n",
    "        sim = self.sim(outputs[0].view(1, -1), outputs[1:]) # (N,) or (M,)\n",
    "        sim_divided_by_temp = torch.div(sim, self.cl_temperature)\n",
    "        \n",
    "        outputs = outputs.to(torch.device('cpu'))\n",
    "        return sim_divided_by_temp\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Start Transfer Learning using [contrastive_adapter]\n",
      "========================================================================\n",
      "Load image embedding of Waterbirds: /home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings_unnormalized/waterbirds/RN50/clip.json\n",
      "/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings_unnormalized/waterbirds/RN50/clip.json\n",
      "ㄴ Corresponding text embedding of Waterbirds: /home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings_unnormalized/waterbirds/clip_class.json\n",
      "Load Data Loader (train, validation, test)\n",
      "/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings_unnormalized/waterbirds/RN50/clip.json\n",
      "/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings_unnormalized/waterbirds/RN50/clip.json\n",
      "/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings_unnormalized/waterbirds/RN50/clip.json\n",
      "Training target : class (Land bird(0) / Water bird(1))\n",
      "Off-the-shelf classifier : Contrastive Adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155349/164181140.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sliced_data_incorrect = np.array(sliced_data_incorrect)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> set and load Contrastive data-handler\n",
      "========================================================================\n",
      ">> Slice 0, target: 0, counts: 3588\n",
      ">> Slice 0, target: 1, counts: 254\n",
      "Slice 0 % incorrect: 6.6111 %\n",
      "Slice 0 # negative (correct): 3588\n",
      "Slice 0 % negative (correct): 93.3889 %\n",
      "Unique negative targets: (array([0]), array([3588]))\n",
      "Slice 0 # Positive: (for 'other' slice) 3588\n",
      ">> Slice 1, target: 0, counts: 94\n",
      ">> Slice 1, target: 1, counts: 859\n",
      "Slice 1 % incorrect: 9.8636 %\n",
      "Slice 1 # negative (correct): 859\n",
      "Slice 1 % negative (correct): 90.1364 %\n",
      "Unique negative targets: (array([1]), array([859]))\n",
      "Slice 1 # Positive: (for 'other' slice) 859\n",
      "given number of positives: 1\n",
      "given number of positives: 2048\n",
      "given number of negatives: 2048\n",
      "Adjusted number of anchors:   1\n",
      "Adjusted number of positives: 859\n",
      "Adjusted number of negatives: 859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data from slice 0: 100%|██████████| 254/254 [00:00<00:00, 10092.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_samples_per_slice.shape (254, 1719)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data from slice 1: 100%|██████████| 94/94 [00:00<00:00, 9892.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_samples_per_slice.shape (94, 1719)\n",
      "(94, 2, 1719)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(resampled_set.targets) 323172\n",
      "> batchsize of contrastive data loader : 55008\n",
      "> len of contrastive dataset : 323172\n",
      "========================================================================\n",
      "Set Optimizer: SGD (default)\n",
      "========================================================================\n",
      "--- Epoch 1 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.018 (0.111)\tDT 0.112 (2.848)\tloss 7.538 (7.404)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.5180138945579529\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.41150522232055664\n",
      "(Last) Denominator in Train(Contrastive Learning): 798.4569091796875\n",
      "Loss in Train(Contrastive Learning): 7.395518794972846\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.038)\tDT 0.001 (0.035)\tloss 1.597 (1.608)\tAcc@1 0.219 (0.186)\n",
      "Train(class): {'worst_acc': 0.0377, 'acc_0_0': 0.0377, 'acc_0_1': 0.2174, 'acc_1_0': 0.8571, 'acc_1_1': 0.6216, 'mean_acc': 0.1829}\n",
      "Val(class): {'weighted_mean_acc': 0.1819, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.2382, 'acc_1_0': 0.8797, 'acc_1_1': 0.6165, 'mean_acc': 0.2727}\n",
      "Test(class): [0/1]\tTime 0.002 (0.032)\tLoss 0.5328 (1.1632)\tAcc@1 0.758 (0.363)\n",
      "Test(class): [0/1]\tTime 0.004 (0.019)\tLoss 1.6807 (1.3111)\tAcc@1 0.039 (0.294)\n",
      "Test(class): {'weighted_mean_acc': 0.1776, 'worst_acc': 0.0341, 'acc_0_0': 0.0341, 'acc_0_1': 0.2461, 'acc_1_0': 0.891, 'acc_1_1': 0.6028, 'mean_acc': 0.2746}\n",
      "--- Epoch 2 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.092)\tDT 0.093 (2.537)\tloss 7.014 (7.048)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.45031920075416565\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.22640077769756317\n",
      "(Last) Denominator in Train(Contrastive Learning): 581.302490234375\n",
      "Loss in Train(Contrastive Learning): 7.060564594065889\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.039)\tDT 0.001 (0.037)\tloss 1.511 (1.591)\tAcc@1 0.234 (0.176)\n",
      "Train(class): {'worst_acc': 0.0269, 'acc_0_0': 0.0269, 'acc_0_1': 0.212, 'acc_1_0': 0.9107, 'acc_1_1': 0.5913, 'mean_acc': 0.1687}\n",
      "Val(class): {'weighted_mean_acc': 0.1802, 'worst_acc': 0.0428, 'acc_0_0': 0.0428, 'acc_0_1': 0.2361, 'acc_1_0': 0.9098, 'acc_1_1': 0.5865, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.005 (0.041)\tLoss 0.5208 (1.1515)\tAcc@1 0.758 (0.357)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.6924 (1.2951)\tAcc@1 0.055 (0.286)\n",
      "Test(class): {'weighted_mean_acc': 0.1628, 'worst_acc': 0.0257, 'acc_0_0': 0.0257, 'acc_0_1': 0.2399, 'acc_1_0': 0.905, 'acc_1_1': 0.5639, 'mean_acc': 0.2661}\n",
      "--- Epoch 3 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.012 (0.094)\tDT 0.087 (2.586)\tloss 6.821 (6.905)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.5788636207580566\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.05310353636741638\n",
      "(Last) Denominator in Train(Contrastive Learning): 542.8597412109375\n",
      "Loss in Train(Contrastive Learning): 6.905312748665505\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.036)\tDT 0.001 (0.033)\tloss 1.570 (1.566)\tAcc@1 0.141 (0.180)\n",
      "Train(class): {'worst_acc': 0.0309, 'acc_0_0': 0.0309, 'acc_0_1': 0.1793, 'acc_1_0': 0.8929, 'acc_1_1': 0.685, 'mean_acc': 0.1908}\n",
      "Val(class): {'weighted_mean_acc': 0.1957, 'worst_acc': 0.0428, 'acc_0_0': 0.0428, 'acc_0_1': 0.2103, 'acc_1_0': 0.9023, 'acc_1_1': 0.6617, 'mean_acc': 0.2719}\n",
      "Test(class): [0/1]\tTime 0.002 (0.032)\tLoss 0.4831 (1.1462)\tAcc@1 0.781 (0.362)\n",
      "Test(class): [0/1]\tTime 0.001 (0.019)\tLoss 1.6930 (1.2924)\tAcc@1 0.055 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1894, 'worst_acc': 0.0319, 'acc_0_0': 0.0319, 'acc_0_1': 0.2067, 'acc_1_0': 0.8988, 'acc_1_1': 0.6698, 'mean_acc': 0.2667}\n",
      "--- Epoch 4 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.085)\tDT 0.100 (2.311)\tloss 6.798 (6.830)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.6069788932800293\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00241456413641572\n",
      "(Last) Denominator in Train(Contrastive Learning): 523.4689331054688\n",
      "Loss in Train(Contrastive Learning): 6.828620885280853\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.033)\tDT 0.005 (0.030)\tloss 1.584 (1.573)\tAcc@1 0.180 (0.189)\n",
      "Train(class): {'worst_acc': 0.0263, 'acc_0_0': 0.0263, 'acc_0_1': 0.1739, 'acc_1_0': 0.8929, 'acc_1_1': 0.667, 'mean_acc': 0.1833}\n",
      "Val(class): {'weighted_mean_acc': 0.185, 'worst_acc': 0.0428, 'acc_0_0': 0.0428, 'acc_0_1': 0.2296, 'acc_1_0': 0.9173, 'acc_1_1': 0.609, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.002 (0.033)\tLoss 0.4753 (1.1368)\tAcc@1 0.797 (0.370)\n",
      "Test(class): [0/1]\tTime 0.011 (0.019)\tLoss 1.6767 (1.2763)\tAcc@1 0.062 (0.292)\n",
      "Test(class): {'weighted_mean_acc': 0.1807, 'worst_acc': 0.0297, 'acc_0_0': 0.0297, 'acc_0_1': 0.2262, 'acc_1_0': 0.905, 'acc_1_1': 0.634, 'mean_acc': 0.2701}\n",
      "--- Epoch 5 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.090)\tDT 0.091 (2.470)\tloss 6.779 (6.792)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.6538030505180359\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0019864020869135857\n",
      "(Last) Denominator in Train(Contrastive Learning): 563.3231201171875\n",
      "Loss in Train(Contrastive Learning): 6.792444490371866\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.033)\tDT 0.004 (0.030)\tloss 1.542 (1.575)\tAcc@1 0.219 (0.200)\n",
      "Train(class): {'worst_acc': 0.028, 'acc_0_0': 0.028, 'acc_0_1': 0.1902, 'acc_1_0': 0.8929, 'acc_1_1': 0.6868, 'mean_acc': 0.1896}\n",
      "Val(class): {'weighted_mean_acc': 0.1949, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2167, 'acc_1_0': 0.9398, 'acc_1_1': 0.6692, 'mean_acc': 0.2777}\n",
      "Test(class): [0/1]\tTime 0.002 (0.034)\tLoss 0.4415 (1.1558)\tAcc@1 0.828 (0.379)\n",
      "Test(class): [0/1]\tTime 0.003 (0.020)\tLoss 1.7062 (1.2941)\tAcc@1 0.062 (0.298)\n",
      "Test(class): {'weighted_mean_acc': 0.1865, 'worst_acc': 0.0279, 'acc_0_0': 0.0279, 'acc_0_1': 0.2271, 'acc_1_0': 0.9252, 'acc_1_1': 0.6651, 'mean_acc': 0.2755}\n",
      "--- Epoch 6 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.085)\tDT 0.092 (2.306)\tloss 6.770 (6.776)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.6795662045478821\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.002612561918795109\n",
      "(Last) Denominator in Train(Contrastive Learning): 585.9915771484375\n",
      "Loss in Train(Contrastive Learning): 6.776624674492694\n",
      "Train(class): [0][1/2]\tBT 0.008 (0.034)\tDT 0.005 (0.031)\tloss 1.561 (1.574)\tAcc@1 0.203 (0.189)\n",
      "Train(class): {'worst_acc': 0.0272, 'acc_0_0': 0.0272, 'acc_0_1': 0.1848, 'acc_1_0': 0.9107, 'acc_1_1': 0.6963, 'mean_acc': 0.191}\n",
      "Val(class): {'weighted_mean_acc': 0.1954, 'worst_acc': 0.0428, 'acc_0_0': 0.0428, 'acc_0_1': 0.2339, 'acc_1_0': 0.9398, 'acc_1_1': 0.6541, 'mean_acc': 0.2844}\n",
      "Test(class): [0/1]\tTime 0.002 (0.034)\tLoss 0.4441 (1.1573)\tAcc@1 0.820 (0.379)\n",
      "Test(class): [0/1]\tTime 0.003 (0.019)\tLoss 1.7209 (1.2926)\tAcc@1 0.070 (0.299)\n",
      "Test(class): {'weighted_mean_acc': 0.1853, 'worst_acc': 0.0284, 'acc_0_0': 0.0284, 'acc_0_1': 0.231, 'acc_1_0': 0.9268, 'acc_1_1': 0.6573, 'mean_acc': 0.2765}\n",
      "--- Epoch 7 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.086)\tDT 0.090 (2.321)\tloss 6.766 (6.769)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.7115958333015442\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0020882573444396257\n",
      "(Last) Denominator in Train(Contrastive Learning): 613.0546264648438\n",
      "Loss in Train(Contrastive Learning): 6.769555586449643\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.033)\tDT 0.002 (0.030)\tloss 1.584 (1.569)\tAcc@1 0.172 (0.196)\n",
      "Train(class): {'worst_acc': 0.0269, 'acc_0_0': 0.0269, 'acc_0_1': 0.1848, 'acc_1_0': 0.9286, 'acc_1_1': 0.6868, 'mean_acc': 0.1889}\n",
      "Val(class): {'weighted_mean_acc': 0.197, 'worst_acc': 0.0428, 'acc_0_0': 0.0428, 'acc_0_1': 0.2339, 'acc_1_0': 0.9323, 'acc_1_1': 0.6617, 'mean_acc': 0.2844}\n",
      "Test(class): [0/1]\tTime 0.012 (0.033)\tLoss 0.4329 (1.1597)\tAcc@1 0.828 (0.380)\n",
      "Test(class): [0/1]\tTime 0.005 (0.019)\tLoss 1.7221 (1.2932)\tAcc@1 0.062 (0.301)\n",
      "Test(class): {'weighted_mean_acc': 0.1873, 'worst_acc': 0.0315, 'acc_0_0': 0.0315, 'acc_0_1': 0.2324, 'acc_1_0': 0.933, 'acc_1_1': 0.6558, 'mean_acc': 0.2787}\n",
      "--- Epoch 8 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.086)\tDT 0.107 (2.335)\tloss 6.763 (6.765)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.7334786057472229\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.001642565825022757\n",
      "(Last) Denominator in Train(Contrastive Learning): 631.4690551757812\n",
      "Loss in Train(Contrastive Learning): 6.765464295732214\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.034)\tDT 0.004 (0.032)\tloss 1.629 (1.580)\tAcc@1 0.195 (0.196)\n",
      "Train(class): {'worst_acc': 0.0286, 'acc_0_0': 0.0286, 'acc_0_1': 0.2065, 'acc_1_0': 0.9464, 'acc_1_1': 0.6973, 'mean_acc': 0.1935}\n",
      "Val(class): {'weighted_mean_acc': 0.2005, 'worst_acc': 0.0428, 'acc_0_0': 0.0428, 'acc_0_1': 0.2339, 'acc_1_0': 0.9474, 'acc_1_1': 0.6767, 'mean_acc': 0.2877}\n",
      "Test(class): [0/1]\tTime 0.002 (0.034)\tLoss 0.4242 (1.1600)\tAcc@1 0.844 (0.383)\n",
      "Test(class): [0/1]\tTime 0.003 (0.019)\tLoss 1.7296 (1.2916)\tAcc@1 0.062 (0.303)\n",
      "Test(class): {'weighted_mean_acc': 0.193, 'worst_acc': 0.0324, 'acc_0_0': 0.0324, 'acc_0_1': 0.2271, 'acc_1_0': 0.9408, 'acc_1_1': 0.6791, 'mean_acc': 0.2805}\n",
      "--- Epoch 9 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.087)\tDT 0.091 (2.361)\tloss 6.762 (6.763)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.7493919134140015\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0014324237126857042\n",
      "(Last) Denominator in Train(Contrastive Learning): 644.9581298828125\n",
      "Loss in Train(Contrastive Learning): 6.763044479045462\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.032)\tDT 0.002 (0.029)\tloss 1.617 (1.593)\tAcc@1 0.164 (0.184)\n",
      "Train(class): {'worst_acc': 0.0283, 'acc_0_0': 0.0283, 'acc_0_1': 0.1848, 'acc_1_0': 0.9286, 'acc_1_1': 0.6878, 'mean_acc': 0.1902}\n",
      "Val(class): {'weighted_mean_acc': 0.1974, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2403, 'acc_1_0': 0.9398, 'acc_1_1': 0.6692, 'mean_acc': 0.2877}\n",
      "Test(class): [0/1]\tTime 0.002 (0.032)\tLoss 0.4296 (1.1629)\tAcc@1 0.836 (0.378)\n",
      "Test(class): [0/1]\tTime 0.001 (0.018)\tLoss 1.7406 (1.2947)\tAcc@1 0.062 (0.299)\n",
      "Test(class): {'weighted_mean_acc': 0.1889, 'worst_acc': 0.0328, 'acc_0_0': 0.0328, 'acc_0_1': 0.2302, 'acc_1_0': 0.9315, 'acc_1_1': 0.6589, 'mean_acc': 0.2786}\n",
      "--- Epoch 10 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.015 (0.087)\tDT 0.088 (2.360)\tloss 6.761 (6.761)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.7660455703735352\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0011109757469967008\n",
      "(Last) Denominator in Train(Contrastive Learning): 658.9874877929688\n",
      "Loss in Train(Contrastive Learning): 6.761468359764586\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.039)\tDT 0.004 (0.036)\tloss 1.521 (1.583)\tAcc@1 0.227 (0.192)\n",
      "Train(class): {'worst_acc': 0.0294, 'acc_0_0': 0.0294, 'acc_0_1': 0.1902, 'acc_1_0': 0.9286, 'acc_1_1': 0.7001, 'mean_acc': 0.194}\n",
      "Val(class): {'weighted_mean_acc': 0.2035, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2275, 'acc_1_0': 0.9398, 'acc_1_1': 0.6992, 'mean_acc': 0.2861}\n",
      "Test(class): [0/1]\tTime 0.002 (0.034)\tLoss 0.4211 (1.1653)\tAcc@1 0.836 (0.378)\n",
      "Test(class): [0/1]\tTime 0.004 (0.019)\tLoss 1.7597 (1.2994)\tAcc@1 0.062 (0.298)\n",
      "Test(class): {'weighted_mean_acc': 0.1916, 'worst_acc': 0.0341, 'acc_0_0': 0.0341, 'acc_0_1': 0.2217, 'acc_1_0': 0.9346, 'acc_1_1': 0.6682, 'mean_acc': 0.2772}\n",
      "--- Epoch 11 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.088)\tDT 0.092 (2.354)\tloss 6.760 (6.760)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.7814815044403076\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0009160286863334477\n",
      "(Last) Denominator in Train(Contrastive Learning): 672.0794677734375\n",
      "Loss in Train(Contrastive Learning): 6.760403308462589\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.039)\tDT 0.002 (0.036)\tloss 1.531 (1.585)\tAcc@1 0.188 (0.191)\n",
      "Train(class): {'worst_acc': 0.0294, 'acc_0_0': 0.0294, 'acc_0_1': 0.212, 'acc_1_0': 0.9464, 'acc_1_1': 0.7039, 'mean_acc': 0.1958}\n",
      "Val(class): {'weighted_mean_acc': 0.2049, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2167, 'acc_1_0': 0.9474, 'acc_1_1': 0.7068, 'mean_acc': 0.2836}\n",
      "Test(class): [0/1]\tTime 0.012 (0.041)\tLoss 0.4096 (1.1741)\tAcc@1 0.844 (0.379)\n",
      "Test(class): [0/1]\tTime 0.009 (0.023)\tLoss 1.7734 (1.3096)\tAcc@1 0.055 (0.297)\n",
      "Test(class): {'weighted_mean_acc': 0.1932, 'worst_acc': 0.0324, 'acc_0_0': 0.0324, 'acc_0_1': 0.2124, 'acc_1_0': 0.9439, 'acc_1_1': 0.6822, 'mean_acc': 0.2755}\n",
      "--- Epoch 12 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.015 (0.088)\tDT 0.091 (2.387)\tloss 6.759 (6.760)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.7921384572982788\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0007847339729778469\n",
      "(Last) Denominator in Train(Contrastive Learning): 681.1209716796875\n",
      "Loss in Train(Contrastive Learning): 6.759662455700814\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.044)\tDT 0.002 (0.040)\tloss 1.682 (1.584)\tAcc@1 0.141 (0.202)\n",
      "Train(class): {'worst_acc': 0.0306, 'acc_0_0': 0.0306, 'acc_0_1': 0.1848, 'acc_1_0': 0.9286, 'acc_1_1': 0.7124, 'mean_acc': 0.1973}\n",
      "Val(class): {'weighted_mean_acc': 0.197, 'worst_acc': 0.0428, 'acc_0_0': 0.0428, 'acc_0_1': 0.2339, 'acc_1_0': 0.9398, 'acc_1_1': 0.6617, 'mean_acc': 0.2852}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4260 (1.1589)\tAcc@1 0.828 (0.380)\n",
      "Test(class): [0/1]\tTime 0.014 (0.023)\tLoss 1.7539 (1.2894)\tAcc@1 0.062 (0.303)\n",
      "Test(class): {'weighted_mean_acc': 0.1917, 'worst_acc': 0.035, 'acc_0_0': 0.035, 'acc_0_1': 0.2341, 'acc_1_0': 0.9361, 'acc_1_1': 0.6636, 'mean_acc': 0.282}\n",
      "--- Epoch 13 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.017 (0.086)\tDT 0.097 (2.197)\tloss 6.759 (6.759)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8012871742248535\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.000684737169649452\n",
      "(Last) Denominator in Train(Contrastive Learning): 688.8938598632812\n",
      "Loss in Train(Contrastive Learning): 6.759131144970022\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.042)\tDT 0.001 (0.038)\tloss 1.561 (1.604)\tAcc@1 0.195 (0.190)\n",
      "Train(class): {'worst_acc': 0.0306, 'acc_0_0': 0.0306, 'acc_0_1': 0.163, 'acc_1_0': 0.9464, 'acc_1_1': 0.7029, 'mean_acc': 0.1946}\n",
      "Val(class): {'weighted_mean_acc': 0.2031, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2167, 'acc_1_0': 0.9398, 'acc_1_1': 0.6992, 'mean_acc': 0.2819}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4125 (1.1821)\tAcc@1 0.844 (0.376)\n",
      "Test(class): [0/1]\tTime 0.005 (0.023)\tLoss 1.7911 (1.3205)\tAcc@1 0.062 (0.294)\n",
      "Test(class): {'weighted_mean_acc': 0.1907, 'worst_acc': 0.0315, 'acc_0_0': 0.0315, 'acc_0_1': 0.2102, 'acc_1_0': 0.9439, 'acc_1_1': 0.6745, 'mean_acc': 0.2734}\n",
      "--- Epoch 14 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.015 (0.083)\tDT 0.106 (2.178)\tloss 6.759 (6.759)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8109885454177856\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.000593059987295419\n",
      "(Last) Denominator in Train(Contrastive Learning): 697.1486206054688\n",
      "Loss in Train(Contrastive Learning): 6.758721080232174\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.565 (1.581)\tAcc@1 0.242 (0.195)\n",
      "Train(class): {'worst_acc': 0.0286, 'acc_0_0': 0.0286, 'acc_0_1': 0.1848, 'acc_1_0': 0.9286, 'acc_1_1': 0.6963, 'mean_acc': 0.1923}\n",
      "Val(class): {'weighted_mean_acc': 0.2013, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2124, 'acc_1_0': 0.9398, 'acc_1_1': 0.6917, 'mean_acc': 0.2794}\n",
      "Test(class): [0/1]\tTime 0.005 (0.041)\tLoss 0.4103 (1.1837)\tAcc@1 0.852 (0.375)\n",
      "Test(class): [0/1]\tTime 0.005 (0.024)\tLoss 1.7946 (1.3220)\tAcc@1 0.062 (0.295)\n",
      "Test(class): {'weighted_mean_acc': 0.1898, 'worst_acc': 0.0315, 'acc_0_0': 0.0315, 'acc_0_1': 0.2129, 'acc_1_0': 0.9439, 'acc_1_1': 0.6698, 'mean_acc': 0.2739}\n",
      "--- Epoch 15 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.015 (0.090)\tDT 0.087 (2.414)\tloss 6.758 (6.758)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8180250525474548\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0005563999875448644\n",
      "(Last) Denominator in Train(Contrastive Learning): 703.1614990234375\n",
      "Loss in Train(Contrastive Learning): 6.758399785833156\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.043)\tDT 0.004 (0.039)\tloss 1.491 (1.593)\tAcc@1 0.250 (0.198)\n",
      "Train(class): {'worst_acc': 0.0277, 'acc_0_0': 0.0277, 'acc_0_1': 0.1576, 'acc_1_0': 0.9286, 'acc_1_1': 0.7096, 'mean_acc': 0.1935}\n",
      "Val(class): {'weighted_mean_acc': 0.2015, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2189, 'acc_1_0': 0.9398, 'acc_1_1': 0.6917, 'mean_acc': 0.2819}\n",
      "Test(class): [0/1]\tTime 0.004 (0.040)\tLoss 0.4159 (1.1800)\tAcc@1 0.844 (0.377)\n",
      "Test(class): [0/1]\tTime 0.001 (0.022)\tLoss 1.7817 (1.3144)\tAcc@1 0.062 (0.297)\n",
      "Test(class): {'weighted_mean_acc': 0.1915, 'worst_acc': 0.0337, 'acc_0_0': 0.0337, 'acc_0_1': 0.2164, 'acc_1_0': 0.9408, 'acc_1_1': 0.6698, 'mean_acc': 0.2758}\n",
      "--- Epoch 16 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.083)\tDT 0.097 (2.195)\tloss 6.758 (6.758)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8256344199180603\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.000484615215100348\n",
      "(Last) Denominator in Train(Contrastive Learning): 709.63623046875\n",
      "Loss in Train(Contrastive Learning): 6.758137723232838\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.041)\tDT 0.001 (0.038)\tloss 1.651 (1.592)\tAcc@1 0.148 (0.193)\n",
      "Train(class): {'worst_acc': 0.0283, 'acc_0_0': 0.0283, 'acc_0_1': 0.1685, 'acc_1_0': 0.9286, 'acc_1_1': 0.7143, 'mean_acc': 0.1954}\n",
      "Val(class): {'weighted_mean_acc': 0.2012, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2103, 'acc_1_0': 0.9398, 'acc_1_1': 0.6917, 'mean_acc': 0.2786}\n",
      "Test(class): [0/1]\tTime 0.002 (0.041)\tLoss 0.4104 (1.1780)\tAcc@1 0.844 (0.379)\n",
      "Test(class): [0/1]\tTime 0.006 (0.023)\tLoss 1.7778 (1.3143)\tAcc@1 0.062 (0.295)\n",
      "Test(class): {'weighted_mean_acc': 0.1917, 'worst_acc': 0.0324, 'acc_0_0': 0.0324, 'acc_0_1': 0.2111, 'acc_1_0': 0.9424, 'acc_1_1': 0.676, 'mean_acc': 0.2741}\n",
      "--- Epoch 17 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.018 (0.093)\tDT 0.089 (2.415)\tloss 6.758 (6.758)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8316356539726257\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0004371780378278345\n",
      "(Last) Denominator in Train(Contrastive Learning): 714.7505493164062\n",
      "Loss in Train(Contrastive Learning): 6.757924960014668\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.042)\tDT 0.001 (0.039)\tloss 1.652 (1.593)\tAcc@1 0.148 (0.201)\n",
      "Train(class): {'worst_acc': 0.0294, 'acc_0_0': 0.0294, 'acc_0_1': 0.1467, 'acc_1_0': 0.9643, 'acc_1_1': 0.7171, 'mean_acc': 0.1965}\n",
      "Val(class): {'weighted_mean_acc': 0.1996, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2124, 'acc_1_0': 0.9398, 'acc_1_1': 0.6842, 'mean_acc': 0.2786}\n",
      "Test(class): [0/1]\tTime 0.003 (0.041)\tLoss 0.4172 (1.1734)\tAcc@1 0.828 (0.377)\n",
      "Test(class): [0/1]\tTime 0.003 (0.023)\tLoss 1.7728 (1.3070)\tAcc@1 0.062 (0.296)\n",
      "Test(class): {'weighted_mean_acc': 0.1914, 'worst_acc': 0.035, 'acc_0_0': 0.035, 'acc_0_1': 0.216, 'acc_1_0': 0.9393, 'acc_1_1': 0.6651, 'mean_acc': 0.2755}\n",
      "--- Epoch 18 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.018 (0.086)\tDT 0.089 (2.208)\tloss 6.758 (6.758)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8374748229980469\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00039928845944814384\n",
      "(Last) Denominator in Train(Contrastive Learning): 719.73388671875\n",
      "Loss in Train(Contrastive Learning): 6.7577434423122\n",
      "Train(class): [0][1/2]\tBT 0.009 (0.043)\tDT 0.006 (0.040)\tloss 1.611 (1.599)\tAcc@1 0.203 (0.195)\n",
      "Train(class): {'worst_acc': 0.0294, 'acc_0_0': 0.0294, 'acc_0_1': 0.1902, 'acc_1_0': 0.9286, 'acc_1_1': 0.7124, 'mean_acc': 0.1967}\n",
      "Val(class): {'weighted_mean_acc': 0.1998, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2146, 'acc_1_0': 0.9398, 'acc_1_1': 0.6917, 'mean_acc': 0.2794}\n",
      "Test(class): [0/1]\tTime 0.011 (0.041)\tLoss 0.4156 (1.1768)\tAcc@1 0.828 (0.377)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.7802 (1.3122)\tAcc@1 0.062 (0.296)\n",
      "Test(class): {'weighted_mean_acc': 0.1908, 'worst_acc': 0.0341, 'acc_0_0': 0.0341, 'acc_0_1': 0.2173, 'acc_1_0': 0.9377, 'acc_1_1': 0.6651, 'mean_acc': 0.2755}\n",
      "--- Epoch 19 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.084)\tDT 0.091 (2.204)\tloss 6.758 (6.758)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.842788815498352\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0003826435131486505\n",
      "(Last) Denominator in Train(Contrastive Learning): 724.2842407226562\n",
      "Loss in Train(Contrastive Learning): 6.757591671132027\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.530 (1.598)\tAcc@1 0.242 (0.193)\n",
      "Train(class): {'worst_acc': 0.0274, 'acc_0_0': 0.0274, 'acc_0_1': 0.212, 'acc_1_0': 0.9286, 'acc_1_1': 0.7077, 'mean_acc': 0.195}\n",
      "Val(class): {'weighted_mean_acc': 0.2012, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2103, 'acc_1_0': 0.9398, 'acc_1_1': 0.6917, 'mean_acc': 0.2786}\n",
      "Test(class): [0/1]\tTime 0.004 (0.040)\tLoss 0.4095 (1.1794)\tAcc@1 0.852 (0.374)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.7872 (1.3145)\tAcc@1 0.062 (0.294)\n",
      "Test(class): {'weighted_mean_acc': 0.1914, 'worst_acc': 0.0315, 'acc_0_0': 0.0315, 'acc_0_1': 0.2102, 'acc_1_0': 0.9393, 'acc_1_1': 0.6776, 'mean_acc': 0.2732}\n",
      "--- Epoch 20 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.015 (0.084)\tDT 0.090 (2.218)\tloss 6.758 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8473025560379028\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0003614219604060054\n",
      "(Last) Denominator in Train(Contrastive Learning): 728.1433715820312\n",
      "Loss in Train(Contrastive Learning): 6.757460566277199\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.042)\tDT 0.001 (0.039)\tloss 1.622 (1.610)\tAcc@1 0.156 (0.189)\n",
      "Train(class): {'worst_acc': 0.0274, 'acc_0_0': 0.0274, 'acc_0_1': 0.1957, 'acc_1_0': 0.9464, 'acc_1_1': 0.7171, 'mean_acc': 0.1967}\n",
      "Val(class): {'weighted_mean_acc': 0.1996, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2103, 'acc_1_0': 0.9398, 'acc_1_1': 0.6917, 'mean_acc': 0.2777}\n",
      "Test(class): [0/1]\tTime 0.003 (0.039)\tLoss 0.4092 (1.1736)\tAcc@1 0.852 (0.374)\n",
      "Test(class): [0/1]\tTime 0.012 (0.022)\tLoss 1.7794 (1.3092)\tAcc@1 0.062 (0.294)\n",
      "Test(class): {'weighted_mean_acc': 0.1939, 'worst_acc': 0.0341, 'acc_0_0': 0.0341, 'acc_0_1': 0.2067, 'acc_1_0': 0.9393, 'acc_1_1': 0.6807, 'mean_acc': 0.2732}\n",
      "--- Epoch 21 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.017 (0.087)\tDT 0.089 (2.227)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8517312407493591\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0003243224637117237\n",
      "(Last) Denominator in Train(Contrastive Learning): 731.9157104492188\n",
      "Loss in Train(Contrastive Learning): 6.757346059413666\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.043)\tDT 0.004 (0.040)\tloss 1.618 (1.578)\tAcc@1 0.172 (0.200)\n",
      "Train(class): {'worst_acc': 0.0286, 'acc_0_0': 0.0286, 'acc_0_1': 0.1739, 'acc_1_0': 0.9286, 'acc_1_1': 0.7181, 'mean_acc': 0.1967}\n",
      "Val(class): {'weighted_mean_acc': 0.2015, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2189, 'acc_1_0': 0.9398, 'acc_1_1': 0.6917, 'mean_acc': 0.2819}\n",
      "Test(class): [0/1]\tTime 0.007 (0.041)\tLoss 0.4155 (1.1698)\tAcc@1 0.836 (0.376)\n",
      "Test(class): [0/1]\tTime 0.007 (0.023)\tLoss 1.7782 (1.3038)\tAcc@1 0.062 (0.297)\n",
      "Test(class): {'weighted_mean_acc': 0.1912, 'worst_acc': 0.0341, 'acc_0_0': 0.0341, 'acc_0_1': 0.2191, 'acc_1_0': 0.9361, 'acc_1_1': 0.6667, 'mean_acc': 0.2761}\n",
      "--- Epoch 22 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.019 (0.092)\tDT 0.090 (2.472)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8553743362426758\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0003034202673006803\n",
      "(Last) Denominator in Train(Contrastive Learning): 735.0271606445312\n",
      "Loss in Train(Contrastive Learning): 6.757246471465902\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.041)\tDT 0.003 (0.038)\tloss 1.598 (1.604)\tAcc@1 0.156 (0.186)\n",
      "Train(class): {'worst_acc': 0.0297, 'acc_0_0': 0.0297, 'acc_0_1': 0.1685, 'acc_1_0': 0.9464, 'acc_1_1': 0.7077, 'mean_acc': 0.1952}\n",
      "Val(class): {'weighted_mean_acc': 0.1996, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2103, 'acc_1_0': 0.9398, 'acc_1_1': 0.6917, 'mean_acc': 0.2777}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4116 (1.1819)\tAcc@1 0.836 (0.372)\n",
      "Test(class): [0/1]\tTime 0.006 (0.023)\tLoss 1.7889 (1.3170)\tAcc@1 0.062 (0.294)\n",
      "Test(class): {'weighted_mean_acc': 0.1913, 'worst_acc': 0.0346, 'acc_0_0': 0.0346, 'acc_0_1': 0.2124, 'acc_1_0': 0.9393, 'acc_1_1': 0.6667, 'mean_acc': 0.2741}\n",
      "--- Epoch 23 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.015 (0.085)\tDT 0.091 (2.263)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8589354753494263\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0002854471676982939\n",
      "(Last) Denominator in Train(Contrastive Learning): 738.0707397460938\n",
      "Loss in Train(Contrastive Learning): 6.757158619292239\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.585 (1.612)\tAcc@1 0.180 (0.183)\n",
      "Train(class): {'worst_acc': 0.0269, 'acc_0_0': 0.0269, 'acc_0_1': 0.2065, 'acc_1_0': 0.9464, 'acc_1_1': 0.7077, 'mean_acc': 0.1946}\n",
      "Val(class): {'weighted_mean_acc': 0.2029, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2082, 'acc_1_0': 0.9398, 'acc_1_1': 0.7068, 'mean_acc': 0.2786}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.4054 (1.1854)\tAcc@1 0.859 (0.372)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.7903 (1.3216)\tAcc@1 0.062 (0.292)\n",
      "Test(class): {'weighted_mean_acc': 0.1941, 'worst_acc': 0.0328, 'acc_0_0': 0.0328, 'acc_0_1': 0.2009, 'acc_1_0': 0.9408, 'acc_1_1': 0.6869, 'mean_acc': 0.2713}\n",
      "--- Epoch 24 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.084)\tDT 0.088 (2.253)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8618952035903931\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0002668974339030683\n",
      "(Last) Denominator in Train(Contrastive Learning): 740.5972290039062\n",
      "Loss in Train(Contrastive Learning): 6.75708026835259\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.042)\tDT 0.002 (0.039)\tloss 1.516 (1.599)\tAcc@1 0.258 (0.193)\n",
      "Train(class): {'worst_acc': 0.0283, 'acc_0_0': 0.0283, 'acc_0_1': 0.1848, 'acc_1_0': 0.9286, 'acc_1_1': 0.7114, 'mean_acc': 0.1954}\n",
      "Val(class): {'weighted_mean_acc': 0.2015, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2167, 'acc_1_0': 0.9398, 'acc_1_1': 0.6917, 'mean_acc': 0.2811}\n",
      "Test(class): [0/1]\tTime 0.003 (0.040)\tLoss 0.4154 (1.1763)\tAcc@1 0.828 (0.373)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.7794 (1.3097)\tAcc@1 0.062 (0.295)\n",
      "Test(class): {'weighted_mean_acc': 0.1888, 'worst_acc': 0.0341, 'acc_0_0': 0.0341, 'acc_0_1': 0.2186, 'acc_1_0': 0.9361, 'acc_1_1': 0.6558, 'mean_acc': 0.2748}\n",
      "--- Epoch 25 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.015 (0.080)\tDT 0.089 (2.100)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8647717833518982\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0002520100970286876\n",
      "(Last) Denominator in Train(Contrastive Learning): 743.055419921875\n",
      "Loss in Train(Contrastive Learning): 6.757009491007379\n",
      "Train(class): [0][1/2]\tBT 0.003 (0.041)\tDT 0.001 (0.038)\tloss 1.557 (1.596)\tAcc@1 0.219 (0.191)\n",
      "Train(class): {'worst_acc': 0.028, 'acc_0_0': 0.028, 'acc_0_1': 0.1957, 'acc_1_0': 0.9464, 'acc_1_1': 0.7096, 'mean_acc': 0.1954}\n",
      "Val(class): {'weighted_mean_acc': 0.1982, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2167, 'acc_1_0': 0.9398, 'acc_1_1': 0.6842, 'mean_acc': 0.2794}\n",
      "Test(class): [0/1]\tTime 0.003 (0.040)\tLoss 0.4116 (1.1834)\tAcc@1 0.836 (0.374)\n",
      "Test(class): [0/1]\tTime 0.008 (0.022)\tLoss 1.7885 (1.3182)\tAcc@1 0.062 (0.295)\n",
      "Test(class): {'weighted_mean_acc': 0.1901, 'worst_acc': 0.0333, 'acc_0_0': 0.0333, 'acc_0_1': 0.2151, 'acc_1_0': 0.9424, 'acc_1_1': 0.6651, 'mean_acc': 0.2748}\n",
      "--- Epoch 26 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.093)\tDT 0.090 (2.565)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8671736717224121\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00023995789524633437\n",
      "(Last) Denominator in Train(Contrastive Learning): 745.1082763671875\n",
      "Loss in Train(Contrastive Learning): 6.756946416611367\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.043)\tDT 0.004 (0.039)\tloss 1.628 (1.593)\tAcc@1 0.180 (0.197)\n",
      "Train(class): {'worst_acc': 0.0289, 'acc_0_0': 0.0289, 'acc_0_1': 0.1848, 'acc_1_0': 0.9286, 'acc_1_1': 0.7114, 'mean_acc': 0.1958}\n",
      "Val(class): {'weighted_mean_acc': 0.2028, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9474, 'acc_1_1': 0.7218, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3972 (1.1932)\tAcc@1 0.859 (0.373)\n",
      "Test(class): [0/1]\tTime 0.005 (0.022)\tLoss 1.8128 (1.3338)\tAcc@1 0.062 (0.292)\n",
      "Test(class): {'weighted_mean_acc': 0.1933, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1951, 'acc_1_0': 0.947, 'acc_1_1': 0.6931, 'mean_acc': 0.2694}\n",
      "--- Epoch 27 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.077)\tDT 0.084 (2.042)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8697873950004578\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00022261896810960025\n",
      "(Last) Denominator in Train(Contrastive Learning): 747.3385620117188\n",
      "Loss in Train(Contrastive Learning): 6.756889373698133\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.589 (1.596)\tAcc@1 0.211 (0.198)\n",
      "Train(class): {'worst_acc': 0.0266, 'acc_0_0': 0.0266, 'acc_0_1': 0.1848, 'acc_1_0': 0.9286, 'acc_1_1': 0.7124, 'mean_acc': 0.1944}\n",
      "Val(class): {'weighted_mean_acc': 0.1998, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.206, 'acc_1_0': 0.9474, 'acc_1_1': 0.7068, 'mean_acc': 0.2769}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4027 (1.1893)\tAcc@1 0.859 (0.373)\n",
      "Test(class): [0/1]\tTime 0.002 (0.022)\tLoss 1.8141 (1.3293)\tAcc@1 0.062 (0.291)\n",
      "Test(class): {'weighted_mean_acc': 0.1917, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1991, 'acc_1_0': 0.9424, 'acc_1_1': 0.6854, 'mean_acc': 0.2696}\n",
      "--- Epoch 28 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.083)\tDT 0.089 (2.249)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8720614314079285\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0002076547680189833\n",
      "(Last) Denominator in Train(Contrastive Learning): 749.2791748046875\n",
      "Loss in Train(Contrastive Learning): 6.756837307138646\n",
      "Train(class): [0][1/2]\tBT 0.008 (0.041)\tDT 0.005 (0.038)\tloss 1.614 (1.596)\tAcc@1 0.172 (0.198)\n",
      "Train(class): {'worst_acc': 0.0266, 'acc_0_0': 0.0266, 'acc_0_1': 0.1685, 'acc_1_0': 0.9643, 'acc_1_1': 0.7143, 'mean_acc': 0.1946}\n",
      "Val(class): {'weighted_mean_acc': 0.1998, 'worst_acc': 0.0407, 'acc_0_0': 0.0407, 'acc_0_1': 0.2167, 'acc_1_0': 0.9398, 'acc_1_1': 0.6842, 'mean_acc': 0.2802}\n",
      "Test(class): [0/1]\tTime 0.011 (0.040)\tLoss 0.4143 (1.1729)\tAcc@1 0.828 (0.373)\n",
      "Test(class): [0/1]\tTime 0.006 (0.022)\tLoss 1.7747 (1.3055)\tAcc@1 0.062 (0.296)\n",
      "Test(class): {'weighted_mean_acc': 0.1917, 'worst_acc': 0.035, 'acc_0_0': 0.035, 'acc_0_1': 0.2146, 'acc_1_0': 0.9346, 'acc_1_1': 0.6667, 'mean_acc': 0.2746}\n",
      "--- Epoch 29 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.083)\tDT 0.091 (2.257)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8742774128913879\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00020092970225960016\n",
      "(Last) Denominator in Train(Contrastive Learning): 751.1768798828125\n",
      "Loss in Train(Contrastive Learning): 6.756789570159101\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.042)\tDT 0.004 (0.039)\tloss 1.660 (1.616)\tAcc@1 0.141 (0.182)\n",
      "Train(class): {'worst_acc': 0.0252, 'acc_0_0': 0.0252, 'acc_0_1': 0.1685, 'acc_1_0': 0.9286, 'acc_1_1': 0.7133, 'mean_acc': 0.1929}\n",
      "Val(class): {'weighted_mean_acc': 0.1982, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2146, 'acc_1_0': 0.9398, 'acc_1_1': 0.6842, 'mean_acc': 0.2786}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4091 (1.1836)\tAcc@1 0.836 (0.371)\n",
      "Test(class): [0/1]\tTime 0.007 (0.023)\tLoss 1.7881 (1.3182)\tAcc@1 0.062 (0.294)\n",
      "Test(class): {'weighted_mean_acc': 0.1903, 'worst_acc': 0.0337, 'acc_0_0': 0.0337, 'acc_0_1': 0.2106, 'acc_1_0': 0.9408, 'acc_1_1': 0.6651, 'mean_acc': 0.273}\n",
      "--- Epoch 30 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.084)\tDT 0.089 (2.275)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8764107823371887\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0001863538200268522\n",
      "(Last) Denominator in Train(Contrastive Learning): 752.9969482421875\n",
      "Loss in Train(Contrastive Learning): 6.756746195732279\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.041)\tDT 0.003 (0.038)\tloss 1.626 (1.588)\tAcc@1 0.195 (0.202)\n",
      "Train(class): {'worst_acc': 0.0269, 'acc_0_0': 0.0269, 'acc_0_1': 0.1793, 'acc_1_0': 0.9286, 'acc_1_1': 0.719, 'mean_acc': 0.1958}\n",
      "Val(class): {'weighted_mean_acc': 0.198, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.206, 'acc_1_0': 0.9474, 'acc_1_1': 0.6917, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.4061 (1.1871)\tAcc@1 0.844 (0.371)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8012 (1.3242)\tAcc@1 0.062 (0.292)\n",
      "Test(class): {'weighted_mean_acc': 0.1895, 'worst_acc': 0.031, 'acc_0_0': 0.031, 'acc_0_1': 0.2044, 'acc_1_0': 0.9424, 'acc_1_1': 0.6713, 'mean_acc': 0.2705}\n",
      "--- Epoch 31 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.084)\tDT 0.082 (2.281)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8783280253410339\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00018023172742687166\n",
      "(Last) Denominator in Train(Contrastive Learning): 754.6386108398438\n",
      "Loss in Train(Contrastive Learning): 6.756706270765751\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.041)\tDT 0.001 (0.038)\tloss 1.589 (1.606)\tAcc@1 0.203 (0.196)\n",
      "Train(class): {'worst_acc': 0.0272, 'acc_0_0': 0.0272, 'acc_0_1': 0.1576, 'acc_1_0': 0.9286, 'acc_1_1': 0.7209, 'mean_acc': 0.1956}\n",
      "Val(class): {'weighted_mean_acc': 0.1993, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.1974, 'acc_1_0': 0.9474, 'acc_1_1': 0.6992, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.3976 (1.1976)\tAcc@1 0.867 (0.371)\n",
      "Test(class): [0/1]\tTime 0.005 (0.022)\tLoss 1.8173 (1.3382)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1938, 'worst_acc': 0.031, 'acc_0_0': 0.031, 'acc_0_1': 0.1916, 'acc_1_0': 0.9455, 'acc_1_1': 0.6931, 'mean_acc': 0.2682}\n",
      "--- Epoch 32 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.084)\tDT 0.084 (2.285)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8801820874214172\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00016701010463293642\n",
      "(Last) Denominator in Train(Contrastive Learning): 756.2198486328125\n",
      "Loss in Train(Contrastive Learning): 6.756669501040844\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.041)\tDT 0.001 (0.038)\tloss 1.553 (1.595)\tAcc@1 0.219 (0.190)\n",
      "Train(class): {'worst_acc': 0.0289, 'acc_0_0': 0.0289, 'acc_0_1': 0.1522, 'acc_1_0': 0.9286, 'acc_1_1': 0.7152, 'mean_acc': 0.1954}\n",
      "Val(class): {'weighted_mean_acc': 0.2013, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.206, 'acc_1_0': 0.9474, 'acc_1_1': 0.7068, 'mean_acc': 0.2777}\n",
      "Test(class): [0/1]\tTime 0.003 (0.040)\tLoss 0.4033 (1.1927)\tAcc@1 0.852 (0.369)\n",
      "Test(class): [0/1]\tTime 0.009 (0.023)\tLoss 1.8203 (1.3324)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1902, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1956, 'acc_1_0': 0.9408, 'acc_1_1': 0.6791, 'mean_acc': 0.2673}\n",
      "--- Epoch 33 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.085)\tDT 0.084 (2.293)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8820459842681885\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0001588101586094126\n",
      "(Last) Denominator in Train(Contrastive Learning): 757.8139038085938\n",
      "Loss in Train(Contrastive Learning): 6.756635262611065\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.043)\tDT 0.004 (0.040)\tloss 1.521 (1.606)\tAcc@1 0.250 (0.190)\n",
      "Train(class): {'worst_acc': 0.0289, 'acc_0_0': 0.0289, 'acc_0_1': 0.1685, 'acc_1_0': 0.9286, 'acc_1_1': 0.7105, 'mean_acc': 0.195}\n",
      "Val(class): {'weighted_mean_acc': 0.2017, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.2146, 'acc_1_0': 0.9474, 'acc_1_1': 0.7068, 'mean_acc': 0.2811}\n",
      "Test(class): [0/1]\tTime 0.002 (0.041)\tLoss 0.4070 (1.1843)\tAcc@1 0.836 (0.371)\n",
      "Test(class): [0/1]\tTime 0.001 (0.023)\tLoss 1.7918 (1.3199)\tAcc@1 0.062 (0.293)\n",
      "Test(class): {'weighted_mean_acc': 0.1899, 'worst_acc': 0.0328, 'acc_0_0': 0.0328, 'acc_0_1': 0.2089, 'acc_1_0': 0.9408, 'acc_1_1': 0.6667, 'mean_acc': 0.2722}\n",
      "--- Epoch 34 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.075)\tDT 0.085 (1.984)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8835715651512146\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00015430884377565235\n",
      "(Last) Denominator in Train(Contrastive Learning): 759.1205444335938\n",
      "Loss in Train(Contrastive Learning): 6.756603560549148\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.043)\tDT 0.001 (0.039)\tloss 1.655 (1.593)\tAcc@1 0.156 (0.200)\n",
      "Train(class): {'worst_acc': 0.0272, 'acc_0_0': 0.0272, 'acc_0_1': 0.1739, 'acc_1_0': 0.9286, 'acc_1_1': 0.719, 'mean_acc': 0.1958}\n",
      "Val(class): {'weighted_mean_acc': 0.2012, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.2017, 'acc_1_0': 0.9474, 'acc_1_1': 0.7068, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.013 (0.042)\tLoss 0.4036 (1.1923)\tAcc@1 0.852 (0.369)\n",
      "Test(class): [0/1]\tTime 0.003 (0.023)\tLoss 1.8134 (1.3309)\tAcc@1 0.055 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1905, 'worst_acc': 0.031, 'acc_0_0': 0.031, 'acc_0_1': 0.1956, 'acc_1_0': 0.9408, 'acc_1_1': 0.6776, 'mean_acc': 0.2675}\n",
      "--- Epoch 35 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.084)\tDT 0.082 (2.274)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8852733969688416\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00014616645057685673\n",
      "(Last) Denominator in Train(Contrastive Learning): 760.5753784179688\n",
      "Loss in Train(Contrastive Learning): 6.7565741539001465\n",
      "Train(class): [0][1/2]\tBT 0.014 (0.042)\tDT 0.011 (0.039)\tloss 1.660 (1.596)\tAcc@1 0.195 (0.205)\n",
      "Train(class): {'worst_acc': 0.0292, 'acc_0_0': 0.0292, 'acc_0_1': 0.1739, 'acc_1_0': 0.9464, 'acc_1_1': 0.719, 'mean_acc': 0.1975}\n",
      "Val(class): {'weighted_mean_acc': 0.1995, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9549, 'acc_1_1': 0.7068, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.005 (0.040)\tLoss 0.3995 (1.2000)\tAcc@1 0.859 (0.371)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8231 (1.3406)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1899, 'worst_acc': 0.0284, 'acc_0_0': 0.0284, 'acc_0_1': 0.1929, 'acc_1_0': 0.9455, 'acc_1_1': 0.6838, 'mean_acc': 0.2667}\n",
      "--- Epoch 36 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.084)\tDT 0.084 (2.284)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8869076371192932\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00014015909982845187\n",
      "(Last) Denominator in Train(Contrastive Learning): 761.9740600585938\n",
      "Loss in Train(Contrastive Learning): 6.756546444081246\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.041)\tDT 0.002 (0.039)\tloss 1.635 (1.613)\tAcc@1 0.172 (0.188)\n",
      "Train(class): {'worst_acc': 0.028, 'acc_0_0': 0.028, 'acc_0_1': 0.1902, 'acc_1_0': 0.9464, 'acc_1_1': 0.7133, 'mean_acc': 0.196}\n",
      "Val(class): {'weighted_mean_acc': 0.2027, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.1996, 'acc_1_0': 0.9474, 'acc_1_1': 0.7143, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.005 (0.041)\tLoss 0.4007 (1.1913)\tAcc@1 0.859 (0.371)\n",
      "Test(class): [0/1]\tTime 0.012 (0.024)\tLoss 1.8001 (1.3286)\tAcc@1 0.055 (0.291)\n",
      "Test(class): {'weighted_mean_acc': 0.1933, 'worst_acc': 0.0324, 'acc_0_0': 0.0324, 'acc_0_1': 0.1973, 'acc_1_0': 0.9424, 'acc_1_1': 0.6854, 'mean_acc': 0.2698}\n",
      "--- Epoch 37 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.085)\tDT 0.085 (2.324)\tloss 6.757 (6.757)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8883284330368042\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00013668522296939045\n",
      "(Last) Denominator in Train(Contrastive Learning): 763.1915283203125\n",
      "Loss in Train(Contrastive Learning): 6.756520897784132\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.042)\tDT 0.004 (0.038)\tloss 1.616 (1.609)\tAcc@1 0.156 (0.186)\n",
      "Train(class): {'worst_acc': 0.0272, 'acc_0_0': 0.0272, 'acc_0_1': 0.1522, 'acc_1_0': 0.9286, 'acc_1_1': 0.7105, 'mean_acc': 0.1931}\n",
      "Val(class): {'weighted_mean_acc': 0.2012, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9474, 'acc_1_1': 0.7143, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4023 (1.1937)\tAcc@1 0.852 (0.370)\n",
      "Test(class): [0/1]\tTime 0.004 (0.022)\tLoss 1.8123 (1.3316)\tAcc@1 0.055 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1899, 'worst_acc': 0.0297, 'acc_0_0': 0.0297, 'acc_0_1': 0.1956, 'acc_1_0': 0.9455, 'acc_1_1': 0.6791, 'mean_acc': 0.2677}\n",
      "--- Epoch 38 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.076)\tDT 0.074 (2.039)\tloss 6.757 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8897652626037598\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0001301229785894975\n",
      "(Last) Denominator in Train(Contrastive Learning): 764.4201049804688\n",
      "Loss in Train(Contrastive Learning): 6.756496738880239\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.042)\tDT 0.004 (0.039)\tloss 1.470 (1.600)\tAcc@1 0.297 (0.195)\n",
      "Train(class): {'worst_acc': 0.028, 'acc_0_0': 0.028, 'acc_0_1': 0.1793, 'acc_1_0': 0.9107, 'acc_1_1': 0.7256, 'mean_acc': 0.1979}\n",
      "Val(class): {'weighted_mean_acc': 0.2014, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.206, 'acc_1_0': 0.9474, 'acc_1_1': 0.7143, 'mean_acc': 0.2777}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.4019 (1.1940)\tAcc@1 0.852 (0.372)\n",
      "Test(class): [0/1]\tTime 0.007 (0.022)\tLoss 1.8116 (1.3312)\tAcc@1 0.062 (0.292)\n",
      "Test(class): {'weighted_mean_acc': 0.1912, 'worst_acc': 0.0319, 'acc_0_0': 0.0319, 'acc_0_1': 0.204, 'acc_1_0': 0.9439, 'acc_1_1': 0.676, 'mean_acc': 0.2713}\n",
      "--- Epoch 39 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.088)\tDT 0.088 (2.394)\tloss 6.757 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8911719918251038\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00012449728092178702\n",
      "(Last) Denominator in Train(Contrastive Learning): 765.6236572265625\n",
      "Loss in Train(Contrastive Learning): 6.756474000342349\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.041)\tDT 0.001 (0.038)\tloss 1.567 (1.600)\tAcc@1 0.219 (0.200)\n",
      "Train(class): {'worst_acc': 0.028, 'acc_0_0': 0.028, 'acc_0_1': 0.1848, 'acc_1_0': 0.9464, 'acc_1_1': 0.719, 'mean_acc': 0.1971}\n",
      "Val(class): {'weighted_mean_acc': 0.1998, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.2103, 'acc_1_0': 0.9474, 'acc_1_1': 0.6992, 'mean_acc': 0.2786}\n",
      "Test(class): [0/1]\tTime 0.003 (0.039)\tLoss 0.4065 (1.1944)\tAcc@1 0.836 (0.370)\n",
      "Test(class): [0/1]\tTime 0.005 (0.022)\tLoss 1.8285 (1.3362)\tAcc@1 0.055 (0.291)\n",
      "Test(class): {'weighted_mean_acc': 0.1901, 'worst_acc': 0.0315, 'acc_0_0': 0.0315, 'acc_0_1': 0.2027, 'acc_1_0': 0.9408, 'acc_1_1': 0.6729, 'mean_acc': 0.2699}\n",
      "--- Epoch 40 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.086)\tDT 0.081 (2.356)\tloss 6.757 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8925011157989502\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00012181050988147035\n",
      "(Last) Denominator in Train(Contrastive Learning): 766.7630615234375\n",
      "Loss in Train(Contrastive Learning): 6.756452907907202\n",
      "Train(class): [0][1/2]\tBT 0.003 (0.042)\tDT 0.001 (0.039)\tloss 1.513 (1.607)\tAcc@1 0.266 (0.191)\n",
      "Train(class): {'worst_acc': 0.0274, 'acc_0_0': 0.0274, 'acc_0_1': 0.1685, 'acc_1_0': 0.9286, 'acc_1_1': 0.7247, 'mean_acc': 0.1971}\n",
      "Val(class): {'weighted_mean_acc': 0.2012, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9549, 'acc_1_1': 0.7143, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.003 (0.039)\tLoss 0.3981 (1.1944)\tAcc@1 0.859 (0.373)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8146 (1.3338)\tAcc@1 0.062 (0.291)\n",
      "Test(class): {'weighted_mean_acc': 0.192, 'worst_acc': 0.0306, 'acc_0_0': 0.0306, 'acc_0_1': 0.1969, 'acc_1_0': 0.9439, 'acc_1_1': 0.6854, 'mean_acc': 0.2691}\n",
      "--- Epoch 41 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.081)\tDT 0.089 (2.186)\tloss 6.757 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8936907052993774\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00011672831897158176\n",
      "(Last) Denominator in Train(Contrastive Learning): 767.7805786132812\n",
      "Loss in Train(Contrastive Learning): 6.756432634718875\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.645 (1.612)\tAcc@1 0.172 (0.192)\n",
      "Train(class): {'worst_acc': 0.0257, 'acc_0_0': 0.0257, 'acc_0_1': 0.1685, 'acc_1_0': 0.9821, 'acc_1_1': 0.7256, 'mean_acc': 0.1967}\n",
      "Val(class): {'weighted_mean_acc': 0.2014, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.206, 'acc_1_0': 0.9474, 'acc_1_1': 0.7143, 'mean_acc': 0.2777}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3996 (1.1980)\tAcc@1 0.852 (0.372)\n",
      "Test(class): [0/1]\tTime 0.001 (0.022)\tLoss 1.8270 (1.3388)\tAcc@1 0.055 (0.291)\n",
      "Test(class): {'weighted_mean_acc': 0.1917, 'worst_acc': 0.0306, 'acc_0_0': 0.0306, 'acc_0_1': 0.1982, 'acc_1_0': 0.9455, 'acc_1_1': 0.6838, 'mean_acc': 0.2696}\n",
      "--- Epoch 42 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.085)\tDT 0.086 (2.332)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8950013518333435\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.0001121599052567035\n",
      "(Last) Denominator in Train(Contrastive Learning): 768.9025268554688\n",
      "Loss in Train(Contrastive Learning): 6.756413589132593\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.042)\tDT 0.002 (0.039)\tloss 1.593 (1.604)\tAcc@1 0.227 (0.194)\n",
      "Train(class): {'worst_acc': 0.0274, 'acc_0_0': 0.0274, 'acc_0_1': 0.163, 'acc_1_0': 0.9464, 'acc_1_1': 0.7181, 'mean_acc': 0.1956}\n",
      "Val(class): {'weighted_mean_acc': 0.1982, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2146, 'acc_1_0': 0.9398, 'acc_1_1': 0.6842, 'mean_acc': 0.2786}\n",
      "Test(class): [0/1]\tTime 0.003 (0.041)\tLoss 0.4148 (1.1826)\tAcc@1 0.828 (0.369)\n",
      "Test(class): [0/1]\tTime 0.006 (0.023)\tLoss 1.7925 (1.3170)\tAcc@1 0.062 (0.292)\n",
      "Test(class): {'weighted_mean_acc': 0.1882, 'worst_acc': 0.0333, 'acc_0_0': 0.0333, 'acc_0_1': 0.2106, 'acc_1_0': 0.9346, 'acc_1_1': 0.6573, 'mean_acc': 0.2713}\n",
      "--- Epoch 43 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.086)\tDT 0.086 (2.338)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8961576819419861\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00010979581566061825\n",
      "(Last) Denominator in Train(Contrastive Learning): 769.8937377929688\n",
      "Loss in Train(Contrastive Learning): 6.756395852312129\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.630 (1.599)\tAcc@1 0.219 (0.198)\n",
      "Train(class): {'worst_acc': 0.026, 'acc_0_0': 0.026, 'acc_0_1': 0.1685, 'acc_1_0': 0.9464, 'acc_1_1': 0.7105, 'mean_acc': 0.1931}\n",
      "Val(class): {'weighted_mean_acc': 0.2046, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2082, 'acc_1_0': 0.9474, 'acc_1_1': 0.7143, 'mean_acc': 0.2802}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4060 (1.1848)\tAcc@1 0.852 (0.374)\n",
      "Test(class): [0/1]\tTime 0.005 (0.022)\tLoss 1.8060 (1.3223)\tAcc@1 0.062 (0.294)\n",
      "Test(class): {'weighted_mean_acc': 0.1916, 'worst_acc': 0.0315, 'acc_0_0': 0.0315, 'acc_0_1': 0.2049, 'acc_1_0': 0.9455, 'acc_1_1': 0.6791, 'mean_acc': 0.272}\n",
      "--- Epoch 44 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.012 (0.074)\tDT 0.104 (1.976)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8973258137702942\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00010528694110689685\n",
      "(Last) Denominator in Train(Contrastive Learning): 770.893310546875\n",
      "Loss in Train(Contrastive Learning): 6.7563788459656084\n",
      "Train(class): [0][1/2]\tBT 0.008 (0.043)\tDT 0.005 (0.039)\tloss 1.696 (1.607)\tAcc@1 0.172 (0.194)\n",
      "Train(class): {'worst_acc': 0.0257, 'acc_0_0': 0.0257, 'acc_0_1': 0.163, 'acc_1_0': 0.9286, 'acc_1_1': 0.7237, 'mean_acc': 0.1954}\n",
      "Val(class): {'weighted_mean_acc': 0.2011, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.1996, 'acc_1_0': 0.9474, 'acc_1_1': 0.7068, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4008 (1.1953)\tAcc@1 0.844 (0.372)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.8238 (1.3346)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1916, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1956, 'acc_1_0': 0.947, 'acc_1_1': 0.6854, 'mean_acc': 0.2687}\n",
      "--- Epoch 45 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.086)\tDT 0.086 (2.334)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8984692692756653\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 0.00010274562373524532\n",
      "(Last) Denominator in Train(Contrastive Learning): 771.8733520507812\n",
      "Loss in Train(Contrastive Learning): 6.75636277553883\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.042)\tDT 0.004 (0.039)\tloss 1.640 (1.594)\tAcc@1 0.156 (0.197)\n",
      "Train(class): {'worst_acc': 0.0252, 'acc_0_0': 0.0252, 'acc_0_1': 0.1793, 'acc_1_0': 0.9286, 'acc_1_1': 0.719, 'mean_acc': 0.1946}\n",
      "Val(class): {'weighted_mean_acc': 0.1994, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9474, 'acc_1_1': 0.7068, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4033 (1.1946)\tAcc@1 0.852 (0.372)\n",
      "Test(class): [0/1]\tTime 0.009 (0.022)\tLoss 1.8225 (1.3353)\tAcc@1 0.055 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1906, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1951, 'acc_1_0': 0.9455, 'acc_1_1': 0.6807, 'mean_acc': 0.2679}\n",
      "--- Epoch 46 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.087)\tDT 0.083 (2.373)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.8994977474212646\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 9.835295350058004e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 772.7530517578125\n",
      "Loss in Train(Contrastive Learning): 6.756347443195099\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.628 (1.612)\tAcc@1 0.211 (0.194)\n",
      "Train(class): {'worst_acc': 0.0263, 'acc_0_0': 0.0263, 'acc_0_1': 0.163, 'acc_1_0': 0.9464, 'acc_1_1': 0.7247, 'mean_acc': 0.1962}\n",
      "Val(class): {'weighted_mean_acc': 0.2027, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1931, 'acc_1_0': 0.9549, 'acc_1_1': 0.7218, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3942 (1.2028)\tAcc@1 0.859 (0.371)\n",
      "Test(class): [0/1]\tTime 0.003 (0.023)\tLoss 1.8283 (1.3422)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1932, 'worst_acc': 0.0288, 'acc_0_0': 0.0288, 'acc_0_1': 0.1898, 'acc_1_0': 0.947, 'acc_1_1': 0.6978, 'mean_acc': 0.2673}\n",
      "--- Epoch 47 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.075)\tDT 0.082 (1.983)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9005465507507324\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 9.520192543277517e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 773.6512451171875\n",
      "Loss in Train(Contrastive Learning): 6.756332942780028\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.658 (1.611)\tAcc@1 0.156 (0.195)\n",
      "Train(class): {'worst_acc': 0.028, 'acc_0_0': 0.028, 'acc_0_1': 0.1848, 'acc_1_0': 0.9286, 'acc_1_1': 0.7171, 'mean_acc': 0.1965}\n",
      "Val(class): {'weighted_mean_acc': 0.2028, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1931, 'acc_1_0': 0.9624, 'acc_1_1': 0.7218, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.002 (0.041)\tLoss 0.3940 (1.2008)\tAcc@1 0.859 (0.372)\n",
      "Test(class): [0/1]\tTime 0.003 (0.023)\tLoss 1.8355 (1.3432)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1947, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1867, 'acc_1_0': 0.947, 'acc_1_1': 0.7009, 'mean_acc': 0.267}\n",
      "--- Epoch 48 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.087)\tDT 0.083 (2.369)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.90156489610672\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 9.44813436944969e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 774.525390625\n",
      "Loss in Train(Contrastive Learning): 6.756319051093244\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.043)\tDT 0.003 (0.039)\tloss 1.517 (1.599)\tAcc@1 0.250 (0.198)\n",
      "Train(class): {'worst_acc': 0.0274, 'acc_0_0': 0.0274, 'acc_0_1': 0.163, 'acc_1_0': 0.9643, 'acc_1_1': 0.7275, 'mean_acc': 0.1979}\n",
      "Val(class): {'weighted_mean_acc': 0.2027, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.191, 'acc_1_0': 0.9624, 'acc_1_1': 0.7218, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.005 (0.034)\tLoss 0.3920 (1.2045)\tAcc@1 0.867 (0.371)\n",
      "Test(class): [0/1]\tTime 0.001 (0.020)\tLoss 1.8363 (1.3473)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1935, 'worst_acc': 0.0284, 'acc_0_0': 0.0284, 'acc_0_1': 0.1871, 'acc_1_0': 0.9486, 'acc_1_1': 0.7009, 'mean_acc': 0.2667}\n",
      "--- Epoch 49 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.074)\tDT 0.106 (1.981)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9024943709373474\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 8.93718606675975e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 775.3194580078125\n",
      "Loss in Train(Contrastive Learning): 6.756305841689414\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.042)\tDT 0.004 (0.039)\tloss 1.656 (1.608)\tAcc@1 0.211 (0.196)\n",
      "Train(class): {'worst_acc': 0.0269, 'acc_0_0': 0.0269, 'acc_0_1': 0.1685, 'acc_1_0': 0.9464, 'acc_1_1': 0.7256, 'mean_acc': 0.1971}\n",
      "Val(class): {'weighted_mean_acc': 0.2025, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.191, 'acc_1_0': 0.9474, 'acc_1_1': 0.7218, 'mean_acc': 0.2727}\n",
      "Test(class): [0/1]\tTime 0.005 (0.041)\tLoss 0.3934 (1.1969)\tAcc@1 0.859 (0.371)\n",
      "Test(class): [0/1]\tTime 0.001 (0.023)\tLoss 1.8250 (1.3370)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1938, 'worst_acc': 0.0288, 'acc_0_0': 0.0288, 'acc_0_1': 0.188, 'acc_1_0': 0.947, 'acc_1_1': 0.7009, 'mean_acc': 0.267}\n",
      "--- Epoch 50 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.087)\tDT 0.085 (2.389)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9034625887870789\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 8.720607002032921e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 776.1492309570312\n",
      "Loss in Train(Contrastive Learning): 6.756293289204861\n",
      "Train(class): [0][1/2]\tBT 0.009 (0.042)\tDT 0.006 (0.039)\tloss 1.530 (1.621)\tAcc@1 0.242 (0.193)\n",
      "Train(class): {'worst_acc': 0.0257, 'acc_0_0': 0.0257, 'acc_0_1': 0.1793, 'acc_1_0': 0.9286, 'acc_1_1': 0.7266, 'mean_acc': 0.1967}\n",
      "Val(class): {'weighted_mean_acc': 0.2026, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.191, 'acc_1_0': 0.9549, 'acc_1_1': 0.7218, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.041)\tLoss 0.3947 (1.2013)\tAcc@1 0.852 (0.371)\n",
      "Test(class): [0/1]\tTime 0.006 (0.023)\tLoss 1.8337 (1.3440)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1926, 'worst_acc': 0.0293, 'acc_0_0': 0.0293, 'acc_0_1': 0.1916, 'acc_1_0': 0.947, 'acc_1_1': 0.6931, 'mean_acc': 0.2677}\n",
      "--- Epoch 51 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.080)\tDT 0.083 (2.153)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9043667316436768\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 8.519008406437933e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 776.9241943359375\n",
      "Loss in Train(Contrastive Learning): 6.756281358130435\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.043)\tDT 0.003 (0.040)\tloss 1.573 (1.608)\tAcc@1 0.211 (0.198)\n",
      "Train(class): {'worst_acc': 0.0266, 'acc_0_0': 0.0266, 'acc_0_1': 0.1739, 'acc_1_0': 0.9464, 'acc_1_1': 0.72, 'mean_acc': 0.1958}\n",
      "Val(class): {'weighted_mean_acc': 0.1947, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.2039, 'acc_1_0': 0.9474, 'acc_1_1': 0.6842, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.003 (0.040)\tLoss 0.4062 (1.1920)\tAcc@1 0.836 (0.371)\n",
      "Test(class): [0/1]\tTime 0.001 (0.023)\tLoss 1.8196 (1.3308)\tAcc@1 0.055 (0.291)\n",
      "Test(class): {'weighted_mean_acc': 0.189, 'worst_acc': 0.031, 'acc_0_0': 0.031, 'acc_0_1': 0.2013, 'acc_1_0': 0.9439, 'acc_1_1': 0.6698, 'mean_acc': 0.2692}\n",
      "--- Epoch 52 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.088)\tDT 0.084 (2.400)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9052035212516785\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 8.273041748907417e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 777.640869140625\n",
      "Loss in Train(Contrastive Learning): 6.756269855702177\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.043)\tDT 0.003 (0.040)\tloss 1.624 (1.614)\tAcc@1 0.164 (0.198)\n",
      "Train(class): {'worst_acc': 0.0283, 'acc_0_0': 0.0283, 'acc_0_1': 0.163, 'acc_1_0': 0.9643, 'acc_1_1': 0.7323, 'mean_acc': 0.1996}\n",
      "Val(class): {'weighted_mean_acc': 0.2015, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.2039, 'acc_1_0': 0.9624, 'acc_1_1': 0.7143, 'mean_acc': 0.2786}\n",
      "Test(class): [0/1]\tTime 0.003 (0.041)\tLoss 0.4002 (1.2001)\tAcc@1 0.852 (0.370)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.8326 (1.3411)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1905, 'worst_acc': 0.0293, 'acc_0_0': 0.0293, 'acc_0_1': 0.1933, 'acc_1_0': 0.9455, 'acc_1_1': 0.6838, 'mean_acc': 0.2672}\n",
      "--- Epoch 53 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.012 (0.081)\tDT 0.084 (2.188)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.906104326248169\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 7.935850590001792e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 778.4118041992188\n",
      "Loss in Train(Contrastive Learning): 6.756258809820134\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.621 (1.601)\tAcc@1 0.211 (0.198)\n",
      "Train(class): {'worst_acc': 0.0249, 'acc_0_0': 0.0249, 'acc_0_1': 0.163, 'acc_1_0': 0.9286, 'acc_1_1': 0.7228, 'mean_acc': 0.1946}\n",
      "Val(class): {'weighted_mean_acc': 0.2011, 'worst_acc': 0.0385, 'acc_0_0': 0.0385, 'acc_0_1': 0.2039, 'acc_1_0': 0.9474, 'acc_1_1': 0.6992, 'mean_acc': 0.2769}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4098 (1.1852)\tAcc@1 0.836 (0.371)\n",
      "Test(class): [0/1]\tTime 0.007 (0.022)\tLoss 1.8008 (1.3221)\tAcc@1 0.047 (0.292)\n",
      "Test(class): {'weighted_mean_acc': 0.1904, 'worst_acc': 0.0319, 'acc_0_0': 0.0319, 'acc_0_1': 0.2044, 'acc_1_0': 0.9393, 'acc_1_1': 0.6729, 'mean_acc': 0.2706}\n",
      "--- Epoch 54 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.087)\tDT 0.088 (2.389)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9069177508354187\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 7.7887911174912e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 779.1092529296875\n",
      "Loss in Train(Contrastive Learning): 6.756248367593644\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.043)\tDT 0.002 (0.040)\tloss 1.460 (1.604)\tAcc@1 0.273 (0.199)\n",
      "Train(class): {'worst_acc': 0.0289, 'acc_0_0': 0.0289, 'acc_0_1': 0.1467, 'acc_1_0': 0.9464, 'acc_1_1': 0.7285, 'mean_acc': 0.1983}\n",
      "Val(class): {'weighted_mean_acc': 0.1979, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.2017, 'acc_1_0': 0.9474, 'acc_1_1': 0.6992, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.003 (0.040)\tLoss 0.4049 (1.1935)\tAcc@1 0.836 (0.370)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.8250 (1.3336)\tAcc@1 0.055 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1893, 'worst_acc': 0.0315, 'acc_0_0': 0.0315, 'acc_0_1': 0.2009, 'acc_1_0': 0.9424, 'acc_1_1': 0.6698, 'mean_acc': 0.2691}\n",
      "--- Epoch 55 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.088)\tDT 0.099 (2.421)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.907770574092865\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 7.584581908304244e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 779.8400268554688\n",
      "Loss in Train(Contrastive Learning): 6.756238252558607\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.042)\tDT 0.004 (0.038)\tloss 1.629 (1.604)\tAcc@1 0.203 (0.197)\n",
      "Train(class): {'worst_acc': 0.026, 'acc_0_0': 0.026, 'acc_0_1': 0.163, 'acc_1_0': 0.9464, 'acc_1_1': 0.7105, 'mean_acc': 0.1929}\n",
      "Val(class): {'weighted_mean_acc': 0.201, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1953, 'acc_1_0': 0.9474, 'acc_1_1': 0.7143, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.003 (0.040)\tLoss 0.4030 (1.1977)\tAcc@1 0.844 (0.369)\n",
      "Test(class): [0/1]\tTime 0.005 (0.023)\tLoss 1.8277 (1.3374)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1905, 'worst_acc': 0.0297, 'acc_0_0': 0.0297, 'acc_0_1': 0.1938, 'acc_1_0': 0.9424, 'acc_1_1': 0.6822, 'mean_acc': 0.267}\n",
      "--- Epoch 56 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.012 (0.078)\tDT 0.723 (2.096)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9085161089897156\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 7.378782902378589e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 780.4786987304688\n",
      "Loss in Train(Contrastive Learning): 6.7562285839243135\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.043)\tDT 0.001 (0.040)\tloss 1.588 (1.603)\tAcc@1 0.195 (0.200)\n",
      "Train(class): {'worst_acc': 0.026, 'acc_0_0': 0.026, 'acc_0_1': 0.1793, 'acc_1_0': 0.9107, 'acc_1_1': 0.7304, 'mean_acc': 0.1975}\n",
      "Val(class): {'weighted_mean_acc': 0.2013, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9549, 'acc_1_1': 0.7143, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.002 (0.041)\tLoss 0.3992 (1.1991)\tAcc@1 0.852 (0.371)\n",
      "Test(class): [0/1]\tTime 0.012 (0.023)\tLoss 1.8269 (1.3380)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1922, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1938, 'acc_1_0': 0.9455, 'acc_1_1': 0.6885, 'mean_acc': 0.2682}\n",
      "--- Epoch 57 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.089)\tDT 0.082 (2.438)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9093213677406311\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 7.217851816676557e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 781.1690673828125\n",
      "Loss in Train(Contrastive Learning): 6.7562192855997285\n",
      "Train(class): [0][1/2]\tBT 0.015 (0.041)\tDT 0.012 (0.038)\tloss 1.673 (1.621)\tAcc@1 0.180 (0.192)\n",
      "Train(class): {'worst_acc': 0.0272, 'acc_0_0': 0.0272, 'acc_0_1': 0.1522, 'acc_1_0': 0.9286, 'acc_1_1': 0.7285, 'mean_acc': 0.1971}\n",
      "Val(class): {'weighted_mean_acc': 0.1947, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.2017, 'acc_1_0': 0.9549, 'acc_1_1': 0.6842, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.4079 (1.1960)\tAcc@1 0.836 (0.370)\n",
      "Test(class): [0/1]\tTime 0.012 (0.022)\tLoss 1.8193 (1.3349)\tAcc@1 0.055 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.188, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.2009, 'acc_1_0': 0.9424, 'acc_1_1': 0.6682, 'mean_acc': 0.2684}\n",
      "--- Epoch 58 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.074)\tDT 0.090 (1.974)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9100503921508789\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 6.998840399319306e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 781.7933959960938\n",
      "Loss in Train(Contrastive Learning): 6.756210451430463\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.042)\tDT 0.004 (0.039)\tloss 1.556 (1.607)\tAcc@1 0.234 (0.196)\n",
      "Train(class): {'worst_acc': 0.03, 'acc_0_0': 0.03, 'acc_0_1': 0.1739, 'acc_1_0': 0.9286, 'acc_1_1': 0.7228, 'mean_acc': 0.1987}\n",
      "Val(class): {'weighted_mean_acc': 0.2013, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9549, 'acc_1_1': 0.7143, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4003 (1.1960)\tAcc@1 0.852 (0.371)\n",
      "Test(class): [0/1]\tTime 0.004 (0.022)\tLoss 1.8247 (1.3361)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1906, 'worst_acc': 0.0297, 'acc_0_0': 0.0297, 'acc_0_1': 0.1969, 'acc_1_0': 0.9439, 'acc_1_1': 0.6822, 'mean_acc': 0.2684}\n",
      "--- Epoch 59 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.089)\tDT 0.084 (2.453)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9108035564422607\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 6.835488602519035e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 782.43896484375\n",
      "Loss in Train(Contrastive Learning): 6.756201853143408\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.041)\tDT 0.001 (0.038)\tloss 1.631 (1.596)\tAcc@1 0.172 (0.203)\n",
      "Train(class): {'worst_acc': 0.0234, 'acc_0_0': 0.0234, 'acc_0_1': 0.1739, 'acc_1_0': 0.9286, 'acc_1_1': 0.7209, 'mean_acc': 0.1935}\n",
      "Val(class): {'weighted_mean_acc': 0.198, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.2017, 'acc_1_0': 0.9549, 'acc_1_1': 0.6992, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4032 (1.2024)\tAcc@1 0.844 (0.368)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8292 (1.3431)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1883, 'worst_acc': 0.0279, 'acc_0_0': 0.0279, 'acc_0_1': 0.196, 'acc_1_0': 0.9439, 'acc_1_1': 0.6776, 'mean_acc': 0.2668}\n",
      "--- Epoch 60 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.078)\tDT 0.085 (2.083)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9115580320358276\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 6.704084808006883e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 783.0859375\n",
      "Loss in Train(Contrastive Learning): 6.756193533856818\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.678 (1.621)\tAcc@1 0.148 (0.186)\n",
      "Train(class): {'worst_acc': 0.026, 'acc_0_0': 0.026, 'acc_0_1': 0.1685, 'acc_1_0': 0.9464, 'acc_1_1': 0.7275, 'mean_acc': 0.1969}\n",
      "Val(class): {'weighted_mean_acc': 0.2042, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1867, 'acc_1_0': 0.9624, 'acc_1_1': 0.7293, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.003 (0.040)\tLoss 0.3870 (1.2114)\tAcc@1 0.867 (0.371)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8480 (1.3555)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1935, 'worst_acc': 0.0279, 'acc_0_0': 0.0279, 'acc_0_1': 0.1858, 'acc_1_0': 0.9502, 'acc_1_1': 0.7025, 'mean_acc': 0.2663}\n",
      "--- Epoch 61 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.074)\tDT 0.109 (1.970)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9121931791305542\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 6.447739724535495e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 783.6292724609375\n",
      "Loss in Train(Contrastive Learning): 6.756185607707247\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.618 (1.613)\tAcc@1 0.195 (0.204)\n",
      "Train(class): {'worst_acc': 0.0292, 'acc_0_0': 0.0292, 'acc_0_1': 0.163, 'acc_1_0': 0.9286, 'acc_1_1': 0.7304, 'mean_acc': 0.1994}\n",
      "Val(class): {'weighted_mean_acc': 0.2009, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1931, 'acc_1_0': 0.9474, 'acc_1_1': 0.7143, 'mean_acc': 0.2727}\n",
      "Test(class): [0/1]\tTime 0.003 (0.039)\tLoss 0.3986 (1.1999)\tAcc@1 0.859 (0.372)\n",
      "Test(class): [0/1]\tTime 0.005 (0.022)\tLoss 1.8339 (1.3421)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1906, 'worst_acc': 0.0279, 'acc_0_0': 0.0279, 'acc_0_1': 0.1938, 'acc_1_0': 0.947, 'acc_1_1': 0.6885, 'mean_acc': 0.2675}\n",
      "--- Epoch 62 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.089)\tDT 0.087 (2.439)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.91291743516922\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 6.32512746960856e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 784.2503662109375\n",
      "Loss in Train(Contrastive Learning): 6.7561778819307365\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.042)\tDT 0.002 (0.039)\tloss 1.592 (1.608)\tAcc@1 0.203 (0.198)\n",
      "Train(class): {'worst_acc': 0.026, 'acc_0_0': 0.026, 'acc_0_1': 0.1685, 'acc_1_0': 0.9464, 'acc_1_1': 0.7304, 'mean_acc': 0.1975}\n",
      "Val(class): {'weighted_mean_acc': 0.1993, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1888, 'acc_1_0': 0.9624, 'acc_1_1': 0.7068, 'mean_acc': 0.2719}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3947 (1.2050)\tAcc@1 0.867 (0.369)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.8467 (1.3488)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1924, 'worst_acc': 0.0279, 'acc_0_0': 0.0279, 'acc_0_1': 0.1876, 'acc_1_0': 0.9455, 'acc_1_1': 0.6978, 'mean_acc': 0.266}\n",
      "--- Epoch 63 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.075)\tDT 0.288 (2.002)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.913578450679779\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 6.17525729467161e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 784.81689453125\n",
      "Loss in Train(Contrastive Learning): 6.756170485882049\n",
      "Train(class): [0][1/2]\tBT 0.009 (0.042)\tDT 0.006 (0.039)\tloss 1.547 (1.609)\tAcc@1 0.250 (0.192)\n",
      "Train(class): {'worst_acc': 0.0254, 'acc_0_0': 0.0254, 'acc_0_1': 0.1522, 'acc_1_0': 0.9643, 'acc_1_1': 0.7219, 'mean_acc': 0.1948}\n",
      "Val(class): {'weighted_mean_acc': 0.201, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.1974, 'acc_1_0': 0.9474, 'acc_1_1': 0.7068, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.002 (0.041)\tLoss 0.4025 (1.1926)\tAcc@1 0.844 (0.370)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.8175 (1.3319)\tAcc@1 0.055 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1903, 'worst_acc': 0.0306, 'acc_0_0': 0.0306, 'acc_0_1': 0.1996, 'acc_1_0': 0.9408, 'acc_1_1': 0.6776, 'mean_acc': 0.2689}\n",
      "--- Epoch 64 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.012 (0.088)\tDT 0.082 (2.418)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.914237380027771\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 6.094109994592145e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 785.3822631835938\n",
      "Loss in Train(Contrastive Learning): 6.7561634753612765\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.043)\tDT 0.001 (0.040)\tloss 1.593 (1.613)\tAcc@1 0.180 (0.189)\n",
      "Train(class): {'worst_acc': 0.024, 'acc_0_0': 0.024, 'acc_0_1': 0.1576, 'acc_1_0': 0.9464, 'acc_1_1': 0.7152, 'mean_acc': 0.1923}\n",
      "Val(class): {'weighted_mean_acc': 0.1979, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9474, 'acc_1_1': 0.6992, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.008 (0.040)\tLoss 0.4049 (1.1921)\tAcc@1 0.836 (0.372)\n",
      "Test(class): [0/1]\tTime 0.009 (0.022)\tLoss 1.8200 (1.3305)\tAcc@1 0.055 (0.292)\n",
      "Test(class): {'weighted_mean_acc': 0.1921, 'worst_acc': 0.0315, 'acc_0_0': 0.0315, 'acc_0_1': 0.2009, 'acc_1_0': 0.9439, 'acc_1_1': 0.6822, 'mean_acc': 0.2706}\n",
      "--- Epoch 65 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.074)\tDT 0.106 (1.965)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9148415327072144\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 5.884269194211811e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 785.8994140625\n",
      "Loss in Train(Contrastive Learning): 6.756156538395172\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.043)\tDT 0.002 (0.040)\tloss 1.598 (1.600)\tAcc@1 0.172 (0.192)\n",
      "Train(class): {'worst_acc': 0.024, 'acc_0_0': 0.024, 'acc_0_1': 0.1467, 'acc_1_0': 0.9464, 'acc_1_1': 0.7133, 'mean_acc': 0.1914}\n",
      "Val(class): {'weighted_mean_acc': 0.2011, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1953, 'acc_1_0': 0.9549, 'acc_1_1': 0.7143, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.003 (0.041)\tLoss 0.3994 (1.2011)\tAcc@1 0.859 (0.369)\n",
      "Test(class): [0/1]\tTime 0.003 (0.023)\tLoss 1.8333 (1.3417)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1912, 'worst_acc': 0.0293, 'acc_0_0': 0.0293, 'acc_0_1': 0.1925, 'acc_1_0': 0.9439, 'acc_1_1': 0.6869, 'mean_acc': 0.267}\n",
      "--- Epoch 66 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.012 (0.089)\tDT 0.086 (2.458)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9154695272445679\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 5.7691806432558224e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 786.4378662109375\n",
      "Loss in Train(Contrastive Learning): 6.756149875356796\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.040)\tDT 0.001 (0.037)\tloss 1.610 (1.615)\tAcc@1 0.148 (0.193)\n",
      "Train(class): {'worst_acc': 0.0237, 'acc_0_0': 0.0237, 'acc_0_1': 0.1522, 'acc_1_0': 0.9464, 'acc_1_1': 0.7342, 'mean_acc': 0.196}\n",
      "Val(class): {'weighted_mean_acc': 0.201, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1931, 'acc_1_0': 0.9549, 'acc_1_1': 0.7143, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3980 (1.1997)\tAcc@1 0.867 (0.370)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8276 (1.3399)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1912, 'worst_acc': 0.0288, 'acc_0_0': 0.0288, 'acc_0_1': 0.1916, 'acc_1_0': 0.9424, 'acc_1_1': 0.6885, 'mean_acc': 0.2665}\n",
      "--- Epoch 67 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.012 (0.076)\tDT 0.515 (2.052)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9160876870155334\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 5.6505949032725766e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 786.9678344726562\n",
      "Loss in Train(Contrastive Learning): 6.756143458346103\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.667 (1.604)\tAcc@1 0.211 (0.206)\n",
      "Train(class): {'worst_acc': 0.028, 'acc_0_0': 0.028, 'acc_0_1': 0.1413, 'acc_1_0': 0.9286, 'acc_1_1': 0.7266, 'mean_acc': 0.1969}\n",
      "Val(class): {'weighted_mean_acc': 0.1945, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9474, 'acc_1_1': 0.6842, 'mean_acc': 0.2719}\n",
      "Test(class): [0/1]\tTime 0.007 (0.039)\tLoss 0.4080 (1.1926)\tAcc@1 0.836 (0.370)\n",
      "Test(class): [0/1]\tTime 0.005 (0.022)\tLoss 1.8176 (1.3310)\tAcc@1 0.055 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1883, 'worst_acc': 0.0306, 'acc_0_0': 0.0306, 'acc_0_1': 0.2013, 'acc_1_0': 0.9408, 'acc_1_1': 0.6682, 'mean_acc': 0.2686}\n",
      "--- Epoch 68 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.092)\tDT 0.086 (2.529)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9166635274887085\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 5.533938019652851e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 787.4614868164062\n",
      "Loss in Train(Contrastive Learning): 6.756137155471964\n",
      "Train(class): [0][1/2]\tBT 0.015 (0.043)\tDT 0.012 (0.039)\tloss 1.679 (1.611)\tAcc@1 0.133 (0.196)\n",
      "Train(class): {'worst_acc': 0.0243, 'acc_0_0': 0.0243, 'acc_0_1': 0.1685, 'acc_1_0': 0.9464, 'acc_1_1': 0.7219, 'mean_acc': 0.1944}\n",
      "Val(class): {'weighted_mean_acc': 0.2013, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9624, 'acc_1_1': 0.7143, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.003 (0.041)\tLoss 0.3965 (1.2022)\tAcc@1 0.859 (0.370)\n",
      "Test(class): [0/1]\tTime 0.003 (0.023)\tLoss 1.8355 (1.3434)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1922, 'worst_acc': 0.0288, 'acc_0_0': 0.0288, 'acc_0_1': 0.1902, 'acc_1_0': 0.9455, 'acc_1_1': 0.6931, 'mean_acc': 0.2668}\n",
      "--- Epoch 69 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.080)\tDT 1.081 (2.167)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9172539710998535\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 5.4400359658757225e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 787.9678955078125\n",
      "Loss in Train(Contrastive Learning): 6.756131197543854\n",
      "Train(class): [0][1/2]\tBT 0.008 (0.042)\tDT 0.005 (0.039)\tloss 1.555 (1.608)\tAcc@1 0.211 (0.204)\n",
      "Train(class): {'worst_acc': 0.0243, 'acc_0_0': 0.0243, 'acc_0_1': 0.1685, 'acc_1_0': 0.9464, 'acc_1_1': 0.7266, 'mean_acc': 0.1954}\n",
      "Val(class): {'weighted_mean_acc': 0.2042, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1867, 'acc_1_0': 0.9624, 'acc_1_1': 0.7293, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.3927 (1.2074)\tAcc@1 0.867 (0.369)\n",
      "Test(class): [0/1]\tTime 0.004 (0.022)\tLoss 1.8521 (1.3513)\tAcc@1 0.047 (0.287)\n",
      "Test(class): {'weighted_mean_acc': 0.1921, 'worst_acc': 0.0262, 'acc_0_0': 0.0262, 'acc_0_1': 0.1854, 'acc_1_0': 0.9455, 'acc_1_1': 0.7025, 'mean_acc': 0.2649}\n",
      "--- Epoch 70 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.075)\tDT 0.104 (1.966)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9178261756896973\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 5.287436579237692e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 788.4580688476562\n",
      "Loss in Train(Contrastive Learning): 6.7561253486795625\n",
      "Train(class): [0][1/2]\tBT 0.003 (0.041)\tDT 0.001 (0.038)\tloss 1.693 (1.619)\tAcc@1 0.125 (0.191)\n",
      "Train(class): {'worst_acc': 0.0272, 'acc_0_0': 0.0272, 'acc_0_1': 0.1413, 'acc_1_0': 0.9464, 'acc_1_1': 0.7313, 'mean_acc': 0.1975}\n",
      "Val(class): {'weighted_mean_acc': 0.2026, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.1953, 'acc_1_0': 0.9474, 'acc_1_1': 0.7143, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.3999 (1.1936)\tAcc@1 0.859 (0.371)\n",
      "Test(class): [0/1]\tTime 0.011 (0.022)\tLoss 1.8120 (1.3323)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1922, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1942, 'acc_1_0': 0.9424, 'acc_1_1': 0.6885, 'mean_acc': 0.268}\n",
      "--- Epoch 71 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.090)\tDT 0.082 (2.468)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9183588624000549\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 5.1638380682561547e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 788.9146118164062\n",
      "Loss in Train(Contrastive Learning): 6.756119646924607\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.650 (1.604)\tAcc@1 0.219 (0.204)\n",
      "Train(class): {'worst_acc': 0.0237, 'acc_0_0': 0.0237, 'acc_0_1': 0.1467, 'acc_1_0': 0.9643, 'acc_1_1': 0.719, 'mean_acc': 0.1927}\n",
      "Val(class): {'weighted_mean_acc': 0.1979, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9549, 'acc_1_1': 0.6992, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.005 (0.041)\tLoss 0.4023 (1.2003)\tAcc@1 0.844 (0.367)\n",
      "Test(class): [0/1]\tTime 0.005 (0.023)\tLoss 1.8257 (1.3403)\tAcc@1 0.047 (0.287)\n",
      "Test(class): {'weighted_mean_acc': 0.1882, 'worst_acc': 0.0275, 'acc_0_0': 0.0275, 'acc_0_1': 0.1933, 'acc_1_0': 0.9424, 'acc_1_1': 0.6791, 'mean_acc': 0.2656}\n",
      "--- Epoch 72 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.077)\tDT 0.700 (2.080)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9189086556434631\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 5.082113784737885e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 789.3861694335938\n",
      "Loss in Train(Contrastive Learning): 6.756114234315588\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.042)\tDT 0.004 (0.039)\tloss 1.619 (1.612)\tAcc@1 0.188 (0.193)\n",
      "Train(class): {'worst_acc': 0.0263, 'acc_0_0': 0.0263, 'acc_0_1': 0.1739, 'acc_1_0': 0.9643, 'acc_1_1': 0.7237, 'mean_acc': 0.1967}\n",
      "Val(class): {'weighted_mean_acc': 0.1995, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.1996, 'acc_1_0': 0.9549, 'acc_1_1': 0.6992, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.009 (0.040)\tLoss 0.4018 (1.1905)\tAcc@1 0.844 (0.371)\n",
      "Test(class): [0/1]\tTime 0.003 (0.023)\tLoss 1.8156 (1.3281)\tAcc@1 0.055 (0.291)\n",
      "Test(class): {'weighted_mean_acc': 0.191, 'worst_acc': 0.031, 'acc_0_0': 0.031, 'acc_0_1': 0.1982, 'acc_1_0': 0.9439, 'acc_1_1': 0.6791, 'mean_acc': 0.2691}\n",
      "--- Epoch 73 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.091)\tDT 0.083 (2.493)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.919445812702179\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.98067238368094e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 789.8467407226562\n",
      "Loss in Train(Contrastive Learning): 6.756108933306755\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.040)\tDT 0.001 (0.038)\tloss 1.605 (1.612)\tAcc@1 0.203 (0.199)\n",
      "Train(class): {'worst_acc': 0.0246, 'acc_0_0': 0.0246, 'acc_0_1': 0.1467, 'acc_1_0': 0.9286, 'acc_1_1': 0.7342, 'mean_acc': 0.1962}\n",
      "Val(class): {'weighted_mean_acc': 0.1979, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9549, 'acc_1_1': 0.6992, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4029 (1.1979)\tAcc@1 0.844 (0.368)\n",
      "Test(class): [0/1]\tTime 0.004 (0.022)\tLoss 1.8260 (1.3385)\tAcc@1 0.055 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1892, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1965, 'acc_1_0': 0.9424, 'acc_1_1': 0.6745, 'mean_acc': 0.2673}\n",
      "--- Epoch 74 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.076)\tDT 0.661 (2.031)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9199807047843933\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.883715519099496e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 790.3053588867188\n",
      "Loss in Train(Contrastive Learning): 6.756103736289004\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.042)\tDT 0.002 (0.039)\tloss 1.583 (1.623)\tAcc@1 0.242 (0.193)\n",
      "Train(class): {'worst_acc': 0.0257, 'acc_0_0': 0.0257, 'acc_0_1': 0.1359, 'acc_1_0': 0.9464, 'acc_1_1': 0.7209, 'mean_acc': 0.194}\n",
      "Val(class): {'weighted_mean_acc': 0.2029, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9549, 'acc_1_1': 0.7218, 'mean_acc': 0.2769}\n",
      "Test(class): [0/1]\tTime 0.006 (0.040)\tLoss 0.3993 (1.1990)\tAcc@1 0.852 (0.371)\n",
      "Test(class): [0/1]\tTime 0.002 (0.022)\tLoss 1.8295 (1.3391)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1916, 'worst_acc': 0.0293, 'acc_0_0': 0.0293, 'acc_0_1': 0.1951, 'acc_1_0': 0.9439, 'acc_1_1': 0.6885, 'mean_acc': 0.2682}\n",
      "--- Epoch 75 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.072)\tDT 0.092 (1.909)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.920496940612793\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.780531526193954e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 790.7479248046875\n",
      "Loss in Train(Contrastive Learning): 6.756098739644314\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.040)\tloss 1.635 (1.610)\tAcc@1 0.203 (0.197)\n",
      "Train(class): {'worst_acc': 0.0249, 'acc_0_0': 0.0249, 'acc_0_1': 0.1359, 'acc_1_0': 0.9643, 'acc_1_1': 0.7294, 'mean_acc': 0.1954}\n",
      "Val(class): {'weighted_mean_acc': 0.1996, 'worst_acc': 0.0364, 'acc_0_0': 0.0364, 'acc_0_1': 0.2017, 'acc_1_0': 0.9549, 'acc_1_1': 0.6992, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.005 (0.040)\tLoss 0.4085 (1.1924)\tAcc@1 0.836 (0.369)\n",
      "Test(class): [0/1]\tTime 0.002 (0.022)\tLoss 1.8184 (1.3309)\tAcc@1 0.055 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1887, 'worst_acc': 0.031, 'acc_0_0': 0.031, 'acc_0_1': 0.2013, 'acc_1_0': 0.9408, 'acc_1_1': 0.6682, 'mean_acc': 0.2687}\n",
      "--- Epoch 76 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.089)\tDT 0.085 (2.447)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9209849238395691\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.7040070057846606e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 791.1664428710938\n",
      "Loss in Train(Contrastive Learning): 6.756093892645328\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.041)\tDT 0.004 (0.039)\tloss 1.589 (1.601)\tAcc@1 0.195 (0.199)\n",
      "Train(class): {'worst_acc': 0.0263, 'acc_0_0': 0.0263, 'acc_0_1': 0.163, 'acc_1_0': 0.9464, 'acc_1_1': 0.7313, 'mean_acc': 0.1977}\n",
      "Val(class): {'weighted_mean_acc': 0.1995, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9549, 'acc_1_1': 0.7068, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.014 (0.040)\tLoss 0.3990 (1.1960)\tAcc@1 0.859 (0.370)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.8272 (1.3363)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.192, 'worst_acc': 0.0293, 'acc_0_0': 0.0293, 'acc_0_1': 0.1951, 'acc_1_0': 0.9439, 'acc_1_1': 0.69, 'mean_acc': 0.2684}\n",
      "--- Epoch 77 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.077)\tDT 0.943 (2.079)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9214769005775452\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.592442564899102e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 791.5880737304688\n",
      "Loss in Train(Contrastive Learning): 6.756089190219311\n",
      "Train(class): [0][1/2]\tBT 0.009 (0.043)\tDT 0.006 (0.040)\tloss 1.622 (1.619)\tAcc@1 0.164 (0.197)\n",
      "Train(class): {'worst_acc': 0.0257, 'acc_0_0': 0.0257, 'acc_0_1': 0.1467, 'acc_1_0': 0.9464, 'acc_1_1': 0.7323, 'mean_acc': 0.1969}\n",
      "Val(class): {'weighted_mean_acc': 0.198, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9624, 'acc_1_1': 0.6992, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.006 (0.039)\tLoss 0.3997 (1.1982)\tAcc@1 0.852 (0.370)\n",
      "Test(class): [0/1]\tTime 0.006 (0.022)\tLoss 1.8239 (1.3388)\tAcc@1 0.055 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1892, 'worst_acc': 0.0288, 'acc_0_0': 0.0288, 'acc_0_1': 0.1956, 'acc_1_0': 0.9424, 'acc_1_1': 0.6791, 'mean_acc': 0.267}\n",
      "--- Epoch 78 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.089)\tDT 0.083 (2.459)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9219510555267334\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.50305815320462e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 791.99462890625\n",
      "Loss in Train(Contrastive Learning): 6.756084558811594\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.039)\tDT 0.004 (0.036)\tloss 1.634 (1.616)\tAcc@1 0.180 (0.200)\n",
      "Train(class): {'worst_acc': 0.0229, 'acc_0_0': 0.0229, 'acc_0_1': 0.163, 'acc_1_0': 0.9464, 'acc_1_1': 0.7285, 'mean_acc': 0.1946}\n",
      "Val(class): {'weighted_mean_acc': 0.1964, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.2017, 'acc_1_0': 0.9549, 'acc_1_1': 0.6917, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.002 (0.038)\tLoss 0.4038 (1.1960)\tAcc@1 0.836 (0.370)\n",
      "Test(class): [0/1]\tTime 0.013 (0.022)\tLoss 1.8281 (1.3371)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1896, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1991, 'acc_1_0': 0.9393, 'acc_1_1': 0.676, 'mean_acc': 0.2682}\n",
      "--- Epoch 79 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.071)\tDT 0.086 (1.880)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9224376082420349\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.443688885658048e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 792.4120483398438\n",
      "Loss in Train(Contrastive Learning): 6.756080150604248\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.726 (1.621)\tAcc@1 0.125 (0.192)\n",
      "Train(class): {'worst_acc': 0.0249, 'acc_0_0': 0.0249, 'acc_0_1': 0.163, 'acc_1_0': 0.9286, 'acc_1_1': 0.7247, 'mean_acc': 0.195}\n",
      "Val(class): {'weighted_mean_acc': 0.2042, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1867, 'acc_1_0': 0.9624, 'acc_1_1': 0.7293, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.3914 (1.2054)\tAcc@1 0.875 (0.368)\n",
      "Test(class): [0/1]\tTime 0.014 (0.023)\tLoss 1.8400 (1.3479)\tAcc@1 0.047 (0.287)\n",
      "Test(class): {'weighted_mean_acc': 0.1924, 'worst_acc': 0.0288, 'acc_0_0': 0.0288, 'acc_0_1': 0.1876, 'acc_1_0': 0.9424, 'acc_1_1': 0.6947, 'mean_acc': 0.2656}\n",
      "--- Epoch 80 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.077)\tDT 0.502 (1.977)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.922907292842865\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.3456289859022945e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 792.814697265625\n",
      "Loss in Train(Contrastive Learning): 6.756075838778881\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.042)\tDT 0.004 (0.038)\tloss 1.664 (1.615)\tAcc@1 0.219 (0.196)\n",
      "Train(class): {'worst_acc': 0.0229, 'acc_0_0': 0.0229, 'acc_0_1': 0.1685, 'acc_1_0': 0.9286, 'acc_1_1': 0.7247, 'mean_acc': 0.1937}\n",
      "Val(class): {'weighted_mean_acc': 0.1995, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9549, 'acc_1_1': 0.7068, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.011 (0.040)\tLoss 0.4025 (1.1917)\tAcc@1 0.852 (0.370)\n",
      "Test(class): [0/1]\tTime 0.005 (0.022)\tLoss 1.8094 (1.3293)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1915, 'worst_acc': 0.0302, 'acc_0_0': 0.0302, 'acc_0_1': 0.1938, 'acc_1_0': 0.9439, 'acc_1_1': 0.6854, 'mean_acc': 0.2677}\n",
      "--- Epoch 81 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.089)\tDT 0.085 (2.445)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9233527779579163\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.2951574869221076e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 793.1968994140625\n",
      "Loss in Train(Contrastive Learning): 6.7560715878263435\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.726 (1.630)\tAcc@1 0.125 (0.186)\n",
      "Train(class): {'worst_acc': 0.0246, 'acc_0_0': 0.0246, 'acc_0_1': 0.1522, 'acc_1_0': 0.9286, 'acc_1_1': 0.7266, 'mean_acc': 0.1948}\n",
      "Val(class): {'weighted_mean_acc': 0.1996, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9624, 'acc_1_1': 0.7068, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.3997 (1.2006)\tAcc@1 0.852 (0.370)\n",
      "Test(class): [0/1]\tTime 0.005 (0.022)\tLoss 1.8426 (1.3428)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1912, 'worst_acc': 0.0275, 'acc_0_0': 0.0275, 'acc_0_1': 0.1907, 'acc_1_0': 0.9455, 'acc_1_1': 0.6931, 'mean_acc': 0.2665}\n",
      "--- Epoch 82 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.081)\tDT 1.596 (2.179)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.923799455165863\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.185620855423622e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 793.5796508789062\n",
      "Loss in Train(Contrastive Learning): 6.756067491592245\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.001 (0.039)\tloss 1.601 (1.623)\tAcc@1 0.188 (0.189)\n",
      "Train(class): {'worst_acc': 0.0237, 'acc_0_0': 0.0237, 'acc_0_1': 0.1467, 'acc_1_0': 0.9464, 'acc_1_1': 0.7304, 'mean_acc': 0.195}\n",
      "Val(class): {'weighted_mean_acc': 0.2039, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1803, 'acc_1_0': 0.9624, 'acc_1_1': 0.7293, 'mean_acc': 0.2711}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3909 (1.2047)\tAcc@1 0.867 (0.371)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.8403 (1.3472)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1939, 'worst_acc': 0.0279, 'acc_0_0': 0.0279, 'acc_0_1': 0.1885, 'acc_1_0': 0.9455, 'acc_1_1': 0.704, 'mean_acc': 0.267}\n",
      "--- Epoch 83 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.084)\tDT 0.093 (2.181)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9242416620254517\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.119913137401454e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 793.958984375\n",
      "Loss in Train(Contrastive Learning): 6.7560635094947\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.041)\tDT 0.001 (0.039)\tloss 1.636 (1.614)\tAcc@1 0.234 (0.202)\n",
      "Train(class): {'worst_acc': 0.0246, 'acc_0_0': 0.0246, 'acc_0_1': 0.163, 'acc_1_0': 0.9464, 'acc_1_1': 0.7228, 'mean_acc': 0.1946}\n",
      "Val(class): {'weighted_mean_acc': 0.2042, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1867, 'acc_1_0': 0.9624, 'acc_1_1': 0.7293, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3909 (1.2010)\tAcc@1 0.875 (0.372)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8315 (1.3424)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1942, 'worst_acc': 0.0293, 'acc_0_0': 0.0293, 'acc_0_1': 0.1902, 'acc_1_0': 0.9439, 'acc_1_1': 0.7009, 'mean_acc': 0.2677}\n",
      "--- Epoch 84 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.090)\tDT 0.082 (2.485)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9246765375137329\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 4.067182817379944e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 794.33203125\n",
      "Loss in Train(Contrastive Learning): 6.7560596085609275\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.043)\tDT 0.001 (0.040)\tloss 1.573 (1.615)\tAcc@1 0.211 (0.196)\n",
      "Train(class): {'worst_acc': 0.0269, 'acc_0_0': 0.0269, 'acc_0_1': 0.1685, 'acc_1_0': 0.9286, 'acc_1_1': 0.7209, 'mean_acc': 0.1958}\n",
      "Val(class): {'weighted_mean_acc': 0.2013, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9624, 'acc_1_1': 0.7143, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.008 (0.038)\tLoss 0.3989 (1.1990)\tAcc@1 0.836 (0.367)\n",
      "Test(class): [0/1]\tTime 0.006 (0.021)\tLoss 1.8329 (1.3404)\tAcc@1 0.047 (0.287)\n",
      "Test(class): {'weighted_mean_acc': 0.1904, 'worst_acc': 0.0306, 'acc_0_0': 0.0306, 'acc_0_1': 0.1911, 'acc_1_0': 0.9424, 'acc_1_1': 0.6791, 'mean_acc': 0.266}\n",
      "--- Epoch 85 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.078)\tDT 1.172 (2.112)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9251105189323425\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.98576412408147e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 794.7041625976562\n",
      "Loss in Train(Contrastive Learning): 6.756055834445547\n",
      "Train(class): [0][1/2]\tBT 0.009 (0.042)\tDT 0.006 (0.039)\tloss 1.580 (1.626)\tAcc@1 0.188 (0.186)\n",
      "Train(class): {'worst_acc': 0.0257, 'acc_0_0': 0.0257, 'acc_0_1': 0.1685, 'acc_1_0': 0.9464, 'acc_1_1': 0.7228, 'mean_acc': 0.1956}\n",
      "Val(class): {'weighted_mean_acc': 0.2038, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1781, 'acc_1_0': 0.9624, 'acc_1_1': 0.7293, 'mean_acc': 0.2702}\n",
      "Test(class): [0/1]\tTime 0.013 (0.041)\tLoss 0.3900 (1.2087)\tAcc@1 0.867 (0.371)\n",
      "Test(class): [0/1]\tTime 0.003 (0.023)\tLoss 1.8493 (1.3530)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1941, 'worst_acc': 0.0275, 'acc_0_0': 0.0275, 'acc_0_1': 0.1849, 'acc_1_0': 0.9455, 'acc_1_1': 0.7072, 'mean_acc': 0.2658}\n",
      "--- Epoch 86 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.083)\tDT 0.085 (2.266)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9255216121673584\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.955442298320122e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 795.0570678710938\n",
      "Loss in Train(Contrastive Learning): 6.756052159248514\n",
      "Train(class): [0][1/2]\tBT 0.009 (0.042)\tDT 0.006 (0.039)\tloss 1.620 (1.618)\tAcc@1 0.195 (0.194)\n",
      "Train(class): {'worst_acc': 0.0254, 'acc_0_0': 0.0254, 'acc_0_1': 0.1685, 'acc_1_0': 0.9643, 'acc_1_1': 0.7219, 'mean_acc': 0.1954}\n",
      "Val(class): {'weighted_mean_acc': 0.2042, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1867, 'acc_1_0': 0.9624, 'acc_1_1': 0.7293, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.3926 (1.2060)\tAcc@1 0.867 (0.370)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8476 (1.3507)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1921, 'worst_acc': 0.0279, 'acc_0_0': 0.0279, 'acc_0_1': 0.1885, 'acc_1_0': 0.9439, 'acc_1_1': 0.6963, 'mean_acc': 0.266}\n",
      "--- Epoch 87 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.090)\tDT 0.089 (2.470)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9259231090545654\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.87800027965568e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 795.4012451171875\n",
      "Loss in Train(Contrastive Learning): 6.75604854492431\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.042)\tDT 0.004 (0.039)\tloss 1.624 (1.621)\tAcc@1 0.211 (0.192)\n",
      "Train(class): {'worst_acc': 0.0246, 'acc_0_0': 0.0246, 'acc_0_1': 0.1413, 'acc_1_0': 0.9464, 'acc_1_1': 0.7256, 'mean_acc': 0.1944}\n",
      "Val(class): {'weighted_mean_acc': 0.2009, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1888, 'acc_1_0': 0.9624, 'acc_1_1': 0.7143, 'mean_acc': 0.2727}\n",
      "Test(class): [0/1]\tTime 0.005 (0.039)\tLoss 0.3959 (1.2076)\tAcc@1 0.852 (0.367)\n",
      "Test(class): [0/1]\tTime 0.005 (0.022)\tLoss 1.8550 (1.3528)\tAcc@1 0.047 (0.286)\n",
      "Test(class): {'weighted_mean_acc': 0.1897, 'worst_acc': 0.0271, 'acc_0_0': 0.0271, 'acc_0_1': 0.1876, 'acc_1_0': 0.9439, 'acc_1_1': 0.6885, 'mean_acc': 0.2644}\n",
      "--- Epoch 88 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.079)\tDT 1.203 (2.138)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9263228178024292\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.783067222684622e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 795.7437744140625\n",
      "Loss in Train(Contrastive Learning): 6.7560450320548195\n",
      "Train(class): [0][1/2]\tBT 0.006 (0.041)\tDT 0.004 (0.037)\tloss 1.655 (1.615)\tAcc@1 0.203 (0.199)\n",
      "Train(class): {'worst_acc': 0.0266, 'acc_0_0': 0.0266, 'acc_0_1': 0.1739, 'acc_1_0': 0.9643, 'acc_1_1': 0.7266, 'mean_acc': 0.1975}\n",
      "Val(class): {'weighted_mean_acc': 0.1995, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9549, 'acc_1_1': 0.7068, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.004 (0.041)\tLoss 0.3997 (1.1972)\tAcc@1 0.844 (0.369)\n",
      "Test(class): [0/1]\tTime 0.001 (0.023)\tLoss 1.8394 (1.3392)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1896, 'worst_acc': 0.0293, 'acc_0_0': 0.0293, 'acc_0_1': 0.1951, 'acc_1_0': 0.9424, 'acc_1_1': 0.6791, 'mean_acc': 0.267}\n",
      "--- Epoch 89 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.012 (0.090)\tDT 0.085 (2.491)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9267234206199646\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.765059955185279e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 796.0877685546875\n",
      "Loss in Train(Contrastive Learning): 6.756041564839951\n",
      "Train(class): [0][1/2]\tBT 0.007 (0.044)\tDT 0.004 (0.041)\tloss 1.579 (1.617)\tAcc@1 0.203 (0.195)\n",
      "Train(class): {'worst_acc': 0.0266, 'acc_0_0': 0.0266, 'acc_0_1': 0.1576, 'acc_1_0': 0.9286, 'acc_1_1': 0.7256, 'mean_acc': 0.1962}\n",
      "Val(class): {'weighted_mean_acc': 0.201, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.191, 'acc_1_0': 0.9624, 'acc_1_1': 0.7143, 'mean_acc': 0.2736}\n",
      "Test(class): [0/1]\tTime 0.005 (0.040)\tLoss 0.3982 (1.1990)\tAcc@1 0.859 (0.371)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.8376 (1.3410)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1912, 'worst_acc': 0.0288, 'acc_0_0': 0.0288, 'acc_0_1': 0.1933, 'acc_1_0': 0.9424, 'acc_1_1': 0.6885, 'mean_acc': 0.2672}\n",
      "--- Epoch 90 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.093)\tDT 0.089 (2.579)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9271088242530823\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.683842078316957e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 796.4180908203125\n",
      "Loss in Train(Contrastive Learning): 6.756038209225269\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.038)\tDT 0.002 (0.035)\tloss 1.632 (1.605)\tAcc@1 0.219 (0.204)\n",
      "Train(class): {'worst_acc': 0.0243, 'acc_0_0': 0.0243, 'acc_0_1': 0.1848, 'acc_1_0': 0.9464, 'acc_1_1': 0.7285, 'mean_acc': 0.1965}\n",
      "Val(class): {'weighted_mean_acc': 0.1997, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9624, 'acc_1_1': 0.7068, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.4043 (1.1953)\tAcc@1 0.836 (0.369)\n",
      "Test(class): [0/1]\tTime 0.007 (0.022)\tLoss 1.8219 (1.3351)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1888, 'worst_acc': 0.0293, 'acc_0_0': 0.0293, 'acc_0_1': 0.1942, 'acc_1_0': 0.9439, 'acc_1_1': 0.676, 'mean_acc': 0.2665}\n",
      "--- Epoch 91 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.084)\tDT 1.953 (2.292)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9274952411651611\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.613652370404452e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 796.7494506835938\n",
      "Loss in Train(Contrastive Learning): 6.756034998183555\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.042)\tDT 0.002 (0.039)\tloss 1.669 (1.619)\tAcc@1 0.180 (0.194)\n",
      "Train(class): {'worst_acc': 0.024, 'acc_0_0': 0.024, 'acc_0_1': 0.1576, 'acc_1_0': 0.9286, 'acc_1_1': 0.7313, 'mean_acc': 0.1956}\n",
      "Val(class): {'weighted_mean_acc': 0.2028, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1931, 'acc_1_0': 0.9624, 'acc_1_1': 0.7218, 'mean_acc': 0.2752}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3910 (1.2071)\tAcc@1 0.859 (0.368)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8449 (1.3512)\tAcc@1 0.047 (0.287)\n",
      "Test(class): {'weighted_mean_acc': 0.1895, 'worst_acc': 0.0266, 'acc_0_0': 0.0266, 'acc_0_1': 0.1894, 'acc_1_0': 0.9486, 'acc_1_1': 0.6885, 'mean_acc': 0.2654}\n",
      "--- Epoch 92 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.092)\tDT 0.088 (2.551)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9278861880302429\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.5817738535115495e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 797.0849609375\n",
      "Loss in Train(Contrastive Learning): 6.756031830260095\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.040)\tDT 0.001 (0.037)\tloss 1.631 (1.607)\tAcc@1 0.219 (0.198)\n",
      "Train(class): {'worst_acc': 0.0249, 'acc_0_0': 0.0249, 'acc_0_1': 0.1522, 'acc_1_0': 0.9464, 'acc_1_1': 0.7209, 'mean_acc': 0.194}\n",
      "Val(class): {'weighted_mean_acc': 0.1946, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1996, 'acc_1_0': 0.9549, 'acc_1_1': 0.6842, 'mean_acc': 0.2727}\n",
      "Test(class): [0/1]\tTime 0.011 (0.043)\tLoss 0.4056 (1.1977)\tAcc@1 0.828 (0.369)\n",
      "Test(class): [0/1]\tTime 0.004 (0.023)\tLoss 1.8298 (1.3384)\tAcc@1 0.055 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1883, 'worst_acc': 0.0293, 'acc_0_0': 0.0293, 'acc_0_1': 0.1991, 'acc_1_0': 0.9424, 'acc_1_1': 0.6729, 'mean_acc': 0.2679}\n",
      "--- Epoch 93 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.095)\tDT 0.088 (2.650)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9282503724098206\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.535983705660328e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 797.3974609375\n",
      "Loss in Train(Contrastive Learning): 6.756028664873002\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.040)\tDT 0.001 (0.037)\tloss 1.652 (1.617)\tAcc@1 0.172 (0.193)\n",
      "Train(class): {'worst_acc': 0.0257, 'acc_0_0': 0.0257, 'acc_0_1': 0.1359, 'acc_1_0': 0.9464, 'acc_1_1': 0.7219, 'mean_acc': 0.1942}\n",
      "Val(class): {'weighted_mean_acc': 0.2028, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1953, 'acc_1_0': 0.9624, 'acc_1_1': 0.7218, 'mean_acc': 0.2761}\n",
      "Test(class): [0/1]\tTime 0.002 (0.041)\tLoss 0.3981 (1.2026)\tAcc@1 0.867 (0.368)\n",
      "Test(class): [0/1]\tTime 0.007 (0.023)\tLoss 1.8297 (1.3426)\tAcc@1 0.047 (0.287)\n",
      "Test(class): {'weighted_mean_acc': 0.1901, 'worst_acc': 0.0275, 'acc_0_0': 0.0275, 'acc_0_1': 0.1898, 'acc_1_0': 0.9424, 'acc_1_1': 0.6885, 'mean_acc': 0.2653}\n",
      "--- Epoch 94 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.084)\tDT 1.745 (2.295)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9286218285560608\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.462304812273942e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 797.7158813476562\n",
      "Loss in Train(Contrastive Learning): 6.756025646595245\n",
      "Train(class): [0][1/2]\tBT 0.005 (0.042)\tDT 0.002 (0.039)\tloss 1.563 (1.623)\tAcc@1 0.219 (0.190)\n",
      "Train(class): {'worst_acc': 0.0252, 'acc_0_0': 0.0252, 'acc_0_1': 0.1467, 'acc_1_0': 0.9286, 'acc_1_1': 0.7266, 'mean_acc': 0.195}\n",
      "Val(class): {'weighted_mean_acc': 0.2023, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1803, 'acc_1_0': 0.9624, 'acc_1_1': 0.7218, 'mean_acc': 0.2702}\n",
      "Test(class): [0/1]\tTime 0.002 (0.039)\tLoss 0.3928 (1.2087)\tAcc@1 0.859 (0.368)\n",
      "Test(class): [0/1]\tTime 0.003 (0.022)\tLoss 1.8452 (1.3519)\tAcc@1 0.047 (0.287)\n",
      "Test(class): {'weighted_mean_acc': 0.191, 'worst_acc': 0.0275, 'acc_0_0': 0.0275, 'acc_0_1': 0.1863, 'acc_1_0': 0.9439, 'acc_1_1': 0.6931, 'mean_acc': 0.2646}\n",
      "--- Epoch 95 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.014 (0.074)\tDT 0.101 (1.937)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9289699196815491\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.4314918593736365e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 798.0146484375\n",
      "Loss in Train(Contrastive Learning): 6.756022646072063\n",
      "Train(class): [0][1/2]\tBT 0.008 (0.043)\tDT 0.005 (0.040)\tloss 1.626 (1.610)\tAcc@1 0.219 (0.197)\n",
      "Train(class): {'worst_acc': 0.0263, 'acc_0_0': 0.0263, 'acc_0_1': 0.1739, 'acc_1_0': 0.9464, 'acc_1_1': 0.7247, 'mean_acc': 0.1967}\n",
      "Val(class): {'weighted_mean_acc': 0.2024, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1845, 'acc_1_0': 0.9624, 'acc_1_1': 0.7218, 'mean_acc': 0.2719}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3936 (1.2050)\tAcc@1 0.875 (0.370)\n",
      "Test(class): [0/1]\tTime 0.005 (0.023)\tLoss 1.8497 (1.3485)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1925, 'worst_acc': 0.0275, 'acc_0_0': 0.0275, 'acc_0_1': 0.1894, 'acc_1_0': 0.9439, 'acc_1_1': 0.6994, 'mean_acc': 0.2665}\n",
      "--- Epoch 96 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.094)\tDT 0.092 (2.612)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9293385744094849\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.347291931277141e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 798.33056640625\n",
      "Loss in Train(Contrastive Learning): 6.756019785049114\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.041)\tDT 0.001 (0.038)\tloss 1.619 (1.631)\tAcc@1 0.203 (0.190)\n",
      "Train(class): {'worst_acc': 0.0237, 'acc_0_0': 0.0237, 'acc_0_1': 0.1576, 'acc_1_0': 0.9464, 'acc_1_1': 0.7342, 'mean_acc': 0.1962}\n",
      "Val(class): {'weighted_mean_acc': 0.2024, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1845, 'acc_1_0': 0.9624, 'acc_1_1': 0.7218, 'mean_acc': 0.2719}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.3930 (1.2090)\tAcc@1 0.867 (0.369)\n",
      "Test(class): [0/1]\tTime 0.005 (0.023)\tLoss 1.8578 (1.3540)\tAcc@1 0.047 (0.288)\n",
      "Test(class): {'weighted_mean_acc': 0.1922, 'worst_acc': 0.0275, 'acc_0_0': 0.0275, 'acc_0_1': 0.1889, 'acc_1_0': 0.9439, 'acc_1_1': 0.6978, 'mean_acc': 0.2661}\n",
      "--- Epoch 97 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.011 (0.089)\tDT 2.651 (2.436)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.929681122303009\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.31527044181712e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 798.6245727539062\n",
      "Loss in Train(Contrastive Learning): 6.7560169569989466\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.045)\tDT 0.001 (0.042)\tloss 1.680 (1.619)\tAcc@1 0.156 (0.200)\n",
      "Train(class): {'worst_acc': 0.0246, 'acc_0_0': 0.0246, 'acc_0_1': 0.1685, 'acc_1_0': 0.9286, 'acc_1_1': 0.7275, 'mean_acc': 0.1956}\n",
      "Val(class): {'weighted_mean_acc': 0.198, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9624, 'acc_1_1': 0.6992, 'mean_acc': 0.2744}\n",
      "Test(class): [0/1]\tTime 0.002 (0.040)\tLoss 0.4017 (1.1978)\tAcc@1 0.852 (0.370)\n",
      "Test(class): [0/1]\tTime 0.006 (0.023)\tLoss 1.8308 (1.3384)\tAcc@1 0.047 (0.290)\n",
      "Test(class): {'weighted_mean_acc': 0.1912, 'worst_acc': 0.031, 'acc_0_0': 0.031, 'acc_0_1': 0.1965, 'acc_1_0': 0.9424, 'acc_1_1': 0.6807, 'mean_acc': 0.2684}\n",
      "--- Epoch 98 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.074)\tDT 0.091 (1.954)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9300233721733093\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.2615243981126696e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 798.9180908203125\n",
      "Loss in Train(Contrastive Learning): 6.7560141695306655\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.039)\tDT 0.001 (0.036)\tloss 1.667 (1.601)\tAcc@1 0.148 (0.204)\n",
      "Train(class): {'worst_acc': 0.0246, 'acc_0_0': 0.0246, 'acc_0_1': 0.1685, 'acc_1_0': 0.9464, 'acc_1_1': 0.7323, 'mean_acc': 0.1969}\n",
      "Val(class): {'weighted_mean_acc': 0.1962, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1974, 'acc_1_0': 0.9549, 'acc_1_1': 0.6917, 'mean_acc': 0.2727}\n",
      "Test(class): [0/1]\tTime 0.002 (0.038)\tLoss 0.4030 (1.1978)\tAcc@1 0.844 (0.370)\n",
      "Test(class): [0/1]\tTime 0.004 (0.021)\tLoss 1.8270 (1.3373)\tAcc@1 0.047 (0.289)\n",
      "Test(class): {'weighted_mean_acc': 0.1899, 'worst_acc': 0.0297, 'acc_0_0': 0.0297, 'acc_0_1': 0.1969, 'acc_1_0': 0.9408, 'acc_1_1': 0.6791, 'mean_acc': 0.2677}\n",
      "--- Epoch 99 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.017 (0.095)\tDT 0.093 (2.638)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9303728938102722\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.215359538444318e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 799.2179565429688\n",
      "Loss in Train(Contrastive Learning): 6.756011420107902\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.037)\tDT 0.001 (0.033)\tloss 1.634 (1.617)\tAcc@1 0.172 (0.195)\n",
      "Train(class): {'worst_acc': 0.0246, 'acc_0_0': 0.0246, 'acc_0_1': 0.1359, 'acc_1_0': 0.9464, 'acc_1_1': 0.7294, 'mean_acc': 0.195}\n",
      "Val(class): {'weighted_mean_acc': 0.2023, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1803, 'acc_1_0': 0.9624, 'acc_1_1': 0.7218, 'mean_acc': 0.2702}\n",
      "Test(class): [0/1]\tTime 0.002 (0.033)\tLoss 0.3903 (1.2168)\tAcc@1 0.867 (0.367)\n",
      "Test(class): [0/1]\tTime 0.003 (0.019)\tLoss 1.8705 (1.3645)\tAcc@1 0.047 (0.285)\n",
      "Test(class): {'weighted_mean_acc': 0.1897, 'worst_acc': 0.0244, 'acc_0_0': 0.0244, 'acc_0_1': 0.1831, 'acc_1_0': 0.9439, 'acc_1_1': 0.6978, 'mean_acc': 0.2627}\n",
      "--- Epoch 100 ---\n",
      "Train(Contrastive Learning): [0][1/2]\tBT 0.013 (0.086)\tDT 2.051 (2.361)\tloss 6.756 (6.756)\t\n",
      "(Last) Averaged Pos-term in Train(Contrastive Learning): 0.9306858777999878\n",
      "(Last) Averaged Neg-term in Train(Contrastive Learning): 3.171499338350259e-05\n",
      "(Last) Denominator in Train(Contrastive Learning): 799.4863891601562\n",
      "Loss in Train(Contrastive Learning): 6.756008696048818\n",
      "Train(class): [0][1/2]\tBT 0.004 (0.035)\tDT 0.001 (0.032)\tloss 1.608 (1.605)\tAcc@1 0.180 (0.200)\n",
      "Train(class): {'worst_acc': 0.0249, 'acc_0_0': 0.0249, 'acc_0_1': 0.1522, 'acc_1_0': 0.9464, 'acc_1_1': 0.7275, 'mean_acc': 0.1954}\n",
      "Val(class): {'weighted_mean_acc': 0.2025, 'worst_acc': 0.0343, 'acc_0_0': 0.0343, 'acc_0_1': 0.1867, 'acc_1_0': 0.9624, 'acc_1_1': 0.7218, 'mean_acc': 0.2727}\n",
      "Test(class): [0/1]\tTime 0.002 (0.034)\tLoss 0.3927 (1.2042)\tAcc@1 0.867 (0.367)\n",
      "Test(class): [0/1]\tTime 0.003 (0.019)\tLoss 1.8385 (1.3456)\tAcc@1 0.047 (0.287)\n",
      "Test(class): {'weighted_mean_acc': 0.1894, 'worst_acc': 0.0271, 'acc_0_0': 0.0271, 'acc_0_1': 0.1889, 'acc_1_0': 0.9439, 'acc_1_1': 0.6869, 'mean_acc': 0.2648}\n",
      "========================================================================\n",
      "> end of training. \n",
      "\n",
      "best epoch : 2\n",
      "best training accuracy on [class]: {'worst_acc': 0.0269, 'acc_0_0': 0.0269, 'acc_0_1': 0.212, 'acc_1_0': 0.9107, 'acc_1_1': 0.5913, 'mean_acc': 0.1687}\n",
      "best validation accuracy on [class]: {'weighted_mean_acc': 0.1802, 'worst_acc': 0.0428, 'acc_0_0': 0.0428, 'acc_0_1': 0.2361, 'acc_1_0': 0.9098, 'acc_1_1': 0.5865, 'mean_acc': 0.2744}\n",
      "best test accuracy on [class]: {'weighted_mean_acc': 0.1628, 'worst_acc': 0.0257, 'acc_0_0': 0.0257, 'acc_0_1': 0.2399, 'acc_1_0': 0.905, 'acc_1_1': 0.5639, 'mean_acc': 0.2661}\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "# opt = parse_option()\n",
    "\n",
    "\n",
    "print(f\"> Start Transfer Learning using [{opt.tl_method}]\")\n",
    "print('========================================================================')\n",
    "if opt.dataset == 'waterbirds':\n",
    "    # build dataset example.\n",
    "    print(f\"Load image embedding of Waterbirds: {opt.image_embedding_dir}\")\n",
    "    trainset = WaterbirdsEmbeddings(opt.data_dir, 'train', opt.image_embedding_dir, None)\n",
    "    print(f\"ㄴ Corresponding text embedding of Waterbirds: {opt.text_embedding_dir}\")\n",
    "    # build data loader\n",
    "    print(\"Load Data Loader (train, validation, test)\")\n",
    "    train_loader, val_loader, test_loader = load_waterbirds_embeddings(opt.data_dir, opt.image_embedding_dir, opt.batch_size, opt.batch_size)\n",
    "    \n",
    "    # print training target\n",
    "    if opt.train_target == \"class\":\n",
    "        print(f\"Training target : {opt.train_target} (Land bird(0) / Water bird(1))\")\n",
    "    elif opt.train_target == \"spurious\":\n",
    "        print(f\"Training target : {opt.train_target} (Land background(0) / Water background(1))\")\n",
    "    \n",
    "elif opt.dataset == 'celeba':\n",
    "    # build dataset example.\n",
    "    print(f\"Load embedding of CelebA: {opt.image_embedding_dir}\")\n",
    "    trainset = CelebaEmbeddings(opt.data_dir, 'train', opt.image_embedding_dir, None)\n",
    "    print(f\"ㄴ Corresponding text embedding of Waterbirds: {opt.text_embedding_dir}\")\n",
    "    # build data loader\n",
    "    print(\"Load Data Loader (train, validation, test)\")\n",
    "    train_loader, val_loader, test_loader = load_celeba_embeddings(opt.data_dir, opt.image_embedding_dir, opt.batch_size, opt.batch_size)\n",
    "    \n",
    "    # print training target\n",
    "    if opt.train_target == \"class\":\n",
    "        print(f\"Training target : {opt.train_target} (non-blond hair(0) / blond hair(1))\")\n",
    "    elif opt.train_target == \"spurious\":\n",
    "        print(f\"Training target : {opt.train_target} (female(0) / male(1))\")\n",
    "\n",
    "# group information\n",
    "get_yp_func = partial(get_y_p, n_places=trainset.n_places)\n",
    "train_group_ratio = trainset.group_ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# build model and criterion\n",
    "classifier, ce_loss = set_model(opt) # model, \n",
    "\n",
    "\n",
    "# Main Ce code\n",
    "if opt.tl_method==\"contrastive_adapter\": \n",
    "    print(\"> set and load Contrastive data-handler\")\n",
    "    print('========================================================================')\n",
    "    sliced_data_indices, sliced_data_correct = compute_slice_indices(trainset)\n",
    "    contrastive_points = prepare_contrastive_points(trainset,sliced_data_indices,sliced_data_correct)\n",
    "    slice_anchors, slice_negatives, positives_by_class, all_targets = contrastive_points\n",
    "    \n",
    "    adjust_num_pos_neg_(positives_by_class, slice_negatives, opt)\n",
    "\n",
    "    contrastive_loss = SupervisedContrastiveLoss(opt)\n",
    "    \n",
    "    # Get contrastive batches for first epoch\n",
    "    contrastive_dataloader = load_contrastive_data(train_loader,\n",
    "                                                    slice_anchors,\n",
    "                                                    slice_negatives,\n",
    "                                                    positives_by_class,\n",
    "                                                    opt, True)\n",
    "    print('========================================================================')\n",
    "# build optimizer\n",
    "print(\"Set Optimizer: SGD (default)\")\n",
    "print('========================================================================')\n",
    "optimizer = set_optimizer(opt, classifier)\n",
    "\n",
    "\n",
    "# training routine\n",
    "train_losses = []\n",
    "train_losses_cl = []\n",
    "train_accs = []\n",
    "train_group_accs = []\n",
    "\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "val_group_accs = []\n",
    "\n",
    "test_losses = [] # NOTE: Don't peek ! \n",
    "test_accs = [] # NOTE: Don't peek ! \n",
    "test_group_accs = [] # NOTE: Don't peek ! \n",
    "\n",
    "# entire training\n",
    "for epoch in range(1, opt.epochs + 1):\n",
    "    adjust_learning_rate(opt, optimizer, epoch)\n",
    "    print(f'--- Epoch {epoch} ---')\n",
    "    \n",
    "    # train one epoch\n",
    "    loss_cl = train_one_epoch_cl(opt, contrastive_dataloader, classifier, contrastive_loss,\n",
    "                        optimizer, epoch, print_label=f'Train(Contrastive Learning)')\n",
    "    train_losses_cl.append(loss_cl)\n",
    "    \n",
    "    loss, acc, group_acc = train_one_epoch(opt, train_loader, classifier, ce_loss,\n",
    "                        optimizer, epoch, get_yp_func, target=opt.train_target, print_label=f'Train({opt.train_target})')\n",
    "    \n",
    "    train_losses.append(loss); train_accs.append(acc); train_group_accs.append(group_acc)\n",
    "    \n",
    "    # eval for one epoch\n",
    "    val_loss, val_acc, val_group_acc = validate(opt, val_loader, classifier, ce_loss, get_yp_func, train_group_ratio, target=opt.train_target, print_label=f'Val({opt.train_target})')\n",
    "    val_losses.append(val_loss); val_accs.append(val_acc); val_group_accs.append(val_group_acc)\n",
    "    \n",
    "    # update best epoch by worst_group accuracy (default)\n",
    "    if val_group_acc['worst_acc'] > best_acc:\n",
    "        best_acc = val_group_acc['worst_acc']\n",
    "        best_epoch = epoch\n",
    "        best_model = copy.deepcopy(classifier)\n",
    "    \n",
    "    # test for one epoch\n",
    "    test_loss, test_acc, test_group_acc = validate(opt, test_loader, classifier, ce_loss, get_yp_func, train_group_ratio, target='class', print_label=f'Test({opt.train_target})')\n",
    "    \n",
    "    test_losses.append(test_loss); test_accs.append(test_acc); test_group_accs.append(test_group_acc)\n",
    "\n",
    "\n",
    "print('========================================================================')\n",
    "print(\"> end of training. \\n\")\n",
    "print('best epoch : {}'.format(best_epoch))\n",
    "\n",
    "best_train_group_acc = train_group_accs[best_epoch-1]\n",
    "best_val_group_acc = val_group_accs[best_epoch-1]\n",
    "best_test_group_acc = test_group_accs[best_epoch-1]\n",
    "\n",
    "print(f'best training accuracy on [{opt.train_target}]: {best_train_group_acc}')\n",
    "print(f'best validation accuracy on [{opt.train_target}]: {best_val_group_acc}')\n",
    "print(f'best test accuracy on [{opt.train_target}]: {best_test_group_acc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debuging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader Iteration 오류  \n",
    "#### Conclusion\n",
    "- Add \"datasets.on_contrastive_batch\" for swichting indexing mechanism.\n",
    "  - in def get_resampled_dataset \n",
    "  - in {dataset}_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = copy.deepcopy(contrastive_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_batch_size = 1 + opt.num_positive + opt.num_negative\n",
    "print(\"Size of each data\", each_batch_size)\n",
    "full_data_size = len(dataset_test.embeddings_df.columns)\n",
    "print(\"Number of Full sample : \", full_data_size)\n",
    "num_data = full_data_size // each_batch_size\n",
    "print(\"Number of data : \", num_data, \"Anchor ~ \")\n",
    "\n",
    "offset = 0\n",
    "for _ in range(num_data):\n",
    "    each_data = dataset_test.embeddings_df.iloc[:, offset: offset+each_batch_size]\n",
    "    offset += each_batch_size\n",
    "\n",
    "    # 하나의 anc-pos-neg triplet 안에 중복되는 샘플이 있는지.\n",
    "    assert(len(np.unique(each_data.columns)) == each_batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.embeddings_df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    try:\n",
    "        sample = dataset_test[i]\n",
    "    except:\n",
    "        \n",
    "        print(\"Problem Index : \", i)\n",
    "        print(\"len(File_name.array)\", len(dataset_test.filename_array))\n",
    "        \n",
    "        img_filename = dataset_test.filename_array[i]\n",
    "        print(\"filename\", img_filename)\n",
    "        # print(e)\n",
    "        ebd_full = dataset_test.embeddings_df[img_filename]\n",
    "        print(ebd_full)\n",
    "        break\n",
    "        \n",
    "\n",
    "# File 이름은 하나지만, 그에 따라 인덱싱되는 샘플이 여러개라 오류 발생.\n",
    "## Waterbirds_embeddings.py에서, self.on_contrastive_batch = [True/False] 를 이용해 인덱싱 방식을 바꿔준다.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: filename_array[idx] == embeddings_df.columns[idx]\n",
    "\n",
    "for idx in range(full_data_size):\n",
    "    # print(dataset_test.filename_array[idx])\n",
    "    # print(dataset_test.embeddings_df.columns[idx]) # ㄹ\n",
    "    # print(dataset_test.embeddings_df.iloc[:, idx])\n",
    "    \n",
    "    # print(dataset_test.embeddings_df[dataset_test.filename_array[idx]].shape) # 문제상황\n",
    "    \n",
    "    assert dataset_test.filename_array[idx] == dataset_test.embeddings_df.columns[idx] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27504, 1024])\n",
      "torch.Size([27504])\n",
      "torch.Size([27504])\n",
      "torch.Size([27504])\n",
      "torch.Size([27504])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_iter = iter(contrastive_dataloader)\n",
    "\n",
    "each_batch =  data_iter.next()\n",
    "ebd, full_y, _ = each_batch\n",
    "\n",
    "target, group, spurious, zs_pred = full_y.values()\n",
    "\n",
    "print(ebd.shape)\n",
    "print(target.shape)\n",
    "print(group.shape)\n",
    "print(spurious.shape)\n",
    "print(zs_pred.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SupervisedContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super(SupervisedContrastiveLoss, self).__init__()\n",
    "#         self.cl_temperature = args.cl_temperature\n",
    "#         self.n_positives = args.num_positive\n",
    "#         self.n_negatives = args.num_negative\n",
    "#         self.args = args\n",
    "    \n",
    "#         self.sim = nn.CosineSimilarity(dim=1)\n",
    "        \n",
    "#     def forward(self, model, contrastive_batch):\n",
    "        \n",
    "#         # contrastive_batch [anc; pos; neg] : (1+N+M(=N), )\n",
    "        \n",
    "#         # Compute negative similarities\n",
    "#         neg_indices = [0] + list(range(len(contrastive_batch))[\n",
    "#             -self.n_negatives:])\n",
    "#         anchor_negatives = contrastive_batch[neg_indices]\n",
    "        \n",
    "        \n",
    "#         # Compute positive similarities\n",
    "#         anchor_positives = contrastive_batch[:1 + self.n_positives]\n",
    "#         exp_pos = self.compute_sim_exp(model, anchor_positives, \n",
    "#                                        return_sum=False)\n",
    "#         # print(\"pos\", pos[:5])\n",
    "#         # print(\"scaled_pos\", pos[:5])\n",
    "#         # exp_pos = torch.exp(pos)\n",
    "#         print(\"exp_pos\", exp_pos[:5])\n",
    "#         # M(=N)개의 exp(sim) score\n",
    "#         exp_neg = self.compute_sim_exp(model, anchor_negatives,\n",
    "#                                        return_sum=False)\n",
    "#         # print(\"neg\", neg[:5])\n",
    "        \n",
    "#         # neg = neg - max_pos.detach() # 같은 Scalining 먹여야.\n",
    "#         # print(\"scaled_neg\", pos[:5])\n",
    "#         # exp_neg = torch.exp(neg)\n",
    "#         print(\"exp_neg\", exp_neg[:5])\n",
    "#         sum_exp_neg = exp_neg.sum(0, keepdim=True)\n",
    "#         print(\"sum_exp_neg\", sum_exp_neg[:5])\n",
    "#         # print(\"(Sum)Exp_neg:, \", sum_exp_neg)\n",
    "        \n",
    "#         print(\"Numeraator: \", torch.log(exp_pos)[:5])\n",
    "#         print(\"Denominator: \", torch.log(sum_exp_neg + exp_pos.sum(0, keepdim=True)))\n",
    "#         log_probs = (torch.log(exp_pos) - \n",
    "#                         torch.log(sum_exp_neg + exp_pos.sum(0, keepdim=True)))\n",
    "#         print(\"log_probs\", log_probs[:5])\n",
    "#         loss = -1 * log_probs\n",
    "#         del exp_pos; del exp_neg; del log_probs\n",
    "#         return loss.mean() # N개의 Positives에 대한 평균. \n",
    "    \n",
    "#     def compute_sim_exp(self, model, features, return_sum=True):\n",
    "#         \"\"\"\n",
    "#         Compute sum(sim(anchor, pos)) or sum(sim(anchor, neg))\n",
    "#         First index : anchor\n",
    "#         \"\"\"\n",
    "#         # in Contrastive Adapter, features:CLIP, outputs:Adapted-CLIP \n",
    "        \n",
    "#         if self.args.tl_method ==\"contrastive_adapter\":\n",
    "#             outputs = model.adapter(features) # unnormalized; model.forward는 (normalized) logits까지. \n",
    "        \n",
    "#         sim = self.sim(outputs[0].view(1, -1), outputs[1:]) # (N,) or (M,)\n",
    "#         # print(\"(in compute_exp_sim), sim : \", sim[:5])\n",
    "#         sim_divided_by_temp = torch.div(sim, self.cl_temperature)\n",
    "        \n",
    "#         # print(\"sim_divided_by_temp\", sim_divided_by_temp[:5])\n",
    "#         # print(torch.max(sim_divided_by_temp, dim = 0, keepdim=True))\n",
    "#         # max_sim_divided_by_temp, _ =  torch.max(sim_divided_by_temp, dim = 0, keepdim=True)\n",
    "#         # print(\"(instability issue) max(sim_divided_by_temp)\", max_sim_divided_by_temp)\n",
    "#         # new_sim_divided_by_temp = sim_divided_by_temp - max_sim_divided_by_temp.detach() \n",
    "#         # print(\"(instability issue) sim_divied_by_temp - max(sim_divided_by_temp)\", new_sim_divided_by_temp[:5])\n",
    "#         # print(\"sim_divided_by_temp\", sim_divided_by_temp[:5])\n",
    "        \n",
    "#         exp_sim = torch.exp(sim_divided_by_temp)\n",
    "        \n",
    "#         # print(\"exp_sim_divided_by_temp\", exp_sim[:5])\n",
    "        \n",
    "#         #####  Should not detach from graph\n",
    "#         features = features.to(torch.device('cpu'))\n",
    "#         outputs = outputs.to(torch.device('cpu'))\n",
    "    \n",
    "#         return exp_sim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_mmd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

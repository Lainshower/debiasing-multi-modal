{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/jinsu/workstation/project/debiasing-multi-modal\")\n",
    "\n",
    "dict_embeddings=np.load(\"/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings/waterbirds/resnet50/image_embedding.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from util import AverageMeter\n",
    "from util import adjust_learning_rate, warmup_learning_rate, accuracy\n",
    "from util import set_optimizer\n",
    "# from networks.resnet_big import SupConResNet, LinearClassifier\n",
    "\n",
    "try:\n",
    "    import apex\n",
    "    from apex import amp, optimizers\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from resnet import resnet50\n",
    "from data.embeddings import Embeddings, load_embeddings\n",
    "model_dict = {'resnet50': [resnet50, 2048]}\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, name='resnet50', num_classes=2):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        _, feat_dim = model_dict[name]\n",
    "        self.fc = nn.Linear(feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.fc(features)\n",
    "\n",
    "def parse_option():\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--print_freq', type=int, default=10,\n",
    "                        help='print frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=50,\n",
    "                        help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=256,\n",
    "                        help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=16,\n",
    "                        help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='number of training epochs')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',\n",
    "                        help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.2,\n",
    "                        help='decay rate for learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='momentum')\n",
    "\n",
    "    # model dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet50')\n",
    "    parser.add_argument('--dataset', type=str, default='waterbirds',\n",
    "                        choices=['celebA', 'waterbirds'], help='dataset')\n",
    "\n",
    "    # other setting\n",
    "    parser.add_argument('--cosine', action='store_true',\n",
    "                        help='using cosine annealing')\n",
    "    parser.add_argument('--warm', action='store_true',\n",
    "                        help='warm-up for large batch training')\n",
    "\n",
    "    parser.add_argument('--embedding_dir', type=str,\n",
    "                        help='extracted embedding')\n",
    "    parser.add_argument('--target', type=str, default=\"class\", choices=[\"class\", \"group\", \"spurious\"])\n",
    "    parser.add_argument('--data_dir', type=str,\n",
    "                        help='metadata.csv')\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "\n",
    "    # set the path according to the environment\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "    opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\\\n",
    "        format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n",
    "               opt.batch_size)\n",
    "\n",
    "    if opt.cosine:\n",
    "        opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "    # warm-up for large-batch training,\n",
    "    if opt.warm:\n",
    "        opt.model_name = '{}_warm'.format(opt.model_name)\n",
    "        opt.warmup_from = 0.01\n",
    "        opt.warm_epochs = 10\n",
    "        if opt.cosine:\n",
    "            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "        else:\n",
    "            opt.warmup_to = opt.learning_rate\n",
    "            \n",
    "    if opt.dataset == 'celebA':\n",
    "        opt.n_cls = 2\n",
    "    elif opt.dataset == 'waterbirds':\n",
    "        opt.n_cls = 2\n",
    "    else:\n",
    "        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "def set_model(opt):\n",
    "    # model = SupConResNet(name=opt.model)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)\n",
    "\n",
    "    # ckpt = torch.load(opt.ckpt, map_location='cpu')\n",
    "    # state_dict = ckpt['model']\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # if torch.cuda.device_count() > 1:\n",
    "        #     model.encoder = torch.nn.DataParallel(model.encoder)\n",
    "        # else:\n",
    "            # new_state_dict = {}\n",
    "            # for k, v in state_dict.items():\n",
    "            #     k = k.replace(\"module.\", \"\")\n",
    "            #     new_state_dict[k] = v\n",
    "            # state_dict = new_state_dict\n",
    "        \n",
    "        # model = model.cuda()\n",
    "        classifier = classifier.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        # model.load_state_dict(state_dict)\n",
    "\n",
    "    return classifier, criterion # model, \n",
    "\n",
    "\n",
    "def train(train_loader, classifier, criterion, optimizer, epoch, opt): # model,\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    # model.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        if opt.dataset == 'waterbirds':\n",
    "            embeddings, all_labels, _ = data\n",
    "            labels = all_labels[opt.target] # (y, y_group, y_spurious)\n",
    "        else:\n",
    "            embeddings, all_labels = data\n",
    "            labels = all_labels[opt.target] # (y, y_group, y_spurious)\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        embeddings = embeddings.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # warm-up learning rate\n",
    "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
    "\n",
    "        # compute loss\n",
    "        # with torch.no_grad():\n",
    "        #     features = model.encoder(embeddings)\n",
    "        output = classifier(features.detach())\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "        acc1 = accuracy(output, labels, bsz)\n",
    "        acc.update(acc1, bsz)\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        if (idx + 1) % opt.print_freq == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
    "                  'Acc@1 {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, acc=acc))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return losses.avg, acc.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, classifier, criterion, opt):\n",
    "    \"\"\"validation\"\"\"\n",
    "    \n",
    "    # model.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, data in enumerate(val_loader):\n",
    "            if opt.dataset == 'waterbirds':\n",
    "                embeddings, all_labels, _ = data\n",
    "                labels = all_labels[opt.target] # (y, y_group, y_spurious)\n",
    "            else:\n",
    "                embeddings, all_labels = data\n",
    "                labels = all_labels[opt.target] # (y, y_group, y_spurious)\n",
    "            \n",
    "            embeddings = embeddings.float().cuda()\n",
    "            labels = labels.cuda()\n",
    "            bsz = labels.shape[0]\n",
    "\n",
    "            # forward\n",
    "            output = classifier(model.encoder(embeddings))\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # update metric\n",
    "            losses.update(loss.item(), bsz)\n",
    "            acc1 = accuracy(output, labels, bsz)\n",
    "            acc.update(acc1[0], bsz)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if idx % opt.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Acc@1 {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                       idx, len(val_loader), batch_time=batch_time,\n",
    "                       loss=losses, acc=acc))\n",
    "\n",
    "    print(' * Acc@1 {acc.avg:.3f}'.format(acc=acc))\n",
    "    return losses.avg, acc.avg\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    best_acc = 0\n",
    "    # opt = parse_option()\n",
    "\n",
    "    # build data loader\n",
    "    train_loader, val_loader, test_loader = load_embeddings(opt.data_dir, opt.embedding_dir, opt.model, opt.batch_size, opt.batch_size)\n",
    "    \n",
    "    # build model and criterion\n",
    "    classifier, criterion = set_model(opt) # model, \n",
    "\n",
    "    # build optimizer\n",
    "    optimizer = set_optimizer(opt, classifier)\n",
    "\n",
    "    # training routine\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "        adjust_learning_rate(opt, optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        time1 = time.time()\n",
    "        loss, acc = train(val_loader, classifier, criterion,\n",
    "                          optimizer, epoch, opt)\n",
    "        time2 = time.time()\n",
    "        print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(\n",
    "            epoch, time2 - time1, acc))\n",
    "\n",
    "        # eval for one epoch\n",
    "        loss, val_acc = validate(val_loader, classifier, criterion, opt)\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            \n",
    "        loss, val_acc = validate(test_loader, classifier, criterion, opt)\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "\n",
    "    print('best accuracy: {:.2f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "parser.add_argument('--print_freq', type=int, default=10,\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--save_freq', type=int, default=50,\n",
    "                    help='save frequency')\n",
    "parser.add_argument('--batch_size', type=int, default=512,\n",
    "                    help='batch_size')\n",
    "parser.add_argument('--num_workers', type=int, default=16,\n",
    "                    help='num of workers to use')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='number of training epochs')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--learning_rate', type=float, default=5,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',\n",
    "                    help='where to decay lr, can be a list')\n",
    "parser.add_argument('--lr_decay_rate', type=float, default=0.2,\n",
    "                    help='decay rate for learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='momentum')\n",
    "\n",
    "# model dataset\n",
    "parser.add_argument('--model', type=str, default='resnet50')\n",
    "parser.add_argument('--dataset', type=str, default='waterbirds',\n",
    "                    choices=['celebA', 'waterbirds'], help='dataset')\n",
    "\n",
    "# other setting\n",
    "parser.add_argument('--cosine', action='store_true',\n",
    "                    help='using cosine annealing')\n",
    "parser.add_argument('--warm', action='store_true',\n",
    "                    help='warm-up for large batch training')\n",
    "\n",
    "parser.add_argument('--embedding_dir', type=str, \n",
    "                    help='extracted embedding')\n",
    "parser.add_argument('--target', type=str, default=\"class\", choices=[\"class\", \"group\", \"spurious\"])\n",
    "parser.add_argument('--data_dir', type=str,\n",
    "                    help='metadata.csv')\n",
    "\n",
    "opt = parser.parse_args(args=[])   \n",
    "\n",
    "\n",
    "\n",
    "iterations = opt.lr_decay_epochs.split(',')\n",
    "opt.lr_decay_epochs = list([])\n",
    "for it in iterations:\n",
    "    opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\\\n",
    "    format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n",
    "            opt.batch_size)\n",
    "\n",
    "if opt.cosine:\n",
    "    opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "# warm-up for large-batch training,\n",
    "if opt.warm:\n",
    "    opt.model_name = '{}_warm'.format(opt.model_name)\n",
    "    opt.warmup_from = 0.01\n",
    "    opt.warm_epochs = 10\n",
    "    if opt.cosine:\n",
    "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "        opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "    else:\n",
    "        opt.warmup_to = opt.learning_rate\n",
    "        \n",
    "if opt.dataset == 'celebA':\n",
    "    opt.n_cls = 2\n",
    "elif opt.dataset == 'waterbirds':\n",
    "    opt.n_cls = 2\n",
    "else:\n",
    "    raise ValueError('dataset not supported: {}'.format(opt.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.embedding_dir = \"/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings/waterbirds/\"\n",
    "opt.data_dir = \"/home/jinsu/workstation/project/debiasing-multi-modal/data/waterbirds/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jinsu/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jinsu/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jinsu/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings.py\", line 61, in __getitem__\n    z = self.embeddings_dict[img_filename.split('/')[-1]]\nKeyError: 'Black_Footed_Albatross_0009_34.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:    \n\u001b[0;32m----> 3\u001b[0m     main(opt)\n",
      "Cell \u001b[0;32mIn[9], line 277\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m# train for one epoch\u001b[39;00m\n\u001b[1;32m    276\u001b[0m time1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 277\u001b[0m loss, acc \u001b[39m=\u001b[39m train(val_loader, classifier, criterion,\n\u001b[1;32m    278\u001b[0m                   optimizer, epoch, opt)\n\u001b[1;32m    279\u001b[0m time2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    280\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, total time \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m, accuracy:\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    281\u001b[0m     epoch, time2 \u001b[39m-\u001b[39m time1, acc))\n",
      "Cell \u001b[0;32mIn[9], line 158\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, classifier, criterion, optimizer, epoch, opt)\u001b[0m\n\u001b[1;32m    155\u001b[0m acc \u001b[39m=\u001b[39m AverageMeter()\n\u001b[1;32m    157\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 158\u001b[0m \u001b[39mfor\u001b[39;00m idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m opt\u001b[39m.\u001b[39mdataset \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mwaterbirds\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    160\u001b[0m         embeddings, all_labels, _ \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1376\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1376\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1402\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1403\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jinsu/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jinsu/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jinsu/anaconda3/envs/cuda_test/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jinsu/workstation/project/debiasing-multi-modal/data/embeddings.py\", line 61, in __getitem__\n    z = self.embeddings_dict[img_filename.split('/')[-1]]\nKeyError: 'Black_Footed_Albatross_0009_34.jpg'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    \n",
    "    main(opt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_embeddings.keys():\n",
    "    if '0009_34' in key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, classifier, criterion, optimizer, epoch, opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
